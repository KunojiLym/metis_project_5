{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "RANDOM = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "haikus_train_df = pd.read_pickle('./data/haikus_train_df.pickle')\n",
    "haikus_test_df = pd.read_pickle('./data/haikus_test_df.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Input, LSTM, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Words:  421611\n",
      "Total Vocab:  24045\n"
     ]
    }
   ],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "corpuswords_raw = [item for item in flatten(list(haikus_train_df['text_withtokens_clean'])) if item != '']\n",
    "\n",
    "words = sorted(set(corpuswords_raw))\n",
    "word_to_int = dict((w, i) for i, w in enumerate(words))\n",
    "\n",
    "n_words = len(corpuswords_raw)\n",
    "n_vocab_words = len(words)\n",
    "print(\"Total Words: \", n_words)\n",
    "print(\"Total Vocab: \", n_vocab_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<eNd>',\n",
       " '<nEXt>',\n",
       " 'a',\n",
       " 'aaaa',\n",
       " 'aah',\n",
       " 'aback',\n",
       " 'abandon',\n",
       " 'abandoned',\n",
       " 'abandoning',\n",
       " 'abating',\n",
       " 'abattoir',\n",
       " 'abbess',\n",
       " 'abbey',\n",
       " 'abbot',\n",
       " 'abbott',\n",
       " 'abc',\n",
       " 'abcs',\n",
       " 'abduction',\n",
       " 'abed',\n",
       " 'abel',\n",
       " 'abelard',\n",
       " 'aberration',\n",
       " 'abhor',\n",
       " 'abhorred',\n",
       " 'abide',\n",
       " 'abilene',\n",
       " 'abjure',\n",
       " 'ablaze',\n",
       " 'able',\n",
       " 'ablowing',\n",
       " 'aboard',\n",
       " 'abode',\n",
       " 'abodes',\n",
       " 'abolish',\n",
       " 'abominable',\n",
       " 'abord',\n",
       " 'abortion',\n",
       " 'abound',\n",
       " 'about',\n",
       " 'above',\n",
       " 'abraham',\n",
       " 'abramoff',\n",
       " 'abreast',\n",
       " 'abriman',\n",
       " 'abroad',\n",
       " 'abrupt',\n",
       " 'abruptly',\n",
       " 'absalom',\n",
       " 'abscond',\n",
       " 'absence',\n",
       " 'absent',\n",
       " 'absently',\n",
       " 'absinthe',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absolution',\n",
       " 'absorb',\n",
       " 'absorbed',\n",
       " 'absorbing',\n",
       " 'absorbs',\n",
       " 'abstain',\n",
       " 'abstemious',\n",
       " 'abstract',\n",
       " 'abstracted',\n",
       " 'abstractedlyone',\n",
       " 'abstraction',\n",
       " 'absurd',\n",
       " 'abundance',\n",
       " 'abundant',\n",
       " 'abuse',\n",
       " 'abuses',\n",
       " 'abydos',\n",
       " 'abyss',\n",
       " 'abysses',\n",
       " 'acacia',\n",
       " 'academy',\n",
       " 'acadian',\n",
       " 'acadians',\n",
       " 'acc',\n",
       " 'acceleration',\n",
       " 'accent',\n",
       " 'accents',\n",
       " 'accept',\n",
       " 'acceptable',\n",
       " 'acceptance',\n",
       " 'accepted',\n",
       " 'accepting',\n",
       " 'accepts',\n",
       " 'access',\n",
       " 'accident',\n",
       " 'accidental',\n",
       " 'accidentally',\n",
       " 'acclaim',\n",
       " 'acclaims',\n",
       " 'accolade',\n",
       " 'accommodate',\n",
       " 'accompany',\n",
       " 'accompanying',\n",
       " 'accomplish',\n",
       " 'accomplished',\n",
       " 'accomplishing',\n",
       " 'accomplishment',\n",
       " 'accomplishments',\n",
       " 'accord',\n",
       " 'according',\n",
       " 'accordion',\n",
       " 'accordions',\n",
       " 'account',\n",
       " 'accountable',\n",
       " 'accountant',\n",
       " 'accounted',\n",
       " 'accounting',\n",
       " 'accounts',\n",
       " 'accoutrements',\n",
       " 'accumulate',\n",
       " 'accumulated',\n",
       " 'accumulation',\n",
       " 'accursed',\n",
       " 'accurst',\n",
       " 'accusation',\n",
       " 'accusations',\n",
       " 'accuse',\n",
       " 'accused',\n",
       " 'accuser',\n",
       " 'accustomed',\n",
       " 'acetone',\n",
       " 'ache',\n",
       " 'ached',\n",
       " 'acheron',\n",
       " 'aches',\n",
       " 'achieve',\n",
       " 'achieved',\n",
       " 'achievement',\n",
       " 'achievements',\n",
       " 'achiever',\n",
       " 'achieves',\n",
       " 'achieving',\n",
       " 'achilles',\n",
       " 'aching',\n",
       " 'acknowledge',\n",
       " 'acknowledged',\n",
       " 'acknowledges',\n",
       " 'acknowledgment',\n",
       " 'acolytes',\n",
       " 'acorn',\n",
       " 'acorns',\n",
       " 'acoustically',\n",
       " 'acquaintance',\n",
       " 'acquaintances',\n",
       " 'acquainted',\n",
       " 'acquire',\n",
       " 'acquired',\n",
       " 'acquisition',\n",
       " 'acquitted',\n",
       " 'acre',\n",
       " 'acreage',\n",
       " 'acres',\n",
       " 'acrid',\n",
       " 'acrisius',\n",
       " 'across',\n",
       " 'act',\n",
       " 'acted',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'actions',\n",
       " 'active',\n",
       " 'actively',\n",
       " 'activity',\n",
       " 'actor',\n",
       " 'actors',\n",
       " 'actress',\n",
       " 'acts',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'actuarial',\n",
       " 'actuary',\n",
       " 'acuminata',\n",
       " 'acupuncture',\n",
       " 'acute',\n",
       " 'ad',\n",
       " 'adage',\n",
       " 'adah',\n",
       " 'adam',\n",
       " 'adamantine',\n",
       " 'adams',\n",
       " 'adapt',\n",
       " 'add',\n",
       " 'added',\n",
       " 'addenda',\n",
       " 'adder',\n",
       " 'addicted',\n",
       " 'addictions',\n",
       " 'addicts',\n",
       " 'adding',\n",
       " 'addison',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'additions',\n",
       " 'address',\n",
       " 'addressed',\n",
       " 'addresses',\n",
       " 'adds',\n",
       " 'adepts',\n",
       " 'adequate',\n",
       " 'adhere',\n",
       " 'adherence',\n",
       " 'adheres',\n",
       " 'adhering',\n",
       " 'adhesives',\n",
       " 'adieu',\n",
       " 'adirondack',\n",
       " 'adj',\n",
       " 'adjoining',\n",
       " 'adjudge',\n",
       " 'adjuring',\n",
       " 'adjust',\n",
       " 'adjusted',\n",
       " 'adjusting',\n",
       " 'adjusts',\n",
       " 'administered',\n",
       " 'admirable',\n",
       " 'admiral',\n",
       " 'admirals',\n",
       " 'admiration',\n",
       " 'admire',\n",
       " 'admired',\n",
       " 'admiring',\n",
       " 'admit',\n",
       " 'admitted',\n",
       " 'admonish',\n",
       " 'admonitions',\n",
       " 'ado',\n",
       " 'adobe',\n",
       " 'adolescence',\n",
       " 'adolescent',\n",
       " 'adonai',\n",
       " 'adonis',\n",
       " 'adopted',\n",
       " 'adorable',\n",
       " 'adoration',\n",
       " 'adore',\n",
       " 'adored',\n",
       " 'adoring',\n",
       " 'adorn',\n",
       " 'adorned',\n",
       " 'adorner',\n",
       " 'adorning',\n",
       " 'adornment',\n",
       " 'adorns',\n",
       " 'adown',\n",
       " 'adrift',\n",
       " 'adroitly',\n",
       " 'ads',\n",
       " 'adult',\n",
       " 'adulterated',\n",
       " 'adulterous',\n",
       " 'adv',\n",
       " 'advance',\n",
       " 'advanced',\n",
       " 'advances',\n",
       " 'advancing',\n",
       " 'advantage',\n",
       " 'advantages',\n",
       " 'advent',\n",
       " 'advents',\n",
       " 'adventure',\n",
       " 'adventures',\n",
       " 'adventurous',\n",
       " 'adverbial',\n",
       " 'adverbs',\n",
       " 'adversary',\n",
       " 'adversity',\n",
       " 'advertise',\n",
       " 'advertised',\n",
       " 'advice',\n",
       " 'advise',\n",
       " 'advised',\n",
       " 'advisedly',\n",
       " 'advising',\n",
       " 'advocate',\n",
       " 'adze',\n",
       " 'aegean',\n",
       " 'aeolian',\n",
       " 'aeolus',\n",
       " 'aeons',\n",
       " 'aerated',\n",
       " 'aereal',\n",
       " 'aerial',\n",
       " 'aerie',\n",
       " 'aeronautical',\n",
       " 'aesthetic',\n",
       " 'aesthetics',\n",
       " 'aethereal',\n",
       " 'afar',\n",
       " 'aff',\n",
       " 'affair',\n",
       " 'affairs',\n",
       " 'affair\\x97',\n",
       " 'affectation',\n",
       " 'affection',\n",
       " 'affectionate',\n",
       " 'affinity',\n",
       " 'affirmative',\n",
       " 'afflicted',\n",
       " 'afflicting',\n",
       " 'afford',\n",
       " 'affords',\n",
       " 'affray',\n",
       " 'affright',\n",
       " 'affrighting',\n",
       " 'afghan',\n",
       " 'afghanistan',\n",
       " 'afghans',\n",
       " 'afield',\n",
       " 'aflame',\n",
       " 'aflare',\n",
       " 'afloat',\n",
       " 'aforetime',\n",
       " 'afraid',\n",
       " 'afresh',\n",
       " 'africa',\n",
       " 'african',\n",
       " 'after',\n",
       " 'afterbirth',\n",
       " 'afterglow',\n",
       " 'afterimage',\n",
       " 'afterlife',\n",
       " 'aftermath',\n",
       " 'afternoon',\n",
       " 'afternoons',\n",
       " 'aftershave',\n",
       " 'aftershock',\n",
       " 'aftershocks',\n",
       " 'afterward',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'againstness',\n",
       " 'agate',\n",
       " 'agave',\n",
       " 'age',\n",
       " 'aged',\n",
       " 'agenda',\n",
       " 'ages',\n",
       " 'aggravate',\n",
       " 'aggravated',\n",
       " 'aggression',\n",
       " 'aggressor’s',\n",
       " 'aggrieved',\n",
       " 'aghast',\n",
       " 'agib',\n",
       " 'aging',\n",
       " 'agitate',\n",
       " 'agitated',\n",
       " 'agitates',\n",
       " 'agitation',\n",
       " 'agleam',\n",
       " 'agloom',\n",
       " 'aglow',\n",
       " 'ago',\n",
       " 'agonies',\n",
       " 'agonized',\n",
       " 'agonizing',\n",
       " 'agony',\n",
       " 'agree',\n",
       " 'agreed',\n",
       " 'agreements',\n",
       " 'ague',\n",
       " 'ah',\n",
       " 'ahab',\n",
       " 'ahae',\n",
       " 'ahead',\n",
       " 'ahelion',\n",
       " 'ahh',\n",
       " 'ahhs',\n",
       " 'ahkond',\n",
       " 'ahkoond',\n",
       " 'aho',\n",
       " 'ahoy',\n",
       " 'ai',\n",
       " 'aid',\n",
       " 'aide',\n",
       " 'aids',\n",
       " 'aik',\n",
       " 'ail',\n",
       " 'ails',\n",
       " 'aim',\n",
       " 'aiming',\n",
       " 'aimless',\n",
       " 'aimlessly',\n",
       " 'aims',\n",
       " 'ain',\n",
       " 'aint',\n",
       " 'air',\n",
       " 'airborne',\n",
       " 'airbus',\n",
       " 'airily',\n",
       " 'airing',\n",
       " 'airless',\n",
       " 'airplane',\n",
       " 'airport',\n",
       " 'airs',\n",
       " 'airy',\n",
       " 'air\\x96',\n",
       " 'aisle',\n",
       " 'aisles',\n",
       " 'ajar',\n",
       " 'ajdabiya',\n",
       " 'ak',\n",
       " 'akhilleus',\n",
       " 'akin',\n",
       " 'akrokeraunian',\n",
       " 'akutagawa',\n",
       " 'al',\n",
       " 'alabaster',\n",
       " 'alack',\n",
       " 'alamo',\n",
       " 'alan',\n",
       " 'alarm',\n",
       " 'alarmed',\n",
       " 'alarming',\n",
       " 'alarms',\n",
       " 'alarums',\n",
       " 'alas',\n",
       " 'alastor',\n",
       " 'alba',\n",
       " 'albans',\n",
       " 'albany',\n",
       " 'albatross',\n",
       " 'albeit',\n",
       " 'albert',\n",
       " 'alberta',\n",
       " 'albrecht',\n",
       " 'album',\n",
       " 'albums',\n",
       " 'alcala',\n",
       " 'alchemied',\n",
       " 'alchemy',\n",
       " 'alcohol',\n",
       " 'alcoves',\n",
       " 'alden',\n",
       " 'alder',\n",
       " 'alders',\n",
       " 'aldrin',\n",
       " 'ale',\n",
       " 'alembic',\n",
       " 'aleppo',\n",
       " 'alert',\n",
       " 'alexander',\n",
       " 'alexanders',\n",
       " 'alexandrian',\n",
       " 'alfred',\n",
       " 'algebar',\n",
       " 'algebra',\n",
       " 'alhambra',\n",
       " 'ali',\n",
       " 'aliases',\n",
       " 'alibi',\n",
       " 'alice',\n",
       " 'alicia',\n",
       " 'alien',\n",
       " 'aliens',\n",
       " 'alight',\n",
       " 'alighting',\n",
       " 'alights',\n",
       " 'aligned',\n",
       " 'alike',\n",
       " 'alit',\n",
       " 'alive',\n",
       " 'alka',\n",
       " 'all',\n",
       " 'allah',\n",
       " 'allan',\n",
       " 'allay',\n",
       " 'alleged',\n",
       " 'allegiance',\n",
       " 'allegretto',\n",
       " 'allegro',\n",
       " 'alleluias',\n",
       " 'alley',\n",
       " 'alleys',\n",
       " 'alleyway',\n",
       " 'allhallows',\n",
       " 'allied',\n",
       " 'allies',\n",
       " 'alligator',\n",
       " 'allopathic',\n",
       " 'allot',\n",
       " 'allotment',\n",
       " 'allotted',\n",
       " 'allow',\n",
       " 'allowance',\n",
       " 'allowed',\n",
       " 'allows',\n",
       " 'alloy',\n",
       " 'alls',\n",
       " 'allude',\n",
       " 'allure',\n",
       " 'allusion',\n",
       " 'allways',\n",
       " 'ally',\n",
       " 'almanac',\n",
       " 'almanack',\n",
       " 'almandine',\n",
       " 'almightie',\n",
       " 'almighty',\n",
       " 'almond',\n",
       " 'almonds',\n",
       " 'almoners',\n",
       " 'almost',\n",
       " 'alms',\n",
       " 'almshouse',\n",
       " 'aloe',\n",
       " 'aloes',\n",
       " 'aloft',\n",
       " 'alone',\n",
       " 'alonest',\n",
       " 'along',\n",
       " 'alongside',\n",
       " 'aloof',\n",
       " 'aloosa',\n",
       " 'alors',\n",
       " 'alotted',\n",
       " 'aloud',\n",
       " 'alphabet',\n",
       " 'alphabets',\n",
       " 'alpine',\n",
       " 'alps',\n",
       " 'alraschid',\n",
       " 'already',\n",
       " 'alright',\n",
       " 'alsace',\n",
       " 'alsatian',\n",
       " 'also',\n",
       " 'altar',\n",
       " 'altars',\n",
       " 'alter',\n",
       " 'alterations',\n",
       " 'altered',\n",
       " 'altering',\n",
       " 'alternate',\n",
       " 'alternately',\n",
       " 'alternative',\n",
       " 'althea',\n",
       " 'altho',\n",
       " 'although',\n",
       " 'altitude',\n",
       " 'alto',\n",
       " 'altogether',\n",
       " 'alum',\n",
       " 'aluminum',\n",
       " 'alva',\n",
       " 'alway',\n",
       " 'always',\n",
       " 'alyssum',\n",
       " 'alzheimer',\n",
       " 'alzheimers',\n",
       " 'am',\n",
       " 'amain',\n",
       " 'amaranth',\n",
       " 'amaryllis',\n",
       " 'amassing',\n",
       " 'amateur',\n",
       " 'amaze',\n",
       " 'amazed',\n",
       " 'amazement',\n",
       " 'amazing',\n",
       " 'amazingly',\n",
       " 'amazonia',\n",
       " 'amazons',\n",
       " 'ambarvalia',\n",
       " 'ambassadors',\n",
       " 'amber',\n",
       " 'ambers',\n",
       " 'ambiguous',\n",
       " 'ambit',\n",
       " 'ambition',\n",
       " 'ambitions',\n",
       " 'ambitious',\n",
       " 'amble',\n",
       " 'ambling',\n",
       " 'amblongusses',\n",
       " 'ambrosial',\n",
       " 'ambulance',\n",
       " 'ambulation',\n",
       " 'ambush',\n",
       " 'amen',\n",
       " 'amended',\n",
       " 'amendment',\n",
       " 'amends',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americans',\n",
       " 'amerika',\n",
       " 'ametas',\n",
       " 'amethyst',\n",
       " 'amethysts',\n",
       " 'amiable',\n",
       " 'amid',\n",
       " 'amidst',\n",
       " 'amish',\n",
       " 'amiss',\n",
       " 'amity',\n",
       " 'ammonia',\n",
       " 'amnesia',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amor',\n",
       " 'amoretti',\n",
       " 'amorous',\n",
       " 'amorphas',\n",
       " 'amorphous',\n",
       " 'amount',\n",
       " 'amour',\n",
       " 'amp',\n",
       " 'ampersand',\n",
       " 'amphibian',\n",
       " 'amphibians',\n",
       " 'amphictrion',\n",
       " 'amphitheatre',\n",
       " 'ample',\n",
       " 'ampler',\n",
       " 'amplest',\n",
       " 'amptman',\n",
       " 'amsterdam',\n",
       " 'amulets',\n",
       " 'amurath',\n",
       " 'amuse',\n",
       " 'amused',\n",
       " 'amusedly',\n",
       " 'amusement',\n",
       " 'amusing',\n",
       " 'an',\n",
       " 'ana',\n",
       " 'anah',\n",
       " 'anahit',\n",
       " 'analgesia',\n",
       " 'analysis',\n",
       " 'analyze',\n",
       " 'anarchs',\n",
       " 'anarchy',\n",
       " 'anas',\n",
       " 'anathema',\n",
       " 'anathematized',\n",
       " 'anatomical',\n",
       " 'anatomist',\n",
       " 'ance',\n",
       " 'ancestor',\n",
       " 'ancestors',\n",
       " 'ancestral',\n",
       " 'ancestry',\n",
       " 'anchises',\n",
       " 'anchor',\n",
       " 'anchored',\n",
       " 'anchovies',\n",
       " 'ancient',\n",
       " 'ancients',\n",
       " 'and',\n",
       " 'anderson',\n",
       " 'andes',\n",
       " 'andrea',\n",
       " 'andrew',\n",
       " 'anear',\n",
       " 'anecdotes',\n",
       " 'anemone',\n",
       " 'anemones',\n",
       " 'anemonies',\n",
       " 'anew',\n",
       " 'anfractuous',\n",
       " 'ang',\n",
       " 'angel',\n",
       " 'angelheaded',\n",
       " 'angelic',\n",
       " 'angelo',\n",
       " 'angels',\n",
       " 'anger',\n",
       " 'angle',\n",
       " 'angled',\n",
       " 'angler',\n",
       " 'angles',\n",
       " 'anglo',\n",
       " 'angora',\n",
       " 'angrily',\n",
       " 'angry',\n",
       " 'anguish',\n",
       " 'anguished',\n",
       " 'angus',\n",
       " 'animal',\n",
       " 'animals',\n",
       " 'animated',\n",
       " 'anise',\n",
       " 'anither',\n",
       " 'anju',\n",
       " 'ankle',\n",
       " 'ankles',\n",
       " 'anklets',\n",
       " 'ankus',\n",
       " 'ann',\n",
       " 'anna',\n",
       " 'annals',\n",
       " 'annam',\n",
       " 'anne',\n",
       " 'annemeekee',\n",
       " 'annette',\n",
       " 'annexation',\n",
       " 'annihilated',\n",
       " 'annihilating',\n",
       " 'annihilation',\n",
       " 'anniversary',\n",
       " 'anniversary\\x97',\n",
       " 'announce',\n",
       " 'announced',\n",
       " 'announcements',\n",
       " 'announcer',\n",
       " 'announces',\n",
       " 'announcing',\n",
       " 'annulled',\n",
       " 'annulling',\n",
       " 'anoint',\n",
       " 'anoints',\n",
       " 'anon',\n",
       " 'anonymity',\n",
       " 'anonymous',\n",
       " 'another',\n",
       " 'another\\x92s',\n",
       " 'ansel',\n",
       " 'answer',\n",
       " 'answered',\n",
       " 'answerer',\n",
       " 'answering',\n",
       " 'answers',\n",
       " 'ant',\n",
       " 'antarctica',\n",
       " 'antechamber',\n",
       " 'antedated',\n",
       " 'antenna',\n",
       " 'antennae',\n",
       " 'antennas',\n",
       " 'anthill',\n",
       " 'anthology',\n",
       " 'anthropology',\n",
       " 'anti',\n",
       " 'antibody',\n",
       " 'anticipate',\n",
       " 'anticipates',\n",
       " 'anticipating',\n",
       " 'anticipation',\n",
       " 'antics',\n",
       " 'antidotes',\n",
       " 'antient',\n",
       " 'antioch',\n",
       " 'antique',\n",
       " 'antiquity',\n",
       " 'antler',\n",
       " 'antlers',\n",
       " 'antofagastas',\n",
       " 'antoine',\n",
       " 'antonius',\n",
       " 'antony',\n",
       " 'antonys',\n",
       " 'ants',\n",
       " 'ants’',\n",
       " 'antwerp',\n",
       " 'anvil',\n",
       " 'anxieties',\n",
       " 'anxiety',\n",
       " 'anxious',\n",
       " 'anxiously',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anyhow',\n",
       " 'anymore',\n",
       " 'anyoldhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'apace',\n",
       " 'apart',\n",
       " 'apartment',\n",
       " 'ape',\n",
       " 'apennines',\n",
       " 'aperture',\n",
       " 'apes',\n",
       " 'aphids',\n",
       " 'apocalypse',\n",
       " 'apogee',\n",
       " 'apollinax',\n",
       " 'apollos',\n",
       " 'apologetically',\n",
       " 'apologies',\n",
       " 'apologizes',\n",
       " 'apologizing',\n",
       " 'apology',\n",
       " 'apology\\x97',\n",
       " 'apostate',\n",
       " 'apostle',\n",
       " 'apostles',\n",
       " 'app',\n",
       " 'appalachian',\n",
       " 'appalled',\n",
       " 'appalls',\n",
       " 'appanage',\n",
       " 'apparel',\n",
       " 'apparelled',\n",
       " 'apparently',\n",
       " 'apparition',\n",
       " 'apparitions',\n",
       " 'appartment',\n",
       " 'appeal',\n",
       " 'appealing',\n",
       " 'appear',\n",
       " 'appearance',\n",
       " 'appearances',\n",
       " 'appeared',\n",
       " 'appearing',\n",
       " 'appears',\n",
       " 'appease',\n",
       " 'appeased',\n",
       " 'appendix',\n",
       " 'appetite',\n",
       " 'appetites',\n",
       " 'applauding',\n",
       " 'applause',\n",
       " 'apple',\n",
       " 'appleblooms',\n",
       " 'applemint',\n",
       " 'apples',\n",
       " 'applesauce',\n",
       " 'apples\\x97',\n",
       " 'applewood',\n",
       " 'apple\\x97',\n",
       " 'appliance',\n",
       " 'applies',\n",
       " 'apply',\n",
       " 'appoint',\n",
       " 'appointed',\n",
       " 'appointment',\n",
       " 'apportioned',\n",
       " 'appreciate',\n",
       " 'appreciations',\n",
       " 'appreciative',\n",
       " 'apprehended',\n",
       " 'apprehends',\n",
       " 'apprehension',\n",
       " 'apprentice',\n",
       " 'approach',\n",
       " 'approached',\n",
       " 'approaches',\n",
       " 'approaching',\n",
       " 'approbation',\n",
       " 'appropriate',\n",
       " 'approval',\n",
       " 'approve',\n",
       " 'approved',\n",
       " 'apricot',\n",
       " 'apricots',\n",
       " 'april',\n",
       " 'aproned',\n",
       " 'apt',\n",
       " 'aptly',\n",
       " 'aqua',\n",
       " 'aquajogging',\n",
       " 'aquarium',\n",
       " 'aqueduct',\n",
       " 'aquinas',\n",
       " 'ar',\n",
       " 'arab',\n",
       " 'arabesqued',\n",
       " 'arabesques',\n",
       " 'arabia',\n",
       " 'arabic',\n",
       " 'araby',\n",
       " 'arak',\n",
       " 'ararat',\n",
       " 'arb',\n",
       " 'arbiter',\n",
       " 'arbitrement',\n",
       " 'arbor',\n",
       " 'arbors',\n",
       " 'arbour',\n",
       " 'arc',\n",
       " 'arcadia',\n",
       " 'arcadian',\n",
       " 'arcady',\n",
       " 'arcane',\n",
       " 'arch',\n",
       " 'archaic',\n",
       " 'archbishop',\n",
       " 'arched',\n",
       " 'archer',\n",
       " 'archers',\n",
       " 'archery',\n",
       " 'arches',\n",
       " 'arching',\n",
       " 'archipelago',\n",
       " 'architectures',\n",
       " 'archive',\n",
       " 'archly',\n",
       " 'archy',\n",
       " 'arctic',\n",
       " 'ardennes',\n",
       " 'ardent',\n",
       " 'ardently',\n",
       " 'ardor',\n",
       " 'ardour',\n",
       " 'are',\n",
       " 'area',\n",
       " 'areas',\n",
       " 'aren',\n",
       " 'arena',\n",
       " 'argent',\n",
       " 'argot',\n",
       " 'argue',\n",
       " 'arguing',\n",
       " 'argument',\n",
       " 'arguments',\n",
       " 'argus',\n",
       " 'ari',\n",
       " 'aria',\n",
       " 'ariadne',\n",
       " 'arid',\n",
       " 'aright',\n",
       " 'ariosto',\n",
       " 'arise',\n",
       " 'arisen',\n",
       " 'arises',\n",
       " 'aristocrats',\n",
       " 'arizona',\n",
       " 'ark',\n",
       " 'arlington',\n",
       " 'arm',\n",
       " 'armada',\n",
       " 'armadillo',\n",
       " 'armchair',\n",
       " 'armchairs',\n",
       " 'armed',\n",
       " 'armenian',\n",
       " 'armful',\n",
       " 'armies',\n",
       " 'arming',\n",
       " 'armistice',\n",
       " 'armless',\n",
       " 'armload',\n",
       " 'armor',\n",
       " 'armored',\n",
       " 'armour',\n",
       " 'armoured',\n",
       " 'armoury',\n",
       " 'arms',\n",
       " 'armsful',\n",
       " 'army',\n",
       " 'arn',\n",
       " 'arno',\n",
       " 'arnold',\n",
       " 'arnolds',\n",
       " 'aroma',\n",
       " 'aroon',\n",
       " 'arose',\n",
       " 'around',\n",
       " 'aroused',\n",
       " 'arousing',\n",
       " 'arpeggios',\n",
       " 'arprobus',\n",
       " 'arrang',\n",
       " 'arrange',\n",
       " 'arranged',\n",
       " 'arrangement',\n",
       " 'arrangements',\n",
       " 'arranges',\n",
       " 'arras',\n",
       " 'array',\n",
       " 'arrayed',\n",
       " 'arrest',\n",
       " 'arrival',\n",
       " 'arrive',\n",
       " 'arrived',\n",
       " 'arrives',\n",
       " 'arriving',\n",
       " 'arrogance',\n",
       " 'arrogant',\n",
       " 'arrow',\n",
       " 'arrows',\n",
       " 'arroyo',\n",
       " 'arsenal',\n",
       " 'arsonist',\n",
       " 'art',\n",
       " 'arteries',\n",
       " 'arthritis',\n",
       " 'arthur',\n",
       " 'artic',\n",
       " 'artichoke',\n",
       " 'artichokes',\n",
       " 'article',\n",
       " 'articles',\n",
       " 'articulate',\n",
       " 'artifice',\n",
       " 'artificer',\n",
       " 'artificial',\n",
       " 'artillery',\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  421608\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 3\n",
    "wordX = []\n",
    "wordY = []\n",
    "for i in range(0, n_words - seq_length, 1):\n",
    "    seq_in = corpuswords_raw[i:i + seq_length]\n",
    "    seq_out = corpuswords_raw[i + seq_length]\n",
    "    wordX.append([word_to_int[word] if word != '' else '' for word in seq_in])\n",
    "    wordY.append(word_to_int[seq_out] if seq_out != '' else '')\n",
    "n_wordpatterns = len(wordX)\n",
    "print(\"Total Patterns: \", n_wordpatterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<nEXt>', 'the', '<nEXt>']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[14233, 5760, 1],\n",
       " [5760, 1, 2],\n",
       " [1, 2, 20322],\n",
       " [2, 20322, 8895],\n",
       " [20322, 8895, 7067],\n",
       " [8895, 7067, 1],\n",
       " [7067, 1, 25939],\n",
       " [1, 25939, 5379],\n",
       " [25939, 5379, 0],\n",
       " [5379, 0, 21758],\n",
       " [0, 21758, 18188],\n",
       " [21758, 18188, 1],\n",
       " [18188, 1, 1131],\n",
       " [1, 1131, 23284],\n",
       " [1131, 23284, 6611],\n",
       " [23284, 6611, 21565],\n",
       " [6611, 21565, 1],\n",
       " [21565, 1, 11425],\n",
       " [1, 11425, 23356],\n",
       " [11425, 23356, 15642],\n",
       " [23356, 15642, 13227],\n",
       " [15642, 13227, 0],\n",
       " [13227, 0, 21758],\n",
       " [0, 21758, 14713],\n",
       " [21758, 14713, 1],\n",
       " [14713, 1, 2],\n",
       " [1, 2, 19086],\n",
       " [2, 19086, 1541],\n",
       " [19086, 1541, 8895],\n",
       " [1541, 8895, 1],\n",
       " [8895, 1, 2714],\n",
       " [1, 2714, 0],\n",
       " [2714, 0, 22563],\n",
       " [0, 22563, 366],\n",
       " [22563, 366, 1],\n",
       " [366, 1, 700],\n",
       " [1, 700, 15693],\n",
       " [700, 15693, 13835],\n",
       " [15693, 13835, 13282],\n",
       " [13835, 13282, 1],\n",
       " [13282, 1, 15152],\n",
       " [1, 15152, 23284],\n",
       " [15152, 23284, 13769],\n",
       " [23284, 13769, 0],\n",
       " [13769, 0, 4029],\n",
       " [0, 4029, 5780],\n",
       " [4029, 5780, 14106],\n",
       " [5780, 14106, 1],\n",
       " [14106, 1, 11189],\n",
       " [1, 11189, 19276],\n",
       " [11189, 19276, 1],\n",
       " [19276, 1, 11651],\n",
       " [1, 11651, 23284],\n",
       " [11651, 23284, 20349],\n",
       " [23284, 20349, 0],\n",
       " [20349, 0, 18107],\n",
       " [0, 18107, 23580],\n",
       " [18107, 23580, 1],\n",
       " [23580, 1, 23284],\n",
       " [1, 23284, 21136],\n",
       " [23284, 21136, 15642],\n",
       " [21136, 15642, 18188],\n",
       " [15642, 18188, 1],\n",
       " [18188, 1, 11651],\n",
       " [1, 11651, 23284],\n",
       " [11651, 23284, 13379],\n",
       " [23284, 13379, 0],\n",
       " [13379, 0, 25654],\n",
       " [0, 25654, 1],\n",
       " [25654, 1, 21090],\n",
       " [1, 21090, 5301],\n",
       " [21090, 5301, 23816],\n",
       " [5301, 23816, 20540],\n",
       " [23816, 20540, 1],\n",
       " [20540, 1, 2],\n",
       " [1, 2, 8090],\n",
       " [2, 8090, 14699],\n",
       " [8090, 14699, 0],\n",
       " [14699, 0, 16001],\n",
       " [0, 16001, 18188],\n",
       " [16001, 18188, 1],\n",
       " [18188, 1, 23284],\n",
       " [1, 23284, 19849],\n",
       " [23284, 19849, 15642],\n",
       " [19849, 15642, 15803],\n",
       " [15642, 15803, 2347],\n",
       " [15803, 2347, 1],\n",
       " [2347, 1, 11651],\n",
       " [1, 11651, 2],\n",
       " [11651, 2, 6134],\n",
       " [2, 6134, 23827],\n",
       " [6134, 23827, 0],\n",
       " [23827, 0, 14570],\n",
       " [0, 14570, 22513],\n",
       " [14570, 22513, 18188],\n",
       " [22513, 18188, 1],\n",
       " [18188, 1, 3210],\n",
       " [1, 3210, 16755],\n",
       " [3210, 16755, 1],\n",
       " [16755, 1, 11651],\n",
       " [1, 11651, 26591],\n",
       " [11651, 26591, 23155],\n",
       " [26591, 23155, 0],\n",
       " [23155, 0, 5760],\n",
       " [0, 5760, 12161],\n",
       " [5760, 12161, 6672],\n",
       " [12161, 6672, 1],\n",
       " [6672, 1, 17300],\n",
       " [1, 17300, 664],\n",
       " [17300, 664, 1],\n",
       " [664, 1, 23284],\n",
       " [1, 23284, 7056],\n",
       " [23284, 7056, 10000],\n",
       " [7056, 10000, 0],\n",
       " [10000, 0, 25607],\n",
       " [0, 25607, 4287],\n",
       " [25607, 4287, 1],\n",
       " [4287, 1, 23284],\n",
       " [1, 23284, 25939],\n",
       " [23284, 25939, 16712],\n",
       " [25939, 16712, 1],\n",
       " [16712, 1, 15642],\n",
       " [1, 15642, 2],\n",
       " [15642, 2, 5441],\n",
       " [2, 5441, 5355],\n",
       " [5441, 5355, 0],\n",
       " [5355, 0, 14842],\n",
       " [0, 14842, 22227],\n",
       " [14842, 22227, 1],\n",
       " [22227, 1, 24315],\n",
       " [1, 24315, 25774],\n",
       " [24315, 25774, 16986],\n",
       " [25774, 16986, 19263],\n",
       " [16986, 19263, 1],\n",
       " [19263, 1, 23284],\n",
       " [1, 23284, 16378],\n",
       " [23284, 16378, 11067],\n",
       " [16378, 11067, 0],\n",
       " [11067, 0, 15317],\n",
       " [0, 15317, 20465],\n",
       " [15317, 20465, 1],\n",
       " [20465, 1, 11651],\n",
       " [1, 11651, 23284],\n",
       " [11651, 23284, 16281],\n",
       " [23284, 16281, 13501],\n",
       " [16281, 13501, 3353],\n",
       " [13501, 3353, 13209],\n",
       " [3353, 13209, 1],\n",
       " [13209, 1, 6317],\n",
       " [1, 6317, 15152],\n",
       " [6317, 15152, 14753],\n",
       " [15152, 14753, 0],\n",
       " [14753, 0, 2],\n",
       " [0, 2, 26020],\n",
       " [2, 26020, 25284],\n",
       " [26020, 25284, 1],\n",
       " [25284, 1, 15722],\n",
       " [1, 15722, 23284],\n",
       " [15722, 23284, 22563],\n",
       " [23284, 22563, 10942],\n",
       " [22563, 10942, 1],\n",
       " [10942, 1, 15407],\n",
       " [1, 15407, 23580],\n",
       " [15407, 23580, 15085],\n",
       " [23580, 15085, 0],\n",
       " [15085, 0, 2],\n",
       " [0, 2, 22563],\n",
       " [2, 22563, 5760],\n",
       " [22563, 5760, 1],\n",
       " [5760, 1, 16912],\n",
       " [1, 16912, 10635],\n",
       " [16912, 10635, 15642],\n",
       " [10635, 15642, 23284],\n",
       " [15642, 23284, 3815],\n",
       " [23284, 3815, 2347],\n",
       " [3815, 2347, 1],\n",
       " [2347, 1, 15968],\n",
       " [1, 15968, 23284],\n",
       " [15968, 23284, 10942],\n",
       " [23284, 10942, 0],\n",
       " [10942, 0, 17233],\n",
       " [0, 17233, 15535],\n",
       " [17233, 15535, 1],\n",
       " [15535, 1, 23284],\n",
       " [1, 23284, 9131],\n",
       " [23284, 9131, 19512],\n",
       " [9131, 19512, 20322],\n",
       " [19512, 20322, 7470],\n",
       " [20322, 7470, 1],\n",
       " [7470, 1, 11651],\n",
       " [1, 11651, 4386],\n",
       " [11651, 4386, 8816],\n",
       " [4386, 8816, 0],\n",
       " [8816, 0, 15376],\n",
       " [0, 15376, 10848],\n",
       " [15376, 10848, 1],\n",
       " [10848, 1, 2],\n",
       " [1, 2, 22892],\n",
       " [2, 22892, 11651],\n",
       " [22892, 11651, 23284],\n",
       " [11651, 23284, 14842],\n",
       " [23284, 14842, 1],\n",
       " [14842, 1, 21578],\n",
       " [1, 21578, 26162],\n",
       " [21578, 26162, 16712],\n",
       " [26162, 16712, 0],\n",
       " [16712, 0, 8538],\n",
       " [0, 8538, 5738],\n",
       " [8538, 5738, 1],\n",
       " [5738, 1, 15439],\n",
       " [1, 15439, 7785],\n",
       " [15439, 7785, 15453],\n",
       " [7785, 15453, 1],\n",
       " [15453, 1, 23284],\n",
       " [1, 23284, 15267],\n",
       " [23284, 15267, 14699],\n",
       " [15267, 14699, 0],\n",
       " [14699, 0, 25066],\n",
       " [0, 25066, 6291],\n",
       " [25066, 6291, 1],\n",
       " [6291, 1, 9717],\n",
       " [1, 9717, 9512],\n",
       " [9717, 9512, 20282],\n",
       " [9512, 20282, 15738],\n",
       " [20282, 15738, 1],\n",
       " [15738, 1, 23293],\n",
       " [1, 23293, 18562],\n",
       " [23293, 18562, 0],\n",
       " [18562, 0, 6532],\n",
       " [0, 6532, 10613],\n",
       " [6532, 10613, 1],\n",
       " [10613, 1, 2],\n",
       " [1, 2, 10278],\n",
       " [2, 10278, 15642],\n",
       " [10278, 15642, 3815],\n",
       " [15642, 3815, 16712],\n",
       " [3815, 16712, 1],\n",
       " [16712, 1, 5384],\n",
       " [1, 5384, 23284],\n",
       " [5384, 23284, 12959],\n",
       " [23284, 12959, 0],\n",
       " [12959, 0, 15803],\n",
       " [0, 15803, 22566],\n",
       " [15803, 22566, 16459],\n",
       " [22566, 16459, 23466],\n",
       " [16459, 23466, 1],\n",
       " [23466, 1, 23284],\n",
       " [1, 23284, 24178],\n",
       " [23284, 24178, 730],\n",
       " [24178, 730, 26133],\n",
       " [730, 26133, 15642],\n",
       " [26133, 15642, 8222],\n",
       " [15642, 8222, 19512],\n",
       " [8222, 19512, 1],\n",
       " [19512, 1, 13176],\n",
       " [1, 13176, 22621],\n",
       " [13176, 22621, 13685],\n",
       " [22621, 13685, 0],\n",
       " [13685, 0, 14708],\n",
       " [0, 14708, 1],\n",
       " [14708, 1, 21292],\n",
       " [1, 21292, 13897],\n",
       " [21292, 13897, 25671],\n",
       " [13897, 25671, 1],\n",
       " [25671, 1, 11425],\n",
       " [1, 11425, 25518],\n",
       " [11425, 25518, 23652],\n",
       " [25518, 23652, 23800],\n",
       " [23652, 23800, 26541],\n",
       " [23800, 26541, 0],\n",
       " [26541, 0, 7097],\n",
       " [0, 7097, 5760],\n",
       " [7097, 5760, 1],\n",
       " [5760, 1, 7785],\n",
       " [1, 7785, 23284],\n",
       " [7785, 23284, 20327],\n",
       " [23284, 20327, 1241],\n",
       " [20327, 1241, 7018],\n",
       " [1241, 7018, 1],\n",
       " [7018, 1, 21136],\n",
       " [1, 21136, 10064],\n",
       " [21136, 10064, 0],\n",
       " [10064, 0, 21292],\n",
       " [0, 21292, 4419],\n",
       " [21292, 4419, 1],\n",
       " [4419, 1, 2],\n",
       " [1, 2, 9873],\n",
       " [2, 9873, 11107],\n",
       " [9873, 11107, 12205],\n",
       " [11107, 12205, 25666],\n",
       " [12205, 25666, 1],\n",
       " [25666, 1, 182],\n",
       " [1, 182, 23284],\n",
       " [182, 23284, 15317],\n",
       " [23284, 15317, 20940],\n",
       " [15317, 20940, 0],\n",
       " [20940, 0, 5655],\n",
       " [0, 5655, 22215],\n",
       " [5655, 22215, 1],\n",
       " [22215, 1, 23284],\n",
       " [1, 23284, 5760],\n",
       " [23284, 5760, 15693],\n",
       " [5760, 15693, 4466],\n",
       " [15693, 4466, 14306],\n",
       " [4466, 14306, 1],\n",
       " [14306, 1, 3114],\n",
       " [1, 3114, 23284],\n",
       " [3114, 23284, 18154],\n",
       " [23284, 18154, 0],\n",
       " [18154, 0, 17851],\n",
       " [0, 17851, 1],\n",
       " [17851, 1, 1241],\n",
       " [1, 1241, 23284],\n",
       " [1241, 23284, 26068],\n",
       " [23284, 26068, 1],\n",
       " [26068, 1, 26057],\n",
       " [1, 26057, 0],\n",
       " [26057, 0, 11651],\n",
       " [0, 11651, 26111],\n",
       " [11651, 26111, 19512],\n",
       " [26111, 19512, 26057],\n",
       " [19512, 26057, 1],\n",
       " [26057, 1, 23284],\n",
       " [1, 23284, 3200],\n",
       " [23284, 3200, 15642],\n",
       " [3200, 15642, 2],\n",
       " [15642, 2, 9175],\n",
       " [2, 9175, 1],\n",
       " [9175, 1, 25965],\n",
       " [1, 25965, 15474],\n",
       " [25965, 15474, 10553],\n",
       " [15474, 10553, 3266],\n",
       " [10553, 3266, 0],\n",
       " [3266, 0, 18208],\n",
       " [0, 18208, 2770],\n",
       " [18208, 2770, 1],\n",
       " [2770, 1, 23284],\n",
       " [1, 23284, 19201],\n",
       " [23284, 19201, 8774],\n",
       " [19201, 8774, 1],\n",
       " [8774, 1, 8207],\n",
       " [1, 8207, 0],\n",
       " [8207, 0, 21758],\n",
       " [0, 21758, 2746],\n",
       " [21758, 2746, 1],\n",
       " [2746, 1, 23284],\n",
       " [1, 23284, 1530],\n",
       " [23284, 1530, 19512],\n",
       " [1530, 19512, 20327],\n",
       " [19512, 20327, 1],\n",
       " [20327, 1, 15722],\n",
       " [1, 15722, 15026],\n",
       " [15722, 15026, 2507],\n",
       " [15026, 2507, 0],\n",
       " [2507, 0, 26111],\n",
       " [0, 26111, 6886],\n",
       " [26111, 6886, 1],\n",
       " [6886, 1, 529],\n",
       " [1, 529, 23284],\n",
       " [529, 23284, 16345],\n",
       " [23284, 16345, 8056],\n",
       " [16345, 8056, 13449],\n",
       " [8056, 13449, 1],\n",
       " [13449, 1, 12048],\n",
       " [1, 12048, 23284],\n",
       " [12048, 23284, 3163],\n",
       " [23284, 3163, 0],\n",
       " [3163, 0, 21139],\n",
       " [0, 21139, 15642],\n",
       " [21139, 15642, 21758],\n",
       " [15642, 21758, 1],\n",
       " [21758, 1, 278],\n",
       " [1, 278, 11651],\n",
       " [278, 11651, 23284],\n",
       " [11651, 23284, 14753],\n",
       " [23284, 14753, 437],\n",
       " [14753, 437, 1],\n",
       " [437, 1, 2905],\n",
       " [1, 2905, 24499],\n",
       " [2905, 24499, 11432],\n",
       " [24499, 11432, 0],\n",
       " [11432, 0, 21758],\n",
       " [0, 21758, 14753],\n",
       " [21758, 14753, 1],\n",
       " [14753, 1, 26545],\n",
       " [1, 26545, 10406],\n",
       " [26545, 10406, 15722],\n",
       " [10406, 15722, 15026],\n",
       " [15722, 15026, 2722],\n",
       " [15026, 2722, 1],\n",
       " [2722, 1, 2],\n",
       " [1, 2, 2149],\n",
       " [2, 2149, 0],\n",
       " [2149, 0, 5386],\n",
       " [0, 5386, 1],\n",
       " [5386, 1, 23284],\n",
       " [1, 23284, 2874],\n",
       " [23284, 2874, 4985],\n",
       " [2874, 4985, 1],\n",
       " [4985, 1, 15642],\n",
       " [1, 15642, 700],\n",
       " [15642, 700, 926],\n",
       " [700, 926, 0],\n",
       " [926, 0, 11213],\n",
       " [0, 11213, 366],\n",
       " [11213, 366, 1],\n",
       " [366, 1, 2],\n",
       " [1, 2, 8713],\n",
       " [2, 8713, 5122],\n",
       " [8713, 5122, 1],\n",
       " [5122, 1, 11651],\n",
       " [1, 11651, 23284],\n",
       " [11651, 23284, 14233],\n",
       " [23284, 14233, 19512],\n",
       " [14233, 19512, 20319],\n",
       " [19512, 20319, 0],\n",
       " [20319, 0, 12902],\n",
       " [0, 12902, 18496],\n",
       " [12902, 18496, 11651],\n",
       " [18496, 11651, 23284],\n",
       " [11651, 23284, 20940],\n",
       " [23284, 20940, 1],\n",
       " [20940, 1, 2],\n",
       " [1, 2, 21117],\n",
       " [2, 21117, 9644],\n",
       " [21117, 9644, 19512],\n",
       " [9644, 19512, 14699],\n",
       " [19512, 14699, 8052],\n",
       " [14699, 8052, 19186],\n",
       " [8052, 19186, 1],\n",
       " [19186, 1, 15968],\n",
       " [1, 15968, 23284],\n",
       " [15968, 23284, 5102],\n",
       " [23284, 5102, 0],\n",
       " [5102, 0, 5760],\n",
       " [0, 5760, 14699],\n",
       " [5760, 14699, 1],\n",
       " [14699, 1, 23284],\n",
       " [1, 23284, 26208],\n",
       " [23284, 26208, 26162],\n",
       " [26208, 26162, 20764],\n",
       " [26162, 20764, 10341],\n",
       " [20764, 10341, 1],\n",
       " [10341, 1, 22043],\n",
       " [1, 22043, 1466],\n",
       " [22043, 1466, 12048],\n",
       " [1466, 12048, 23284],\n",
       " [12048, 23284, 20319],\n",
       " [23284, 20319, 0],\n",
       " [20319, 0, 14753],\n",
       " [0, 14753, 6005],\n",
       " [14753, 6005, 1],\n",
       " [6005, 1, 21250],\n",
       " [1, 21250, 4493],\n",
       " [21250, 4493, 11651],\n",
       " [4493, 11651, 1],\n",
       " [11651, 1, 26162],\n",
       " [1, 26162, 23284],\n",
       " [26162, 23284, 8292],\n",
       " [23284, 8292, 13835],\n",
       " [8292, 13835, 0],\n",
       " [13835, 0, 7121],\n",
       " [0, 7121, 1],\n",
       " [7121, 1, 700],\n",
       " [1, 700, 861],\n",
       " [700, 861, 14807],\n",
       " [861, 14807, 3213],\n",
       " [14807, 3213, 1],\n",
       " [3213, 1, 11651],\n",
       " [1, 11651, 23284],\n",
       " [11651, 23284, 26057],\n",
       " [23284, 26057, 0],\n",
       " [26057, 0, 26065],\n",
       " [0, 26065, 5760],\n",
       " [26065, 5760, 1],\n",
       " [5760, 1, 23284],\n",
       " [1, 23284, 17724],\n",
       " [23284, 17724, 25733],\n",
       " [17724, 25733, 1],\n",
       " [25733, 1, 1241],\n",
       " [1, 1241, 23284],\n",
       " [1241, 23284, 10014],\n",
       " [23284, 10014, 20856],\n",
       " [10014, 20856, 0],\n",
       " [20856, 0, 21758],\n",
       " [0, 21758, 5760],\n",
       " [21758, 5760, 1],\n",
       " [5760, 1, 4944],\n",
       " [1, 4944, 730],\n",
       " [4944, 730, 10088],\n",
       " [730, 10088, 1],\n",
       " [10088, 1, 5496],\n",
       " [1, 5496, 15642],\n",
       " [5496, 15642, 23077],\n",
       " [15642, 23077, 0],\n",
       " [23077, 0, 10088],\n",
       " [0, 10088, 23665],\n",
       " [10088, 23665, 1],\n",
       " [23665, 1, 23284],\n",
       " [1, 23284, 25654],\n",
       " [23284, 25654, 19483],\n",
       " [25654, 19483, 1],\n",
       " [19483, 1, 14236],\n",
       " [1, 14236, 0],\n",
       " [14236, 0, 21758],\n",
       " [0, 21758, 7635],\n",
       " [21758, 7635, 1],\n",
       " [7635, 1, 17457],\n",
       " [1, 17457, 23284],\n",
       " [17457, 23284, 720],\n",
       " [23284, 720, 10014],\n",
       " [720, 10014, 1],\n",
       " [10014, 1, 11651],\n",
       " [1, 11651, 4419],\n",
       " [11651, 4419, 18188],\n",
       " [4419, 18188, 0],\n",
       " [18188, 0, 8847],\n",
       " [0, 8847, 3652],\n",
       " [8847, 3652, 1],\n",
       " [3652, 1, 23284],\n",
       " [1, 23284, 15270],\n",
       " [23284, 15270, 4466],\n",
       " [15270, 4466, 24058],\n",
       " [4466, 24058, 1],\n",
       " [24058, 1, 23652],\n",
       " [1, 23652, 21912],\n",
       " [23652, 21912, 0],\n",
       " [21912, 0, 19836],\n",
       " [0, 19836, 22526],\n",
       " [19836, 22526, 1],\n",
       " [22526, 1, 15728],\n",
       " [1, 15728, 3845],\n",
       " [15728, 3845, 1],\n",
       " [3845, 1, 13512],\n",
       " [1, 13512, 23263],\n",
       " [13512, 23263, 23284],\n",
       " [23263, 23284, 18925],\n",
       " [23284, 18925, 0],\n",
       " [18925, 0, 15875],\n",
       " [0, 15875, 4218],\n",
       " [15875, 4218, 1],\n",
       " [4218, 1, 2],\n",
       " [1, 2, 23431],\n",
       " [2, 23431, 13061],\n",
       " [23431, 13061, 3652],\n",
       " [13061, 3652, 1],\n",
       " [3652, 1, 15953],\n",
       " [1, 15953, 23284],\n",
       " [15953, 23284, 7580],\n",
       " [23284, 7580, 0],\n",
       " [7580, 0, 23284],\n",
       " [0, 23284, 17007],\n",
       " [23284, 17007, 19512],\n",
       " [17007, 19512, 12832],\n",
       " [19512, 12832, 13209],\n",
       " [12832, 13209, 1],\n",
       " [13209, 1, 2],\n",
       " [1, 2, 25646],\n",
       " [2, 25646, 15642],\n",
       " [25646, 15642, 1648],\n",
       " [15642, 1648, 1],\n",
       " [1648, 1, 22774],\n",
       " [1, 22774, 23284],\n",
       " [22774, 23284, 15216],\n",
       " [23284, 15216, 0],\n",
       " [15216, 0, 23284],\n",
       " [0, 23284, 25666],\n",
       " [23284, 25666, 10000],\n",
       " [25666, 10000, 16325],\n",
       " [10000, 16325, 1],\n",
       " [16325, 1, 1131],\n",
       " [1, 1131, 23284],\n",
       " [1131, 23284, 16755],\n",
       " [23284, 16755, 16344],\n",
       " [16755, 16344, 1],\n",
       " [16344, 1, 21758],\n",
       " [1, 21758, 19512],\n",
       " [21758, 19512, 7451],\n",
       " [19512, 7451, 0],\n",
       " [7451, 0, 1241],\n",
       " [0, 1241, 10357],\n",
       " [1241, 10357, 14032],\n",
       " [10357, 14032, 1],\n",
       " [14032, 1, 2],\n",
       " [1, 2, 3094],\n",
       " [2, 3094, 16344],\n",
       " [3094, 16344, 3114],\n",
       " [16344, 3114, 1],\n",
       " [3114, 1, 23284],\n",
       " [1, 23284, 22076],\n",
       " [23284, 22076, 8584],\n",
       " [22076, 8584, 0],\n",
       " [8584, 0, 10357],\n",
       " [0, 10357, 1161],\n",
       " [10357, 1161, 1],\n",
       " [1161, 1, 23284],\n",
       " [1, 23284, 19075],\n",
       " [23284, 19075, 15642],\n",
       " [19075, 15642, 23284],\n",
       " [15642, 23284, 1],\n",
       " [23284, 1, 18409],\n",
       " [1, 18409, 18188],\n",
       " [18409, 18188, 0],\n",
       " [18188, 0, 7787],\n",
       " [0, 7787, 20268],\n",
       " [7787, 20268, 1],\n",
       " [20268, 1, 13193],\n",
       " [1, 13193, 1789],\n",
       " [13193, 1789, 1],\n",
       " [1789, 1, 12048],\n",
       " [1, 12048, 23284],\n",
       " [12048, 23284, 15317],\n",
       " [23284, 15317, 0],\n",
       " [15317, 0, 25607],\n",
       " [0, 25607, 22513],\n",
       " [25607, 22513, 4287],\n",
       " [22513, 4287, 1],\n",
       " [4287, 1, 23411],\n",
       " [1, 23411, 20373],\n",
       " [23411, 20373, 730],\n",
       " [20373, 730, 1],\n",
       " [730, 1, 18871],\n",
       " [1, 18871, 0],\n",
       " [18871, 0, 9512],\n",
       " [0, 9512, 11106],\n",
       " [9512, 11106, 1],\n",
       " [11106, 1, 3871],\n",
       " [1, 3871, 11651],\n",
       " [3871, 11651, 23284],\n",
       " [11651, 23284, 1],\n",
       " [23284, 1, 1730],\n",
       " [1, 1730, 17059],\n",
       " [1730, 17059, 0],\n",
       " [17059, 0, 18125],\n",
       " [0, 18125, 23869],\n",
       " [18125, 23869, 1],\n",
       " [23869, 1, 15447],\n",
       " [1, 15447, 3079],\n",
       " [15447, 3079, 7096],\n",
       " [3079, 7096, 1],\n",
       " [7096, 1, 52],\n",
       " [1, 52, 23284],\n",
       " [52, 23284, 21250],\n",
       " [23284, 21250, 0],\n",
       " [21250, 0, 12910],\n",
       " [0, 12910, 15317],\n",
       " [12910, 15317, 1241],\n",
       " [15317, 1241, 23284],\n",
       " [1241, 23284, 15657],\n",
       " [23284, 15657, 1],\n",
       " [15657, 1, 700],\n",
       " [1, 700, 7915],\n",
       " [700, 7915, 20721],\n",
       " [7915, 20721, 8727],\n",
       " [20721, 8727, 1],\n",
       " [8727, 1, 15285],\n",
       " [1, 15285, 23652],\n",
       " [15285, 23652, 23284],\n",
       " [23652, 23284, 14699],\n",
       " [23284, 14699, 0],\n",
       " [14699, 0, 3581],\n",
       " [0, 3581, 10527],\n",
       " [3581, 10527, 1],\n",
       " [10527, 1, 23284],\n",
       " [1, 23284, 8538],\n",
       " [23284, 8538, 18196],\n",
       " [8538, 18196, 23800],\n",
       " [18196, 23800, 1],\n",
       " [23800, 1, 15026],\n",
       " [1, 15026, 3784],\n",
       " [15026, 3784, 0],\n",
       " [3784, 0, 23865],\n",
       " [0, 23865, 12254],\n",
       " [23865, 12254, 1],\n",
       " [12254, 1, 23431],\n",
       " [1, 23431, 19555],\n",
       " [23431, 19555, 19247],\n",
       " [19555, 19247, 14240],\n",
       " [19247, 14240, 14949],\n",
       " [14240, 14949, 15527],\n",
       " [14949, 15527, 1],\n",
       " [15527, 1, 11651],\n",
       " [1, 11651, 23293],\n",
       " [11651, 23293, 22707],\n",
       " [23293, 22707, 0],\n",
       " [22707, 0, 25038],\n",
       " [0, 25038, 19512],\n",
       " [25038, 19512, 5760],\n",
       " [19512, 5760, 1],\n",
       " [5760, 1, 2],\n",
       " [1, 2, 9002],\n",
       " [2, 9002, 1],\n",
       " [9002, 1, 26176],\n",
       " [1, 26176, 2160],\n",
       " [26176, 2160, 0],\n",
       " [2160, 0, 7088],\n",
       " [0, 7088, 21758],\n",
       " [7088, 21758, 437],\n",
       " [21758, 437, 1],\n",
       " [437, 1, 529],\n",
       " [1, 529, 5760],\n",
       " [529, 5760, 13436],\n",
       " [5760, 13436, 1],\n",
       " [13436, 1, 15904],\n",
       " [1, 15904, 1850],\n",
       " [15904, 1850, 15439],\n",
       " [1850, 15439, 13709],\n",
       " [15439, 13709, 0],\n",
       " [13709, 0, 11370],\n",
       " [0, 11370, 1408],\n",
       " [11370, 1408, 1],\n",
       " [1408, 1, 25580],\n",
       " [1, 25580, 10983],\n",
       " [25580, 10983, 21322],\n",
       " [10983, 21322, 1],\n",
       " [21322, 1, 3114],\n",
       " [1, 3114, 10406],\n",
       " [3114, 10406, 0],\n",
       " [10406, 0, 10832],\n",
       " [0, 10832, 9153],\n",
       " [10832, 9153, 8772],\n",
       " [9153, 8772, 1],\n",
       " [8772, 1, 10983],\n",
       " [1, 10983, 9153],\n",
       " [10983, 9153, 10014],\n",
       " [9153, 10014, 1],\n",
       " [10014, 1, 25038],\n",
       " [1, 25038, 19512],\n",
       " [25038, 19512, 5760],\n",
       " [19512, 5760, 0],\n",
       " [5760, 0, 25039],\n",
       " [0, 25039, 1],\n",
       " [25039, 1, 23284],\n",
       " [1, 23284, 13772],\n",
       " [23284, 13772, 19512],\n",
       " [13772, 19512, 22043],\n",
       " [19512, 22043, 1],\n",
       " [22043, 1, 2709],\n",
       " [1, 2709, 23284],\n",
       " [2709, 23284, 11432],\n",
       " [23284, 11432, 0],\n",
       " [11432, 0, 15736],\n",
       " [0, 15736, 24315],\n",
       " [15736, 24315, 16901],\n",
       " [24315, 16901, 1],\n",
       " [16901, 1, 15722],\n",
       " [1, 15722, 23284],\n",
       " [15722, 23284, 16278],\n",
       " [23284, 16278, 1983],\n",
       " [16278, 1983, 1],\n",
       " [1983, 1, 25038],\n",
       " [1, 25038, 19512],\n",
       " [25038, 19512, 5760],\n",
       " [19512, 5760, 0],\n",
       " [5760, 0, 12803],\n",
       " [0, 12803, 1],\n",
       " [12803, 1, 1973],\n",
       " [1, 1973, 23284],\n",
       " [1973, 23284, 7232],\n",
       " [23284, 7232, 1],\n",
       " [7232, 1, 2],\n",
       " [1, 2, 4066],\n",
       " [2, 4066, 15642],\n",
       " [4066, 15642, 13209],\n",
       " [15642, 13209, 0],\n",
       " [13209, 0, 10744],\n",
       " [0, 10744, 2160],\n",
       " [10744, 2160, 1],\n",
       " [2160, 1, 23431],\n",
       " [1, 23431, 2639],\n",
       " [23431, 2639, 20376],\n",
       " [2639, 20376, 2],\n",
       " [20376, 2, 21697],\n",
       " [2, 21697, 1],\n",
       " [21697, 1, 3114],\n",
       " [1, 3114, 23284],\n",
       " [3114, 23284, 19870],\n",
       " [23284, 19870, 9469],\n",
       " [19870, 9469, 0],\n",
       " [9469, 0, 360],\n",
       " [0, 360, 23284],\n",
       " [360, 23284, 18188],\n",
       " [23284, 18188, 1],\n",
       " [18188, 1, 14699],\n",
       " [1, 14699, 730],\n",
       " [14699, 730, 21942],\n",
       " [730, 21942, 1],\n",
       " [21942, 1, 24507],\n",
       " [1, 24507, 0],\n",
       " [24507, 0, 7051],\n",
       " [0, 7051, 2496],\n",
       " [7051, 2496, 1],\n",
       " [2496, 1, 2],\n",
       " [1, 2, 20532],\n",
       " [2, 20532, 21922],\n",
       " [20532, 21922, 1],\n",
       " [21922, 1, 15152],\n",
       " [1, 15152, 15860],\n",
       " [15152, 15860, 0],\n",
       " [15860, 0, 4419],\n",
       " [0, 4419, 15317],\n",
       " [4419, 15317, 1],\n",
       " [15317, 1, 2],\n",
       " [1, 2, 17007],\n",
       " [2, 17007, 2308],\n",
       " [17007, 2308, 23466],\n",
       " [2308, 23466, 1],\n",
       " [23466, 1, 15860],\n",
       " [1, 15860, 19512],\n",
       " [15860, 19512, 2615],\n",
       " [19512, 2615, 0],\n",
       " [2615, 0, 5705],\n",
       " [0, 5705, 24003],\n",
       " [5705, 24003, 20747],\n",
       " [24003, 20747, 1],\n",
       " [20747, 1, 13547],\n",
       " [1, 13547, 15722],\n",
       " [13547, 15722, 23284],\n",
       " [15722, 23284, 11161],\n",
       " [23284, 11161, 1],\n",
       " [11161, 1, 15860],\n",
       " [1, 15860, 19186],\n",
       " [15860, 19186, 0],\n",
       " [19186, 0, 19885],\n",
       " [0, 19885, 5760],\n",
       " [19885, 5760, 1],\n",
       " [5760, 1, 23284],\n",
       " [1, 23284, 17713],\n",
       " [23284, 17713, 1899],\n",
       " [17713, 1899, 1],\n",
       " [1899, 1, 26162],\n",
       " [1, 26162, 19195],\n",
       " [26162, 19195, 0],\n",
       " [19195, 0, 23865],\n",
       " [0, 23865, 1029],\n",
       " [23865, 1029, 1],\n",
       " [1029, 1, 23284],\n",
       " [1, 23284, 3231],\n",
       " [23284, 3231, 19512],\n",
       " [3231, 19512, 21222],\n",
       " [19512, 21222, 1],\n",
       " [21222, 1, 12161],\n",
       " [1, 12161, 11570],\n",
       " [12161, 11570, 0],\n",
       " [11570, 0, 26111],\n",
       " [0, 26111, 21942],\n",
       " [26111, 21942, 1],\n",
       " [21942, 1, 22451],\n",
       " [1, 22451, 2],\n",
       " [22451, 2, 25884],\n",
       " [2, 25884, 1],\n",
       " [25884, 1, 15642],\n",
       " [1, 15642, 16627],\n",
       " [15642, 16627, 0],\n",
       " [16627, 0, 23880],\n",
       " [0, 23880, 4488],\n",
       " [23880, 4488, 730],\n",
       " [4488, 730, 9792],\n",
       " [730, 9792, 1],\n",
       " [9792, 1, 3079],\n",
       " [1, 3079, 23284],\n",
       " [3079, 23284, 19630],\n",
       " [23284, 19630, 16584],\n",
       " [19630, 16584, 1],\n",
       " [16584, 1, 6872],\n",
       " [1, 6872, 1241],\n",
       " [6872, 1241, 1614],\n",
       " [1241, 1614, 0],\n",
       " [1614, 0, 8538],\n",
       " [0, 8538, 18196],\n",
       " [8538, 18196, 1],\n",
       " [18196, 1, 23284],\n",
       " [1, 23284, 9141],\n",
       " [23284, 9141, 23876],\n",
       " [9141, 23876, 11425],\n",
       " [23876, 11425, 16337],\n",
       " [11425, 16337, 1],\n",
       " [16337, 1, 24315],\n",
       " [1, 24315, 21968],\n",
       " [24315, 21968, 401],\n",
       " [21968, 401, 0],\n",
       " [401, 0, 20745],\n",
       " [0, 20745, 1],\n",
       " [20745, 1, 2],\n",
       " [1, 2, 20487],\n",
       " [2, 20487, 21250],\n",
       " [20487, 21250, 16378],\n",
       " [21250, 16378, 23652],\n",
       " [16378, 23652, 1],\n",
       " [23652, 1, 2],\n",
       " [1, 2, 16139],\n",
       " [2, 16139, 14699],\n",
       " [16139, 14699, 0],\n",
       " [14699, 0, 23414],\n",
       " [0, 23414, 15642],\n",
       " [23414, 15642, 14418],\n",
       " [15642, 14418, 1],\n",
       " [14418, 1, 1408],\n",
       " [1, 1408, 10832],\n",
       " [1408, 10832, 8538],\n",
       " [10832, 8538, 2169],\n",
       " [8538, 2169, 1],\n",
       " [2169, 1, 9153],\n",
       " [1, 9153, 13852],\n",
       " [9153, 13852, 0],\n",
       " [13852, 0, 22812],\n",
       " [0, 22812, 182],\n",
       " [22812, 182, 1],\n",
       " [182, 1, 23284],\n",
       " [1, 23284, 14710],\n",
       " [23284, 14710, 20940],\n",
       " [14710, 20940, 1],\n",
       " [20940, 1, 2],\n",
       " [1, 2, 10033],\n",
       " [2, 10033, 3422],\n",
       " [10033, 3422, 0],\n",
       " [3422, 0, 10357],\n",
       " [0, 10357, 14699],\n",
       " [10357, 14699, 1],\n",
       " [14699, 1, 5716],\n",
       " [1, 5716, 6871],\n",
       " [5716, 6871, 9220],\n",
       " [6871, 9220, 700],\n",
       " [9220, 700, 1],\n",
       " [700, 1, 463],\n",
       " [1, 463, 23085],\n",
       " [463, 23085, 0],\n",
       " [23085, 0, 8725],\n",
       " [0, 8725, 1],\n",
       " [8725, 1, 6751],\n",
       " [1, 6751, 19201],\n",
       " [6751, 19201, 1],\n",
       " [19201, 1, 700],\n",
       " [1, 700, 7232],\n",
       " [700, 7232, 19512],\n",
       " [7232, 19512, 20322],\n",
       " [19512, 20322, 0],\n",
       " [20322, 0, 2],\n",
       " [0, 2, 14118],\n",
       " [2, 14118, 2628],\n",
       " [14118, 2628, 1],\n",
       " [2628, 1, 8727],\n",
       " [1, 8727, 23816],\n",
       " [8727, 23816, 23284],\n",
       " [23816, 23284, 9047],\n",
       " [23284, 9047, 1],\n",
       " [9047, 1, 11904],\n",
       " [1, 11904, 15722],\n",
       " [11904, 15722, 2411],\n",
       " [15722, 2411, 0],\n",
       " [2411, 0, 15267],\n",
       " [0, 15267, 26473],\n",
       " [15267, 26473, 1],\n",
       " [26473, 1, 2],\n",
       " [1, 2, 13753],\n",
       " [2, 13753, 24247],\n",
       " [13753, 24247, 15968],\n",
       " [24247, 15968, 1],\n",
       " [15968, 1, 700],\n",
       " [1, 700, 15693],\n",
       " [700, 15693, 12992],\n",
       " [15693, 12992, 0],\n",
       " [12992, 0, 15267],\n",
       " [0, 15267, 26473],\n",
       " [15267, 26473, 1],\n",
       " [26473, 1, 8529],\n",
       " [1, 8529, 1],\n",
       " [8529, 1, 11651],\n",
       " [1, 11651, 2],\n",
       " [11651, 2, 14570],\n",
       " [2, 14570, 20940],\n",
       " [14570, 20940, 0],\n",
       " [20940, 0, 20738],\n",
       " [0, 20738, 1],\n",
       " [20738, 1, 23284],\n",
       " [1, 23284, 14699],\n",
       " [23284, 14699, 2718],\n",
       " [14699, 2718, 23466],\n",
       " [2718, 23466, 1],\n",
       " [23466, 1, 23284],\n",
       " [1, 23284, 8528],\n",
       " [23284, 8528, 21158],\n",
       " [8528, 21158, 0],\n",
       " [21158, 0, 21758],\n",
       " [0, 21758, 14559],\n",
       " [21758, 14559, 1],\n",
       " [14559, 1, 23284],\n",
       " [1, 23284, 24003],\n",
       " [23284, 24003, 11425],\n",
       " [24003, 11425, 22926],\n",
       " [11425, 22926, 20442],\n",
       " [22926, 20442, 24499],\n",
       " [20442, 24499, 1],\n",
       " [24499, 1, 17023],\n",
       " [1, 17023, 11651],\n",
       " [17023, 11651, 18707],\n",
       " [11651, 18707, 0],\n",
       " [18707, 0, 13280],\n",
       " [0, 13280, 4419],\n",
       " [13280, 4419, 1],\n",
       " ...]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "528336"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(wordX, (n_wordpatterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab_words)\n",
    "# one hot encode the output variable\n",
    "y = to_categorical(wordY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = load_model('weights/word/word_weights-cont-168-4.9039.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights/word/word_weights-2-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/168\n",
      "421608/421608 [==============================] - 194s 459us/step - loss: 5.6158\n",
      "\n",
      "Epoch 00001: loss improved from inf to 5.61577, saving model to word_weights-cont-01-5.6158.hdf5\n",
      "Epoch 2/168\n",
      "421608/421608 [==============================] - 198s 470us/step - loss: 5.5993\n",
      "\n",
      "Epoch 00002: loss improved from 5.61577 to 5.59928, saving model to word_weights-cont-02-5.5993.hdf5\n",
      "Epoch 3/168\n",
      "421608/421608 [==============================] - 202s 478us/step - loss: 5.5830\n",
      "\n",
      "Epoch 00003: loss improved from 5.59928 to 5.58304, saving model to word_weights-cont-03-5.5830.hdf5\n",
      "Epoch 4/168\n",
      "421608/421608 [==============================] - 208s 494us/step - loss: 5.5692\n",
      "\n",
      "Epoch 00004: loss improved from 5.58304 to 5.56915, saving model to word_weights-cont-04-5.5692.hdf5\n",
      "Epoch 5/168\n",
      "421608/421608 [==============================] - 220s 521us/step - loss: 5.5545\n",
      "\n",
      "Epoch 00005: loss improved from 5.56915 to 5.55446, saving model to word_weights-cont-05-5.5545.hdf5\n",
      "Epoch 6/168\n",
      "421608/421608 [==============================] - 185s 438us/step - loss: 5.5421\n",
      "\n",
      "Epoch 00006: loss improved from 5.55446 to 5.54208, saving model to word_weights-cont-06-5.5421.hdf5\n",
      "Epoch 7/168\n",
      "421608/421608 [==============================] - 178s 423us/step - loss: 5.5281\n",
      "\n",
      "Epoch 00007: loss improved from 5.54208 to 5.52813, saving model to word_weights-cont-07-5.5281.hdf5\n",
      "Epoch 8/168\n",
      "421608/421608 [==============================] - 201s 476us/step - loss: 5.5166\n",
      "\n",
      "Epoch 00008: loss improved from 5.52813 to 5.51655, saving model to word_weights-cont-08-5.5166.hdf5\n",
      "Epoch 9/168\n",
      "421608/421608 [==============================] - 190s 452us/step - loss: 5.5053\n",
      "\n",
      "Epoch 00009: loss improved from 5.51655 to 5.50533, saving model to word_weights-cont-09-5.5053.hdf5\n",
      "Epoch 10/168\n",
      "421608/421608 [==============================] - 184s 435us/step - loss: 5.4926\n",
      "\n",
      "Epoch 00010: loss improved from 5.50533 to 5.49265, saving model to word_weights-cont-10-5.4926.hdf5\n",
      "Epoch 11/168\n",
      "421608/421608 [==============================] - 184s 437us/step - loss: 5.4805\n",
      "\n",
      "Epoch 00011: loss improved from 5.49265 to 5.48049, saving model to word_weights-cont-11-5.4805.hdf5\n",
      "Epoch 12/168\n",
      "421608/421608 [==============================] - 205s 486us/step - loss: 5.4702\n",
      "\n",
      "Epoch 00012: loss improved from 5.48049 to 5.47023, saving model to word_weights-cont-12-5.4702.hdf5\n",
      "Epoch 13/168\n",
      "421608/421608 [==============================] - 200s 474us/step - loss: 5.4609\n",
      "\n",
      "Epoch 00013: loss improved from 5.47023 to 5.46086, saving model to word_weights-cont-13-5.4609.hdf5\n",
      "Epoch 14/168\n",
      "421608/421608 [==============================] - 227s 538us/step - loss: 5.4496\n",
      "\n",
      "Epoch 00014: loss improved from 5.46086 to 5.44961, saving model to word_weights-cont-14-5.4496.hdf5\n",
      "Epoch 15/168\n",
      "421608/421608 [==============================] - 191s 452us/step - loss: 5.4394\n",
      "\n",
      "Epoch 00015: loss improved from 5.44961 to 5.43936, saving model to word_weights-cont-15-5.4394.hdf5\n",
      "Epoch 16/168\n",
      "421608/421608 [==============================] - 189s 449us/step - loss: 5.4315\n",
      "\n",
      "Epoch 00016: loss improved from 5.43936 to 5.43152, saving model to word_weights-cont-16-5.4315.hdf5\n",
      "Epoch 17/168\n",
      "421608/421608 [==============================] - 186s 442us/step - loss: 5.4230\n",
      "\n",
      "Epoch 00017: loss improved from 5.43152 to 5.42298, saving model to word_weights-cont-17-5.4230.hdf5\n",
      "Epoch 18/168\n",
      "421608/421608 [==============================] - 205s 487us/step - loss: 5.4115\n",
      "\n",
      "Epoch 00018: loss improved from 5.42298 to 5.41155, saving model to word_weights-cont-18-5.4115.hdf5\n",
      "Epoch 19/168\n",
      "421608/421608 [==============================] - 202s 479us/step - loss: 5.4046\n",
      "\n",
      "Epoch 00019: loss improved from 5.41155 to 5.40465, saving model to word_weights-cont-19-5.4046.hdf5\n",
      "Epoch 20/168\n",
      "421608/421608 [==============================] - 195s 462us/step - loss: 5.3951\n",
      "\n",
      "Epoch 00020: loss improved from 5.40465 to 5.39509, saving model to word_weights-cont-20-5.3951.hdf5\n",
      "Epoch 21/168\n",
      "421608/421608 [==============================] - 181s 430us/step - loss: 5.3885\n",
      "\n",
      "Epoch 00021: loss improved from 5.39509 to 5.38848, saving model to word_weights-cont-21-5.3885.hdf5\n",
      "Epoch 22/168\n",
      "421608/421608 [==============================] - 179s 424us/step - loss: 5.3793s - \n",
      "\n",
      "Epoch 00022: loss improved from 5.38848 to 5.37927, saving model to word_weights-cont-22-5.3793.hdf5\n",
      "Epoch 23/168\n",
      "421608/421608 [==============================] - 184s 435us/step - loss: 5.3741\n",
      "\n",
      "Epoch 00023: loss improved from 5.37927 to 5.37414, saving model to word_weights-cont-23-5.3741.hdf5\n",
      "Epoch 24/168\n",
      "421608/421608 [==============================] - 185s 440us/step - loss: 5.3682\n",
      "\n",
      "Epoch 00024: loss improved from 5.37414 to 5.36823, saving model to word_weights-cont-24-5.3682.hdf5\n",
      "Epoch 25/168\n",
      "421608/421608 [==============================] - 180s 427us/step - loss: 5.3602\n",
      "\n",
      "Epoch 00025: loss improved from 5.36823 to 5.36019, saving model to word_weights-cont-25-5.3602.hdf5\n",
      "Epoch 26/168\n",
      "421608/421608 [==============================] - 176s 418us/step - loss: 5.3528\n",
      "\n",
      "Epoch 00026: loss improved from 5.36019 to 5.35283, saving model to word_weights-cont-26-5.3528.hdf5\n",
      "Epoch 27/168\n",
      "421608/421608 [==============================] - 181s 430us/step - loss: 5.3487\n",
      "\n",
      "Epoch 00027: loss improved from 5.35283 to 5.34869, saving model to word_weights-cont-27-5.3487.hdf5\n",
      "Epoch 28/168\n",
      "421608/421608 [==============================] - 190s 450us/step - loss: 5.3410\n",
      "\n",
      "Epoch 00028: loss improved from 5.34869 to 5.34099, saving model to word_weights-cont-28-5.3410.hdf5\n",
      "Epoch 29/168\n",
      "421608/421608 [==============================] - 195s 463us/step - loss: 5.3359\n",
      "\n",
      "Epoch 00029: loss improved from 5.34099 to 5.33587, saving model to word_weights-cont-29-5.3359.hdf5\n",
      "Epoch 30/168\n",
      "421608/421608 [==============================] - 187s 443us/step - loss: 5.3297\n",
      "\n",
      "Epoch 00030: loss improved from 5.33587 to 5.32967, saving model to word_weights-cont-30-5.3297.hdf5\n",
      "Epoch 31/168\n",
      "421608/421608 [==============================] - 177s 419us/step - loss: 5.3226\n",
      "\n",
      "Epoch 00031: loss improved from 5.32967 to 5.32259, saving model to word_weights-cont-31-5.3226.hdf5\n",
      "Epoch 32/168\n",
      "421608/421608 [==============================] - 185s 440us/step - loss: 5.3168\n",
      "\n",
      "Epoch 00032: loss improved from 5.32259 to 5.31681, saving model to word_weights-cont-32-5.3168.hdf5\n",
      "Epoch 33/168\n",
      "421608/421608 [==============================] - 175s 414us/step - loss: 5.3130\n",
      "\n",
      "Epoch 00033: loss improved from 5.31681 to 5.31299, saving model to word_weights-cont-33-5.3130.hdf5\n",
      "Epoch 34/168\n",
      "421608/421608 [==============================] - 174s 414us/step - loss: 5.3064\n",
      "\n",
      "Epoch 00034: loss improved from 5.31299 to 5.30645, saving model to word_weights-cont-34-5.3064.hdf5\n",
      "Epoch 35/168\n",
      "421608/421608 [==============================] - 165s 392us/step - loss: 5.3023\n",
      "\n",
      "Epoch 00035: loss improved from 5.30645 to 5.30232, saving model to word_weights-cont-35-5.3023.hdf5\n",
      "Epoch 36/168\n",
      "421608/421608 [==============================] - 174s 412us/step - loss: 5.2960\n",
      "\n",
      "Epoch 00036: loss improved from 5.30232 to 5.29598, saving model to word_weights-cont-36-5.2960.hdf5\n",
      "Epoch 37/168\n",
      "421608/421608 [==============================] - 176s 417us/step - loss: 5.2933\n",
      "\n",
      "Epoch 00037: loss improved from 5.29598 to 5.29328, saving model to word_weights-cont-37-5.2933.hdf5\n",
      "Epoch 38/168\n",
      "421608/421608 [==============================] - 175s 414us/step - loss: 5.2881\n",
      "\n",
      "Epoch 00038: loss improved from 5.29328 to 5.28813, saving model to word_weights-cont-38-5.2881.hdf5\n",
      "Epoch 39/168\n",
      "421608/421608 [==============================] - 174s 413us/step - loss: 5.2810\n",
      "\n",
      "Epoch 00039: loss improved from 5.28813 to 5.28098, saving model to word_weights-cont-39-5.2810.hdf5\n",
      "Epoch 40/168\n",
      "421608/421608 [==============================] - 158s 376us/step - loss: 5.2786\n",
      "\n",
      "Epoch 00040: loss improved from 5.28098 to 5.27856, saving model to word_weights-cont-40-5.2786.hdf5\n",
      "Epoch 41/168\n",
      "421608/421608 [==============================] - 167s 395us/step - loss: 5.2737\n",
      "\n",
      "Epoch 00041: loss improved from 5.27856 to 5.27367, saving model to word_weights-cont-41-5.2737.hdf5\n",
      "Epoch 42/168\n",
      "421608/421608 [==============================] - 149s 354us/step - loss: 5.2667\n",
      "\n",
      "Epoch 00042: loss improved from 5.27367 to 5.26669, saving model to word_weights-cont-42-5.2667.hdf5\n",
      "Epoch 43/168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421608/421608 [==============================] - 165s 390us/step - loss: 5.2648\n",
      "\n",
      "Epoch 00043: loss improved from 5.26669 to 5.26477, saving model to word_weights-cont-43-5.2648.hdf5\n",
      "Epoch 44/168\n",
      "421608/421608 [==============================] - 166s 394us/step - loss: 5.2619\n",
      "\n",
      "Epoch 00044: loss improved from 5.26477 to 5.26195, saving model to word_weights-cont-44-5.2619.hdf5\n",
      "Epoch 45/168\n",
      "421608/421608 [==============================] - 178s 422us/step - loss: 5.2560\n",
      "\n",
      "Epoch 00045: loss improved from 5.26195 to 5.25599, saving model to word_weights-cont-45-5.2560.hdf5\n",
      "Epoch 46/168\n",
      "421608/421608 [==============================] - 173s 411us/step - loss: 5.2534\n",
      "\n",
      "Epoch 00046: loss improved from 5.25599 to 5.25336, saving model to word_weights-cont-46-5.2534.hdf5\n",
      "Epoch 47/168\n",
      "421608/421608 [==============================] - 169s 401us/step - loss: 5.2485\n",
      "\n",
      "Epoch 00047: loss improved from 5.25336 to 5.24849, saving model to word_weights-cont-47-5.2485.hdf5\n",
      "Epoch 48/168\n",
      "421608/421608 [==============================] - 157s 373us/step - loss: 5.2437\n",
      "\n",
      "Epoch 00048: loss improved from 5.24849 to 5.24366, saving model to word_weights-cont-48-5.2437.hdf5\n",
      "Epoch 49/168\n",
      "421608/421608 [==============================] - 159s 378us/step - loss: 5.2408\n",
      "\n",
      "Epoch 00049: loss improved from 5.24366 to 5.24081, saving model to word_weights-cont-49-5.2408.hdf5\n",
      "Epoch 50/168\n",
      "421608/421608 [==============================] - 170s 402us/step - loss: 5.2375\n",
      "\n",
      "Epoch 00050: loss improved from 5.24081 to 5.23748, saving model to word_weights-cont-50-5.2375.hdf5\n",
      "Epoch 51/168\n",
      "421608/421608 [==============================] - 163s 387us/step - loss: 5.2317\n",
      "\n",
      "Epoch 00051: loss improved from 5.23748 to 5.23166, saving model to word_weights-cont-51-5.2317.hdf5\n",
      "Epoch 52/168\n",
      "421608/421608 [==============================] - 174s 412us/step - loss: 5.2309\n",
      "\n",
      "Epoch 00052: loss improved from 5.23166 to 5.23087, saving model to word_weights-cont-52-5.2309.hdf5\n",
      "Epoch 53/168\n",
      "421608/421608 [==============================] - 164s 388us/step - loss: 5.2263\n",
      "\n",
      "Epoch 00053: loss improved from 5.23087 to 5.22632, saving model to word_weights-cont-53-5.2263.hdf5\n",
      "Epoch 54/168\n",
      "421608/421608 [==============================] - 159s 377us/step - loss: 5.2225\n",
      "\n",
      "Epoch 00054: loss improved from 5.22632 to 5.22253, saving model to word_weights-cont-54-5.2225.hdf5\n",
      "Epoch 55/168\n",
      "421608/421608 [==============================] - 171s 405us/step - loss: 5.2163\n",
      "\n",
      "Epoch 00055: loss improved from 5.22253 to 5.21634, saving model to word_weights-cont-55-5.2163.hdf5\n",
      "Epoch 56/168\n",
      "421608/421608 [==============================] - 172s 409us/step - loss: 5.2149\n",
      "\n",
      "Epoch 00056: loss improved from 5.21634 to 5.21488, saving model to word_weights-cont-56-5.2149.hdf5\n",
      "Epoch 57/168\n",
      "421608/421608 [==============================] - 169s 401us/step - loss: 5.2098\n",
      "\n",
      "Epoch 00057: loss improved from 5.21488 to 5.20981, saving model to word_weights-cont-57-5.2098.hdf5\n",
      "Epoch 58/168\n",
      "421608/421608 [==============================] - 171s 407us/step - loss: 5.2064\n",
      "\n",
      "Epoch 00058: loss improved from 5.20981 to 5.20636, saving model to word_weights-cont-58-5.2064.hdf5\n",
      "Epoch 59/168\n",
      "421608/421608 [==============================] - 179s 425us/step - loss: 5.2054\n",
      "\n",
      "Epoch 00059: loss improved from 5.20636 to 5.20542, saving model to word_weights-cont-59-5.2054.hdf5\n",
      "Epoch 60/168\n",
      "421608/421608 [==============================] - 174s 412us/step - loss: 5.1996\n",
      "\n",
      "Epoch 00060: loss improved from 5.20542 to 5.19960, saving model to word_weights-cont-60-5.1996.hdf5\n",
      "Epoch 61/168\n",
      "421608/421608 [==============================] - 178s 422us/step - loss: 5.1967\n",
      "\n",
      "Epoch 00061: loss improved from 5.19960 to 5.19665, saving model to word_weights-cont-61-5.1967.hdf5\n",
      "Epoch 62/168\n",
      "421608/421608 [==============================] - 180s 426us/step - loss: 5.1915\n",
      "\n",
      "Epoch 00062: loss improved from 5.19665 to 5.19149, saving model to word_weights-cont-62-5.1915.hdf5\n",
      "Epoch 63/168\n",
      "421608/421608 [==============================] - 175s 415us/step - loss: 5.1901\n",
      "\n",
      "Epoch 00063: loss improved from 5.19149 to 5.19015, saving model to word_weights-cont-63-5.1901.hdf5\n",
      "Epoch 64/168\n",
      "421608/421608 [==============================] - 162s 383us/step - loss: 5.1867\n",
      "\n",
      "Epoch 00064: loss improved from 5.19015 to 5.18672, saving model to word_weights-cont-64-5.1867.hdf5\n",
      "Epoch 65/168\n",
      "421608/421608 [==============================] - 166s 394us/step - loss: 5.1834\n",
      "\n",
      "Epoch 00065: loss improved from 5.18672 to 5.18345, saving model to word_weights-cont-65-5.1834.hdf5\n",
      "Epoch 66/168\n",
      "421608/421608 [==============================] - 181s 430us/step - loss: 5.1801\n",
      "\n",
      "Epoch 00066: loss improved from 5.18345 to 5.18005, saving model to word_weights-cont-66-5.1801.hdf5\n",
      "Epoch 67/168\n",
      "421608/421608 [==============================] - 172s 408us/step - loss: 5.1771\n",
      "\n",
      "Epoch 00067: loss improved from 5.18005 to 5.17712, saving model to word_weights-cont-67-5.1771.hdf5\n",
      "Epoch 68/168\n",
      "421608/421608 [==============================] - 165s 392us/step - loss: 5.1738\n",
      "\n",
      "Epoch 00068: loss improved from 5.17712 to 5.17384, saving model to word_weights-cont-68-5.1738.hdf5\n",
      "Epoch 69/168\n",
      "421608/421608 [==============================] - 186s 442us/step - loss: 5.1686\n",
      "\n",
      "Epoch 00069: loss improved from 5.17384 to 5.16856, saving model to word_weights-cont-69-5.1686.hdf5\n",
      "Epoch 70/168\n",
      "421608/421608 [==============================] - 172s 407us/step - loss: 5.1639\n",
      "\n",
      "Epoch 00070: loss improved from 5.16856 to 5.16393, saving model to word_weights-cont-70-5.1639.hdf5\n",
      "Epoch 71/168\n",
      "421608/421608 [==============================] - 185s 438us/step - loss: 5.1613\n",
      "\n",
      "Epoch 00071: loss improved from 5.16393 to 5.16129, saving model to word_weights-cont-71-5.1613.hdf5\n",
      "Epoch 72/168\n",
      "421608/421608 [==============================] - 168s 399us/step - loss: 5.1601\n",
      "\n",
      "Epoch 00072: loss improved from 5.16129 to 5.16014, saving model to word_weights-cont-72-5.1601.hdf5\n",
      "Epoch 73/168\n",
      "421608/421608 [==============================] - 173s 410us/step - loss: 5.1537\n",
      "\n",
      "Epoch 00073: loss improved from 5.16014 to 5.15372, saving model to word_weights-cont-73-5.1537.hdf5\n",
      "Epoch 74/168\n",
      "421608/421608 [==============================] - 175s 414us/step - loss: 5.1514\n",
      "\n",
      "Epoch 00074: loss improved from 5.15372 to 5.15138, saving model to word_weights-cont-74-5.1514.hdf5\n",
      "Epoch 75/168\n",
      "421608/421608 [==============================] - 187s 443us/step - loss: 5.1486\n",
      "\n",
      "Epoch 00075: loss improved from 5.15138 to 5.14860, saving model to word_weights-cont-75-5.1486.hdf5\n",
      "Epoch 76/168\n",
      "421608/421608 [==============================] - 180s 426us/step - loss: 5.1467\n",
      "\n",
      "Epoch 00076: loss improved from 5.14860 to 5.14665, saving model to word_weights-cont-76-5.1467.hdf5\n",
      "Epoch 77/168\n",
      "421608/421608 [==============================] - 170s 403us/step - loss: 5.1409\n",
      "\n",
      "Epoch 00077: loss improved from 5.14665 to 5.14089, saving model to word_weights-cont-77-5.1409.hdf5\n",
      "Epoch 78/168\n",
      "421608/421608 [==============================] - 173s 411us/step - loss: 5.1423\n",
      "\n",
      "Epoch 00078: loss did not improve from 5.14089\n",
      "Epoch 79/168\n",
      "421608/421608 [==============================] - 170s 403us/step - loss: 5.1370\n",
      "\n",
      "Epoch 00079: loss improved from 5.14089 to 5.13704, saving model to word_weights-cont-79-5.1370.hdf5\n",
      "Epoch 80/168\n",
      "421608/421608 [==============================] - 164s 389us/step - loss: 5.1328\n",
      "\n",
      "Epoch 00080: loss improved from 5.13704 to 5.13283, saving model to word_weights-cont-80-5.1328.hdf5\n",
      "Epoch 81/168\n",
      "421608/421608 [==============================] - 160s 379us/step - loss: 5.1302\n",
      "\n",
      "Epoch 00081: loss improved from 5.13283 to 5.13017, saving model to word_weights-cont-81-5.1302.hdf5\n",
      "Epoch 82/168\n",
      "421608/421608 [==============================] - 174s 413us/step - loss: 5.1255\n",
      "\n",
      "Epoch 00082: loss improved from 5.13017 to 5.12553, saving model to word_weights-cont-82-5.1255.hdf5\n",
      "Epoch 83/168\n",
      "421608/421608 [==============================] - 170s 404us/step - loss: 5.1246\n",
      "\n",
      "Epoch 00083: loss improved from 5.12553 to 5.12457, saving model to word_weights-cont-83-5.1246.hdf5\n",
      "Epoch 84/168\n",
      "421608/421608 [==============================] - 177s 419us/step - loss: 5.1194s - loss: 5.\n",
      "\n",
      "Epoch 00084: loss improved from 5.12457 to 5.11939, saving model to word_weights-cont-84-5.1194.hdf5\n",
      "Epoch 85/168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421608/421608 [==============================] - 161s 382us/step - loss: 5.1173\n",
      "\n",
      "Epoch 00085: loss improved from 5.11939 to 5.11731, saving model to word_weights-cont-85-5.1173.hdf5\n",
      "Epoch 86/168\n",
      "421608/421608 [==============================] - 178s 422us/step - loss: 5.1164\n",
      "\n",
      "Epoch 00086: loss improved from 5.11731 to 5.11635, saving model to word_weights-cont-86-5.1164.hdf5\n",
      "Epoch 87/168\n",
      "421608/421608 [==============================] - 157s 371us/step - loss: 5.1131\n",
      "\n",
      "Epoch 00087: loss improved from 5.11635 to 5.11307, saving model to word_weights-cont-87-5.1131.hdf5\n",
      "Epoch 88/168\n",
      "421608/421608 [==============================] - 135s 321us/step - loss: 5.1078\n",
      "\n",
      "Epoch 00088: loss improved from 5.11307 to 5.10776, saving model to word_weights-cont-88-5.1078.hdf5\n",
      "Epoch 89/168\n",
      "421608/421608 [==============================] - 169s 401us/step - loss: 5.1055\n",
      "\n",
      "Epoch 00089: loss improved from 5.10776 to 5.10546, saving model to word_weights-cont-89-5.1055.hdf5\n",
      "Epoch 90/168\n",
      "421608/421608 [==============================] - 174s 412us/step - loss: 5.1028\n",
      "\n",
      "Epoch 00090: loss improved from 5.10546 to 5.10282, saving model to word_weights-cont-90-5.1028.hdf5\n",
      "Epoch 91/168\n",
      "421608/421608 [==============================] - 181s 430us/step - loss: 5.1017\n",
      "\n",
      "Epoch 00091: loss improved from 5.10282 to 5.10167, saving model to word_weights-cont-91-5.1017.hdf5\n",
      "Epoch 92/168\n",
      "421608/421608 [==============================] - 180s 428us/step - loss: 5.0962\n",
      "\n",
      "Epoch 00092: loss improved from 5.10167 to 5.09617, saving model to word_weights-cont-92-5.0962.hdf5\n",
      "Epoch 93/168\n",
      "421608/421608 [==============================] - 156s 371us/step - loss: 5.0962\n",
      "\n",
      "Epoch 00093: loss did not improve from 5.09617\n",
      "Epoch 94/168\n",
      "421608/421608 [==============================] - 176s 417us/step - loss: 5.0918\n",
      "\n",
      "Epoch 00094: loss improved from 5.09617 to 5.09176, saving model to word_weights-cont-94-5.0918.hdf5\n",
      "Epoch 95/168\n",
      "421608/421608 [==============================] - 157s 372us/step - loss: 5.0897\n",
      "\n",
      "Epoch 00095: loss improved from 5.09176 to 5.08967, saving model to word_weights-cont-95-5.0897.hdf5\n",
      "Epoch 96/168\n",
      "421608/421608 [==============================] - 164s 388us/step - loss: 5.0852\n",
      "\n",
      "Epoch 00096: loss improved from 5.08967 to 5.08524, saving model to word_weights-cont-96-5.0852.hdf5\n",
      "Epoch 97/168\n",
      "421608/421608 [==============================] - 173s 410us/step - loss: 5.0827\n",
      "\n",
      "Epoch 00097: loss improved from 5.08524 to 5.08271, saving model to word_weights-cont-97-5.0827.hdf5\n",
      "Epoch 98/168\n",
      "421608/421608 [==============================] - 175s 414us/step - loss: 5.0797s - loss: 5.07\n",
      "\n",
      "Epoch 00098: loss improved from 5.08271 to 5.07966, saving model to word_weights-cont-98-5.0797.hdf5\n",
      "Epoch 99/168\n",
      "421608/421608 [==============================] - 189s 449us/step - loss: 5.0760\n",
      "\n",
      "Epoch 00099: loss improved from 5.07966 to 5.07598, saving model to word_weights-cont-99-5.0760.hdf5\n",
      "Epoch 100/168\n",
      "421608/421608 [==============================] - 171s 405us/step - loss: 5.0732\n",
      "\n",
      "Epoch 00100: loss improved from 5.07598 to 5.07315, saving model to word_weights-cont-100-5.0732.hdf5\n",
      "Epoch 101/168\n",
      "421608/421608 [==============================] - 190s 452us/step - loss: 5.0720\n",
      "\n",
      "Epoch 00101: loss improved from 5.07315 to 5.07196, saving model to word_weights-cont-101-5.0720.hdf5\n",
      "Epoch 102/168\n",
      "421608/421608 [==============================] - 177s 420us/step - loss: 5.0676\n",
      "\n",
      "Epoch 00102: loss improved from 5.07196 to 5.06761, saving model to word_weights-cont-102-5.0676.hdf5\n",
      "Epoch 103/168\n",
      "421608/421608 [==============================] - 149s 353us/step - loss: 5.0681\n",
      "\n",
      "Epoch 00103: loss did not improve from 5.06761\n",
      "Epoch 104/168\n",
      "421608/421608 [==============================] - 170s 404us/step - loss: 5.0616\n",
      "\n",
      "Epoch 00104: loss improved from 5.06761 to 5.06162, saving model to word_weights-cont-104-5.0616.hdf5\n",
      "Epoch 105/168\n",
      "421608/421608 [==============================] - 172s 407us/step - loss: 5.0610\n",
      "\n",
      "Epoch 00105: loss improved from 5.06162 to 5.06100, saving model to word_weights-cont-105-5.0610.hdf5\n",
      "Epoch 106/168\n",
      "421608/421608 [==============================] - 180s 428us/step - loss: 5.0558\n",
      "\n",
      "Epoch 00106: loss improved from 5.06100 to 5.05577, saving model to word_weights-cont-106-5.0558.hdf5\n",
      "Epoch 107/168\n",
      "421608/421608 [==============================] - 168s 398us/step - loss: 5.0526\n",
      "\n",
      "Epoch 00107: loss improved from 5.05577 to 5.05260, saving model to word_weights-cont-107-5.0526.hdf5\n",
      "Epoch 108/168\n",
      "421608/421608 [==============================] - 190s 450us/step - loss: 5.0525\n",
      "\n",
      "Epoch 00108: loss improved from 5.05260 to 5.05247, saving model to word_weights-cont-108-5.0525.hdf5\n",
      "Epoch 109/168\n",
      "421608/421608 [==============================] - 190s 451us/step - loss: 5.0488\n",
      "\n",
      "Epoch 00109: loss improved from 5.05247 to 5.04882, saving model to word_weights-cont-109-5.0488.hdf5\n",
      "Epoch 110/168\n",
      "421608/421608 [==============================] - 183s 434us/step - loss: 5.0449\n",
      "\n",
      "Epoch 00110: loss improved from 5.04882 to 5.04485, saving model to word_weights-cont-110-5.0449.hdf5\n",
      "Epoch 111/168\n",
      "421608/421608 [==============================] - 179s 425us/step - loss: 5.0440\n",
      "\n",
      "Epoch 00111: loss improved from 5.04485 to 5.04404, saving model to word_weights-cont-111-5.0440.hdf5\n",
      "Epoch 112/168\n",
      "421608/421608 [==============================] - 152s 361us/step - loss: 5.0404\n",
      "\n",
      "Epoch 00112: loss improved from 5.04404 to 5.04041, saving model to word_weights-cont-112-5.0404.hdf5\n",
      "Epoch 113/168\n",
      "421608/421608 [==============================] - 188s 445us/step - loss: 5.0389\n",
      "\n",
      "Epoch 00113: loss improved from 5.04041 to 5.03890, saving model to word_weights-cont-113-5.0389.hdf5\n",
      "Epoch 114/168\n",
      "421608/421608 [==============================] - 157s 372us/step - loss: 5.0333\n",
      "\n",
      "Epoch 00114: loss improved from 5.03890 to 5.03329, saving model to word_weights-cont-114-5.0333.hdf5\n",
      "Epoch 115/168\n",
      "421608/421608 [==============================] - 167s 397us/step - loss: 5.0309s - loss: 5.\n",
      "\n",
      "Epoch 00115: loss improved from 5.03329 to 5.03093, saving model to word_weights-cont-115-5.0309.hdf5\n",
      "Epoch 116/168\n",
      "421608/421608 [==============================] - 178s 422us/step - loss: 5.0308\n",
      "\n",
      "Epoch 00116: loss improved from 5.03093 to 5.03079, saving model to word_weights-cont-116-5.0308.hdf5\n",
      "Epoch 117/168\n",
      "421608/421608 [==============================] - 187s 443us/step - loss: 5.0257\n",
      "\n",
      "Epoch 00117: loss improved from 5.03079 to 5.02574, saving model to word_weights-cont-117-5.0257.hdf5\n",
      "Epoch 118/168\n",
      "421608/421608 [==============================] - 199s 472us/step - loss: 5.0252\n",
      "\n",
      "Epoch 00118: loss improved from 5.02574 to 5.02525, saving model to word_weights-cont-118-5.0252.hdf5\n",
      "Epoch 119/168\n",
      "421608/421608 [==============================] - 195s 463us/step - loss: 5.0227\n",
      "\n",
      "Epoch 00119: loss improved from 5.02525 to 5.02267, saving model to word_weights-cont-119-5.0227.hdf5\n",
      "Epoch 120/168\n",
      "421608/421608 [==============================] - 190s 450us/step - loss: 5.0220\n",
      "\n",
      "Epoch 00120: loss improved from 5.02267 to 5.02202, saving model to word_weights-cont-120-5.0220.hdf5\n",
      "Epoch 121/168\n",
      "421608/421608 [==============================] - 172s 408us/step - loss: 5.0160\n",
      "\n",
      "Epoch 00121: loss improved from 5.02202 to 5.01602, saving model to word_weights-cont-121-5.0160.hdf5\n",
      "Epoch 122/168\n",
      "421608/421608 [==============================] - 188s 446us/step - loss: 5.0149\n",
      "\n",
      "Epoch 00122: loss improved from 5.01602 to 5.01491, saving model to word_weights-cont-122-5.0149.hdf5\n",
      "Epoch 123/168\n",
      "421608/421608 [==============================] - 196s 465us/step - loss: 5.0106\n",
      "\n",
      "Epoch 00123: loss improved from 5.01491 to 5.01060, saving model to word_weights-cont-123-5.0106.hdf5\n",
      "Epoch 124/168\n",
      "421608/421608 [==============================] - 199s 471us/step - loss: 5.0085\n",
      "\n",
      "Epoch 00124: loss improved from 5.01060 to 5.00852, saving model to word_weights-cont-124-5.0085.hdf5\n",
      "Epoch 125/168\n",
      "421608/421608 [==============================] - 198s 470us/step - loss: 5.0047\n",
      "\n",
      "Epoch 00125: loss improved from 5.00852 to 5.00469, saving model to word_weights-cont-125-5.0047.hdf5\n",
      "Epoch 126/168\n",
      "421608/421608 [==============================] - 176s 418us/step - loss: 5.0041\n",
      "\n",
      "Epoch 00126: loss improved from 5.00469 to 5.00410, saving model to word_weights-cont-126-5.0041.hdf5\n",
      "Epoch 127/168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421608/421608 [==============================] - 161s 383us/step - loss: 5.0009\n",
      "\n",
      "Epoch 00127: loss improved from 5.00410 to 5.00088, saving model to word_weights-cont-127-5.0009.hdf5\n",
      "Epoch 128/168\n",
      "421608/421608 [==============================] - 162s 385us/step - loss: 5.0025\n",
      "\n",
      "Epoch 00128: loss did not improve from 5.00088\n",
      "Epoch 129/168\n",
      "421608/421608 [==============================] - 184s 437us/step - loss: 4.9963\n",
      "\n",
      "Epoch 00129: loss improved from 5.00088 to 4.99632, saving model to word_weights-cont-129-4.9963.hdf5\n",
      "Epoch 130/168\n",
      "421608/421608 [==============================] - 177s 420us/step - loss: 4.9941\n",
      "\n",
      "Epoch 00130: loss improved from 4.99632 to 4.99414, saving model to word_weights-cont-130-4.9941.hdf5\n",
      "Epoch 131/168\n",
      "421608/421608 [==============================] - 192s 456us/step - loss: 4.9885\n",
      "\n",
      "Epoch 00131: loss improved from 4.99414 to 4.98852, saving model to word_weights-cont-131-4.9885.hdf5\n",
      "Epoch 132/168\n",
      "421608/421608 [==============================] - 178s 421us/step - loss: 4.9880\n",
      "\n",
      "Epoch 00132: loss improved from 4.98852 to 4.98801, saving model to word_weights-cont-132-4.9880.hdf5\n",
      "Epoch 133/168\n",
      "421608/421608 [==============================] - 190s 450us/step - loss: 4.9867\n",
      "\n",
      "Epoch 00133: loss improved from 4.98801 to 4.98671, saving model to word_weights-cont-133-4.9867.hdf5\n",
      "Epoch 134/168\n",
      "421608/421608 [==============================] - 193s 458us/step - loss: 4.9831\n",
      "\n",
      "Epoch 00134: loss improved from 4.98671 to 4.98312, saving model to word_weights-cont-134-4.9831.hdf5\n",
      "Epoch 135/168\n",
      "421608/421608 [==============================] - 199s 471us/step - loss: 4.9804\n",
      "\n",
      "Epoch 00135: loss improved from 4.98312 to 4.98039, saving model to word_weights-cont-135-4.9804.hdf5\n",
      "Epoch 136/168\n",
      "421608/421608 [==============================] - 201s 476us/step - loss: 4.9765\n",
      "\n",
      "Epoch 00136: loss improved from 4.98039 to 4.97651, saving model to word_weights-cont-136-4.9765.hdf5\n",
      "Epoch 137/168\n",
      "421608/421608 [==============================] - 191s 454us/step - loss: 4.9762\n",
      "\n",
      "Epoch 00137: loss improved from 4.97651 to 4.97620, saving model to word_weights-cont-137-4.9762.hdf5\n",
      "Epoch 138/168\n",
      "421608/421608 [==============================] - 168s 399us/step - loss: 4.9736\n",
      "\n",
      "Epoch 00138: loss improved from 4.97620 to 4.97363, saving model to word_weights-cont-138-4.9736.hdf5\n",
      "Epoch 139/168\n",
      "421608/421608 [==============================] - 192s 456us/step - loss: 4.9718\n",
      "\n",
      "Epoch 00139: loss improved from 4.97363 to 4.97182, saving model to word_weights-cont-139-4.9718.hdf5\n",
      "Epoch 140/168\n",
      "421608/421608 [==============================] - 193s 458us/step - loss: 4.9668\n",
      "\n",
      "Epoch 00140: loss improved from 4.97182 to 4.96677, saving model to word_weights-cont-140-4.9668.hdf5\n",
      "Epoch 141/168\n",
      "421608/421608 [==============================] - 204s 485us/step - loss: 4.9641\n",
      "\n",
      "Epoch 00141: loss improved from 4.96677 to 4.96409, saving model to word_weights-cont-141-4.9641.hdf5\n",
      "Epoch 142/168\n",
      "421608/421608 [==============================] - 201s 477us/step - loss: 4.9632\n",
      "\n",
      "Epoch 00142: loss improved from 4.96409 to 4.96320, saving model to word_weights-cont-142-4.9632.hdf5\n",
      "Epoch 143/168\n",
      "421608/421608 [==============================] - 186s 442us/step - loss: 4.9601\n",
      "\n",
      "Epoch 00143: loss improved from 4.96320 to 4.96011, saving model to word_weights-cont-143-4.9601.hdf5\n",
      "Epoch 144/168\n",
      "421608/421608 [==============================] - 209s 495us/step - loss: 4.9621\n",
      "\n",
      "Epoch 00144: loss did not improve from 4.96011\n",
      "Epoch 145/168\n",
      "421608/421608 [==============================] - 199s 473us/step - loss: 4.9545\n",
      "\n",
      "Epoch 00145: loss improved from 4.96011 to 4.95449, saving model to word_weights-cont-145-4.9545.hdf5\n",
      "Epoch 146/168\n",
      "421608/421608 [==============================] - 193s 457us/step - loss: 4.9497\n",
      "\n",
      "Epoch 00146: loss improved from 4.95449 to 4.94970, saving model to word_weights-cont-146-4.9497.hdf5\n",
      "Epoch 147/168\n",
      "421608/421608 [==============================] - 201s 476us/step - loss: 4.9515\n",
      "\n",
      "Epoch 00147: loss did not improve from 4.94970\n",
      "Epoch 148/168\n",
      "421608/421608 [==============================] - 185s 439us/step - loss: 4.9490\n",
      "\n",
      "Epoch 00148: loss improved from 4.94970 to 4.94899, saving model to word_weights-cont-148-4.9490.hdf5\n",
      "Epoch 149/168\n",
      "421608/421608 [==============================] - 184s 437us/step - loss: 4.9461\n",
      "\n",
      "Epoch 00149: loss improved from 4.94899 to 4.94615, saving model to word_weights-cont-149-4.9461.hdf5\n",
      "Epoch 150/168\n",
      "421608/421608 [==============================] - 181s 429us/step - loss: 4.9428\n",
      "\n",
      "Epoch 00150: loss improved from 4.94615 to 4.94284, saving model to word_weights-cont-150-4.9428.hdf5\n",
      "Epoch 151/168\n",
      "421608/421608 [==============================] - 194s 460us/step - loss: 4.9423\n",
      "\n",
      "Epoch 00151: loss improved from 4.94284 to 4.94226, saving model to word_weights-cont-151-4.9423.hdf5\n",
      "Epoch 152/168\n",
      "421608/421608 [==============================] - 172s 407us/step - loss: 4.9387\n",
      "\n",
      "Epoch 00152: loss improved from 4.94226 to 4.93874, saving model to word_weights-cont-152-4.9387.hdf5\n",
      "Epoch 153/168\n",
      "421608/421608 [==============================] - 184s 435us/step - loss: 4.9381\n",
      "\n",
      "Epoch 00153: loss improved from 4.93874 to 4.93805, saving model to word_weights-cont-153-4.9381.hdf5\n",
      "Epoch 154/168\n",
      "421608/421608 [==============================] - 159s 377us/step - loss: 4.9336\n",
      "\n",
      "Epoch 00154: loss improved from 4.93805 to 4.93363, saving model to word_weights-cont-154-4.9336.hdf5\n",
      "Epoch 155/168\n",
      "421608/421608 [==============================] - 191s 452us/step - loss: 4.9328\n",
      "\n",
      "Epoch 00155: loss improved from 4.93363 to 4.93277, saving model to word_weights-cont-155-4.9328.hdf5\n",
      "Epoch 156/168\n",
      "421608/421608 [==============================] - 187s 444us/step - loss: 4.9316\n",
      "\n",
      "Epoch 00156: loss improved from 4.93277 to 4.93159, saving model to word_weights-cont-156-4.9316.hdf5\n",
      "Epoch 157/168\n",
      "421608/421608 [==============================] - 188s 446us/step - loss: 4.9263\n",
      "\n",
      "Epoch 00157: loss improved from 4.93159 to 4.92634, saving model to word_weights-cont-157-4.9263.hdf5\n",
      "Epoch 158/168\n",
      "421608/421608 [==============================] - 193s 457us/step - loss: 4.9273\n",
      "\n",
      "Epoch 00158: loss did not improve from 4.92634\n",
      "Epoch 159/168\n",
      "421608/421608 [==============================] - 154s 365us/step - loss: 4.9251\n",
      "\n",
      "Epoch 00159: loss improved from 4.92634 to 4.92506, saving model to word_weights-cont-159-4.9251.hdf5\n",
      "Epoch 160/168\n",
      "421608/421608 [==============================] - 178s 422us/step - loss: 4.9205\n",
      "\n",
      "Epoch 00160: loss improved from 4.92506 to 4.92052, saving model to word_weights-cont-160-4.9205.hdf5\n",
      "Epoch 161/168\n",
      "421608/421608 [==============================] - 177s 420us/step - loss: 4.9222\n",
      "\n",
      "Epoch 00161: loss did not improve from 4.92052\n",
      "Epoch 162/168\n",
      "421608/421608 [==============================] - 200s 474us/step - loss: 4.9173\n",
      "\n",
      "Epoch 00162: loss improved from 4.92052 to 4.91730, saving model to word_weights-cont-162-4.9173.hdf5\n",
      "Epoch 163/168\n",
      "421608/421608 [==============================] - 191s 453us/step - loss: 4.9139\n",
      "\n",
      "Epoch 00163: loss improved from 4.91730 to 4.91394, saving model to word_weights-cont-163-4.9139.hdf5\n",
      "Epoch 164/168\n",
      "421608/421608 [==============================] - 202s 480us/step - loss: 4.9125s\n",
      "\n",
      "Epoch 00164: loss improved from 4.91394 to 4.91251, saving model to word_weights-cont-164-4.9125.hdf5\n",
      "Epoch 165/168\n",
      "421608/421608 [==============================] - 178s 422us/step - loss: 4.9131\n",
      "\n",
      "Epoch 00165: loss did not improve from 4.91251\n",
      "Epoch 166/168\n",
      "421608/421608 [==============================] - 214s 507us/step - loss: 4.9063\n",
      "\n",
      "Epoch 00166: loss improved from 4.91251 to 4.90632, saving model to word_weights-cont-166-4.9063.hdf5\n",
      "Epoch 167/168\n",
      "421608/421608 [==============================] - 206s 488us/step - loss: 4.9089\n",
      "\n",
      "Epoch 00167: loss did not improve from 4.90632\n",
      "Epoch 168/168\n",
      "421608/421608 [==============================] - 186s 440us/step - loss: 4.9039\n",
      "\n",
      "Epoch 00168: loss improved from 4.90632 to 4.90392, saving model to word_weights-cont-168-4.9039.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2314f8a5f08>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(X, y, epochs=200, batch_size=128, callbacks=callbacks_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
