{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "RANDOM = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haikus_train_df = pd.read_pickle('./data/haikus_train_df.pickle')\n",
    "haikus_test_df = pd.read_pickle('./data/haikus_test_df.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Input, LSTM, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  1849446\n",
      "Total Vocab:  107\n"
     ]
    }
   ],
   "source": [
    "corpus_raw = ''.join(haikus_train_df['textchar_withtokens'])\n",
    "\n",
    "chars = sorted(set(list(corpus_raw)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "n_chars = len(corpus_raw)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " '!',\n",
       " '\"',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '[',\n",
       " ']',\n",
       " '_',\n",
       " '`',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '{',\n",
       " '~',\n",
       " '\\x85',\n",
       " '\\x92',\n",
       " '\\x96',\n",
       " '\\x97',\n",
       " '\\xa0',\n",
       " 'à',\n",
       " 'ä',\n",
       " 'é',\n",
       " 'ü',\n",
       " 'ē',\n",
       " 'ū',\n",
       " 'ŭ',\n",
       " '\\u200b',\n",
       " '‘',\n",
       " '’',\n",
       " '“',\n",
       " '”',\n",
       " '…',\n",
       " '↕',\n",
       " '◘']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25128"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_poems = len(haikus_train_df)\n",
    "\n",
    "n_poems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max patterns per poem:  797\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 5\n",
    "\n",
    "poemX = []\n",
    "poemY = []\n",
    "n_patterns = 0\n",
    "\n",
    "corpusX = []\n",
    "corpusY = []\n",
    "for poem_index in range(0, n_poems):\n",
    "\n",
    "    textX = []\n",
    "    textY = []\n",
    "    poem = haikus_train_df['textchar_withtokens'].iloc[poem_index]\n",
    "    for i in range(0,  len(poem) - seq_length, 1):\n",
    "        seq_in = poem[i:i + seq_length]\n",
    "        seq_out = poem[i + seq_length]\n",
    "        textX.append([char_to_int[char] for char in seq_in])\n",
    "        textY.append(char_to_int[seq_out])\n",
    "    n_patterns = max(n_patterns, len(textX))\n",
    "    \n",
    "    poemX.append(textX)\n",
    "    poemY.append(textY)\n",
    "    \n",
    "    corpusX += textX\n",
    "    corpusY += textY\n",
    "\n",
    "print(\"Max patterns per poem: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoem_charindex = char_to_int['◘']\n",
    "newline_charindex = char_to_int['↕']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>59</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>59</td>\n",
       "      <td>77</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73</td>\n",
       "      <td>59</td>\n",
       "      <td>77</td>\n",
       "      <td>67</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59</td>\n",
       "      <td>77</td>\n",
       "      <td>67</td>\n",
       "      <td>77</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723801</th>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>66</td>\n",
       "      <td>67</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723802</th>\n",
       "      <td>77</td>\n",
       "      <td>66</td>\n",
       "      <td>67</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723803</th>\n",
       "      <td>66</td>\n",
       "      <td>67</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723804</th>\n",
       "      <td>67</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>63</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723805</th>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>63</td>\n",
       "      <td>76</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1723806 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0   1   2   3    4\n",
       "0        59  72   0  73   59\n",
       "1        72   0  73  59   77\n",
       "2         0  73  59  77   67\n",
       "3        73  59  77  67   77\n",
       "4        59  77  67  77  105\n",
       "...      ..  ..  ..  ..  ...\n",
       "1723801   0  77  66  67   71\n",
       "1723802  77  66  67  71   71\n",
       "1723803  66  67  71  71   63\n",
       "1723804  67  71  71  63   76\n",
       "1723805  71  71  63  76   77\n",
       "\n",
       "[1723806 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(corpusX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72,\n",
       " 0,\n",
       " 73,\n",
       " 59,\n",
       " 77,\n",
       " 67,\n",
       " 77,\n",
       " 105,\n",
       " 67,\n",
       " 72,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 30,\n",
       " 67,\n",
       " 60,\n",
       " 70,\n",
       " 63,\n",
       " 0,\n",
       " 30,\n",
       " 63,\n",
       " 70,\n",
       " 78,\n",
       " 0,\n",
       " 10,\n",
       " 10,\n",
       " 105,\n",
       " 59,\n",
       " 62,\n",
       " 79,\n",
       " 70,\n",
       " 78,\n",
       " 0,\n",
       " 60,\n",
       " 73,\n",
       " 73,\n",
       " 69,\n",
       " 0,\n",
       " 77,\n",
       " 78,\n",
       " 73,\n",
       " 76,\n",
       " 63,\n",
       " 106,\n",
       " 71,\n",
       " 73,\n",
       " 76,\n",
       " 63,\n",
       " 78,\n",
       " 78,\n",
       " 67,\n",
       " 0,\n",
       " 77,\n",
       " 73,\n",
       " 72,\n",
       " 72,\n",
       " 63,\n",
       " 78,\n",
       " 0,\n",
       " 82,\n",
       " 82,\n",
       " 80,\n",
       " 67,\n",
       " 105,\n",
       " 63,\n",
       " 105,\n",
       " 77,\n",
       " 74,\n",
       " 63,\n",
       " 72,\n",
       " 77,\n",
       " 63,\n",
       " 76,\n",
       " 106,\n",
       " 66,\n",
       " 63,\n",
       " 76,\n",
       " 63,\n",
       " 0,\n",
       " 81,\n",
       " 66,\n",
       " 63,\n",
       " 72,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 83,\n",
       " 0,\n",
       " 61,\n",
       " 59,\n",
       " 71,\n",
       " 63,\n",
       " 0,\n",
       " 71,\n",
       " 67,\n",
       " 72,\n",
       " 62,\n",
       " 0,\n",
       " 77,\n",
       " 79,\n",
       " 64,\n",
       " 64,\n",
       " 63,\n",
       " 76,\n",
       " 63,\n",
       " 62,\n",
       " 0,\n",
       " 77,\n",
       " 66,\n",
       " 59,\n",
       " 71,\n",
       " 63,\n",
       " 105,\n",
       " 58,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 77,\n",
       " 63,\n",
       " 0,\n",
       " 60,\n",
       " 63,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 77,\n",
       " 59,\n",
       " 71,\n",
       " 63,\n",
       " 0,\n",
       " 59,\n",
       " 72,\n",
       " 62,\n",
       " 0,\n",
       " 72,\n",
       " 73,\n",
       " 78,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 77,\n",
       " 59,\n",
       " 71,\n",
       " 63,\n",
       " 105,\n",
       " 59,\n",
       " 10,\n",
       " 81,\n",
       " 73,\n",
       " 72,\n",
       " 62,\n",
       " 63,\n",
       " 76,\n",
       " 67,\n",
       " 72,\n",
       " 65,\n",
       " 0,\n",
       " 81,\n",
       " 66,\n",
       " 67,\n",
       " 77,\n",
       " 74,\n",
       " 63,\n",
       " 76,\n",
       " 63,\n",
       " 62,\n",
       " 0,\n",
       " 71,\n",
       " 67,\n",
       " 72,\n",
       " 62,\n",
       " 106,\n",
       " 62,\n",
       " 70,\n",
       " 63,\n",
       " 0,\n",
       " 61,\n",
       " 73,\n",
       " 72,\n",
       " 80,\n",
       " 63,\n",
       " 76,\n",
       " 77,\n",
       " 59,\n",
       " 78,\n",
       " 67,\n",
       " 73,\n",
       " 72,\n",
       " 105,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 62,\n",
       " 59,\n",
       " 64,\n",
       " 64,\n",
       " 73,\n",
       " 62,\n",
       " 67,\n",
       " 70,\n",
       " 77,\n",
       " 0,\n",
       " 72,\n",
       " 73,\n",
       " 62,\n",
       " 62,\n",
       " 67,\n",
       " 72,\n",
       " 65,\n",
       " 105,\n",
       " 37,\n",
       " 72,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 60,\n",
       " 76,\n",
       " 63,\n",
       " 63,\n",
       " 84,\n",
       " 63,\n",
       " 106,\n",
       " 67,\n",
       " 80,\n",
       " 63,\n",
       " 76,\n",
       " 0,\n",
       " 64,\n",
       " 73,\n",
       " 65,\n",
       " 105,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 77,\n",
       " 70,\n",
       " 73,\n",
       " 81,\n",
       " 0,\n",
       " 61,\n",
       " 76,\n",
       " 63,\n",
       " 59,\n",
       " 69,\n",
       " 67,\n",
       " 72,\n",
       " 65,\n",
       " 105,\n",
       " 73,\n",
       " 64,\n",
       " 0,\n",
       " 59,\n",
       " 72,\n",
       " 0,\n",
       " 59,\n",
       " 72,\n",
       " 61,\n",
       " 66,\n",
       " 73,\n",
       " 76,\n",
       " 0,\n",
       " 70,\n",
       " 67,\n",
       " 72,\n",
       " 63,\n",
       " 106,\n",
       " 70,\n",
       " 78,\n",
       " 66,\n",
       " 73,\n",
       " 79,\n",
       " 65,\n",
       " 66,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 76,\n",
       " 63,\n",
       " 0,\n",
       " 81,\n",
       " 59,\n",
       " 77,\n",
       " 0,\n",
       " 72,\n",
       " 73,\n",
       " 0,\n",
       " 77,\n",
       " 73,\n",
       " 79,\n",
       " 72,\n",
       " 62,\n",
       " 0,\n",
       " 67,\n",
       " 72,\n",
       " 0,\n",
       " 59,\n",
       " 70,\n",
       " 70,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 66,\n",
       " 73,\n",
       " 79,\n",
       " 77,\n",
       " 63,\n",
       " 105,\n",
       " 67,\n",
       " 0,\n",
       " 61,\n",
       " 73,\n",
       " 79,\n",
       " 70,\n",
       " 62,\n",
       " 0,\n",
       " 72,\n",
       " 73,\n",
       " 78,\n",
       " 0,\n",
       " 64,\n",
       " 73,\n",
       " 76,\n",
       " 60,\n",
       " 63,\n",
       " 59,\n",
       " 76,\n",
       " 0,\n",
       " 70,\n",
       " 67,\n",
       " 77,\n",
       " 78,\n",
       " 63,\n",
       " 72,\n",
       " 67,\n",
       " 72,\n",
       " 65,\n",
       " 0,\n",
       " 64,\n",
       " 73,\n",
       " 76,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 61,\n",
       " 76,\n",
       " 83,\n",
       " 0,\n",
       " 73,\n",
       " 64,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 73,\n",
       " 77,\n",
       " 63,\n",
       " 0,\n",
       " 70,\n",
       " 73,\n",
       " 72,\n",
       " 65,\n",
       " 0,\n",
       " 81,\n",
       " 66,\n",
       " 67,\n",
       " 78,\n",
       " 63,\n",
       " 0,\n",
       " 76,\n",
       " 67,\n",
       " 74,\n",
       " 74,\n",
       " 70,\n",
       " 67,\n",
       " 72,\n",
       " 65,\n",
       " 0,\n",
       " 81,\n",
       " 59,\n",
       " 80,\n",
       " 63,\n",
       " 77,\n",
       " 105,\n",
       " 62,\n",
       " 76,\n",
       " 59,\n",
       " 65,\n",
       " 65,\n",
       " 67,\n",
       " 72,\n",
       " 65,\n",
       " 0,\n",
       " 79,\n",
       " 74,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 67,\n",
       " 76,\n",
       " 0,\n",
       " 77,\n",
       " 78,\n",
       " 76,\n",
       " 63,\n",
       " 72,\n",
       " 65,\n",
       " 78,\n",
       " 66,\n",
       " 0,\n",
       " 78,\n",
       " 73,\n",
       " 0,\n",
       " 60,\n",
       " 76,\n",
       " 63,\n",
       " 59,\n",
       " 69,\n",
       " 0,\n",
       " 73,\n",
       " 72,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 77,\n",
       " 79,\n",
       " 70,\n",
       " 70,\n",
       " 63,\n",
       " 72,\n",
       " 0,\n",
       " 60,\n",
       " 63,\n",
       " 59,\n",
       " 61,\n",
       " 66,\n",
       " 0,\n",
       " 73,\n",
       " 64,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 77,\n",
       " 69,\n",
       " 83,\n",
       " 106,\n",
       " 73,\n",
       " 81,\n",
       " 105,\n",
       " 61,\n",
       " 59,\n",
       " 72,\n",
       " 105,\n",
       " 81,\n",
       " 63,\n",
       " 0,\n",
       " 64,\n",
       " 67,\n",
       " 72,\n",
       " 62,\n",
       " 28,\n",
       " 106,\n",
       " 79,\n",
       " 67,\n",
       " 63,\n",
       " 78,\n",
       " 0,\n",
       " 77,\n",
       " 78,\n",
       " 67,\n",
       " 70,\n",
       " 70,\n",
       " 23,\n",
       " 0,\n",
       " 72,\n",
       " 73,\n",
       " 71,\n",
       " 105,\n",
       " 77,\n",
       " 65,\n",
       " 105,\n",
       " 65,\n",
       " 67,\n",
       " 64,\n",
       " 0,\n",
       " 66,\n",
       " 67,\n",
       " 71,\n",
       " 106,\n",
       " 67,\n",
       " 70,\n",
       " 70,\n",
       " 0,\n",
       " 63,\n",
       " 80,\n",
       " 63,\n",
       " 76,\n",
       " 83,\n",
       " 73,\n",
       " 72,\n",
       " 63,\n",
       " 0,\n",
       " 65,\n",
       " 73,\n",
       " 78,\n",
       " 0,\n",
       " 67,\n",
       " 78,\n",
       " 0,\n",
       " 73,\n",
       " 64,\n",
       " 64,\n",
       " 0,\n",
       " 60,\n",
       " 83,\n",
       " 0,\n",
       " 66,\n",
       " 63,\n",
       " 59,\n",
       " 76,\n",
       " 78,\n",
       " 105,\n",
       " 63,\n",
       " 80,\n",
       " 63,\n",
       " 76,\n",
       " 83,\n",
       " 0,\n",
       " 77,\n",
       " 78,\n",
       " 59,\n",
       " 76,\n",
       " 78,\n",
       " 0,\n",
       " 81,\n",
       " 59,\n",
       " 77,\n",
       " 0,\n",
       " 59,\n",
       " 0,\n",
       " 60,\n",
       " 59,\n",
       " 62,\n",
       " 0,\n",
       " 77,\n",
       " 78,\n",
       " 59,\n",
       " 76,\n",
       " 78,\n",
       " 105,\n",
       " 77,\n",
       " 67,\n",
       " 72,\n",
       " 61,\n",
       " 63,\n",
       " 0,\n",
       " 59,\n",
       " 70,\n",
       " 70,\n",
       " 0,\n",
       " 61,\n",
       " 73,\n",
       " 72,\n",
       " 61,\n",
       " 70,\n",
       " 79,\n",
       " 77,\n",
       " 67,\n",
       " 73,\n",
       " 72,\n",
       " 77,\n",
       " 0,\n",
       " 81,\n",
       " 63,\n",
       " 76,\n",
       " 63,\n",
       " 0,\n",
       " 64,\n",
       " 73,\n",
       " 76,\n",
       " 63,\n",
       " 65,\n",
       " 73,\n",
       " 72,\n",
       " 63,\n",
       " 106,\n",
       " 64,\n",
       " 78,\n",
       " 63,\n",
       " 76,\n",
       " 72,\n",
       " 73,\n",
       " 73,\n",
       " 72,\n",
       " 0,\n",
       " 63,\n",
       " 77,\n",
       " 61,\n",
       " 59,\n",
       " 74,\n",
       " 63,\n",
       " 105,\n",
       " 81,\n",
       " 67,\n",
       " 78,\n",
       " 66,\n",
       " 67,\n",
       " 72,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 74,\n",
       " 76,\n",
       " 67,\n",
       " 77,\n",
       " 73,\n",
       " 72,\n",
       " 0,\n",
       " 81,\n",
       " 59,\n",
       " 70,\n",
       " 70,\n",
       " 77,\n",
       " 105,\n",
       " 59,\n",
       " 0,\n",
       " 70,\n",
       " 67,\n",
       " 60,\n",
       " 76,\n",
       " 59,\n",
       " 76,\n",
       " 83,\n",
       " 106,\n",
       " 79,\n",
       " 78,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 59,\n",
       " 78,\n",
       " 105,\n",
       " 67,\n",
       " 77,\n",
       " 0,\n",
       " 72,\n",
       " 73,\n",
       " 78,\n",
       " 0,\n",
       " 81,\n",
       " 66,\n",
       " 59,\n",
       " 78,\n",
       " 0,\n",
       " 73,\n",
       " 61,\n",
       " 61,\n",
       " 79,\n",
       " 74,\n",
       " 67,\n",
       " 63,\n",
       " 77,\n",
       " 0,\n",
       " 71,\n",
       " 83,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 73,\n",
       " 79,\n",
       " 65,\n",
       " 66,\n",
       " 78,\n",
       " 77,\n",
       " 0,\n",
       " 59,\n",
       " 78,\n",
       " 0,\n",
       " 74,\n",
       " 76,\n",
       " 63,\n",
       " 77,\n",
       " 63,\n",
       " 72,\n",
       " 78,\n",
       " 105,\n",
       " 72,\n",
       " 73,\n",
       " 76,\n",
       " 0,\n",
       " 81,\n",
       " 66,\n",
       " 83,\n",
       " 0,\n",
       " 67,\n",
       " 0,\n",
       " 77,\n",
       " 63,\n",
       " 72,\n",
       " 78,\n",
       " 0,\n",
       " 64,\n",
       " 73,\n",
       " 76,\n",
       " 0,\n",
       " 83,\n",
       " 73,\n",
       " 79,\n",
       " 0,\n",
       " 71,\n",
       " 63,\n",
       " 77,\n",
       " 77,\n",
       " 63,\n",
       " 76,\n",
       " 0,\n",
       " 71,\n",
       " 67,\n",
       " 61,\n",
       " 66,\n",
       " 63,\n",
       " 70,\n",
       " 63,\n",
       " 106,\n",
       " 63,\n",
       " 59,\n",
       " 76,\n",
       " 67,\n",
       " 72,\n",
       " 65,\n",
       " 0,\n",
       " 59,\n",
       " 0,\n",
       " 81,\n",
       " 66,\n",
       " 67,\n",
       " 78,\n",
       " 63,\n",
       " 0,\n",
       " 71,\n",
       " 59,\n",
       " 77,\n",
       " 69,\n",
       " 105,\n",
       " 60,\n",
       " 63,\n",
       " 61,\n",
       " 59,\n",
       " 79,\n",
       " 77,\n",
       " 63,\n",
       " 0,\n",
       " 73,\n",
       " 64,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 74,\n",
       " 73,\n",
       " 70,\n",
       " 70,\n",
       " 79,\n",
       " 78,\n",
       " 67,\n",
       " 73,\n",
       " 72,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 105,\n",
       " 59,\n",
       " 0,\n",
       " 62,\n",
       " 59,\n",
       " 83,\n",
       " 0,\n",
       " 71,\n",
       " 73,\n",
       " 73,\n",
       " 72,\n",
       " 106,\n",
       " 61,\n",
       " 63,\n",
       " 72,\n",
       " 78,\n",
       " 0,\n",
       " 73,\n",
       " 64,\n",
       " 0,\n",
       " 59,\n",
       " 74,\n",
       " 74,\n",
       " 70,\n",
       " 63,\n",
       " 77,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 105,\n",
       " 64,\n",
       " 63,\n",
       " 63,\n",
       " 70,\n",
       " 0,\n",
       " 73,\n",
       " 64,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 74,\n",
       " 59,\n",
       " 76,\n",
       " 67,\n",
       " 72,\n",
       " 65,\n",
       " 0,\n",
       " 60,\n",
       " 70,\n",
       " 59,\n",
       " 62,\n",
       " 63,\n",
       " 105,\n",
       " 60,\n",
       " 63,\n",
       " 78,\n",
       " 81,\n",
       " 63,\n",
       " 63,\n",
       " 72,\n",
       " 0,\n",
       " 71,\n",
       " 83,\n",
       " 0,\n",
       " 64,\n",
       " 67,\n",
       " 72,\n",
       " 65,\n",
       " 63,\n",
       " 76,\n",
       " 77,\n",
       " 106,\n",
       " 76,\n",
       " 61,\n",
       " 66,\n",
       " 105,\n",
       " 5,\n",
       " 78,\n",
       " 0,\n",
       " 67,\n",
       " 77,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 59,\n",
       " 70,\n",
       " 66,\n",
       " 59,\n",
       " 71,\n",
       " 60,\n",
       " 76,\n",
       " 59,\n",
       " 105,\n",
       " 79,\n",
       " 72,\n",
       " 62,\n",
       " 63,\n",
       " 76,\n",
       " 0,\n",
       " 81,\n",
       " 66,\n",
       " 73,\n",
       " 77,\n",
       " 63,\n",
       " 0,\n",
       " 78,\n",
       " 73,\n",
       " 81,\n",
       " 63,\n",
       " 76,\n",
       " 77,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 65,\n",
       " 83,\n",
       " 74,\n",
       " 77,\n",
       " 83,\n",
       " 0,\n",
       " 61,\n",
       " 59,\n",
       " 71,\n",
       " 74,\n",
       " 0,\n",
       " 81,\n",
       " 59,\n",
       " 77,\n",
       " 0,\n",
       " 74,\n",
       " 67,\n",
       " 78,\n",
       " 61,\n",
       " 66,\n",
       " 63,\n",
       " 62,\n",
       " 106,\n",
       " 72,\n",
       " 0,\n",
       " 83,\n",
       " 73,\n",
       " 79,\n",
       " 0,\n",
       " 76,\n",
       " 79,\n",
       " 60,\n",
       " 83,\n",
       " 105,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 74,\n",
       " 59,\n",
       " 67,\n",
       " 72,\n",
       " 105,\n",
       " 83,\n",
       " 73,\n",
       " 79,\n",
       " 0,\n",
       " 81,\n",
       " 59,\n",
       " 69,\n",
       " 63,\n",
       " 0,\n",
       " 78,\n",
       " 73,\n",
       " 0,\n",
       " 67,\n",
       " 77,\n",
       " 0,\n",
       " 72,\n",
       " 73,\n",
       " 78,\n",
       " 0,\n",
       " 83,\n",
       " 73,\n",
       " 79,\n",
       " 76,\n",
       " 77,\n",
       " 106,\n",
       " 66,\n",
       " 76,\n",
       " 67,\n",
       " 77,\n",
       " 78,\n",
       " 71,\n",
       " 59,\n",
       " 77,\n",
       " 0,\n",
       " 63,\n",
       " 80,\n",
       " 63,\n",
       " 28,\n",
       " 105,\n",
       " 60,\n",
       " 76,\n",
       " 59,\n",
       " 72,\n",
       " 62,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "#X = np.reshape(poemX, (n_patterns, seq_length, n_poems))\n",
    "# normalize\n",
    "X = np.array([np.array([char / float(n_chars) for char in seq]) for poem in poemX for seq in poem])\n",
    "X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "# one hot encode the output variable\n",
    "y = to_categorical([nextchar for poem in poemY for nextchar in poem])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-f6c53b0fe054>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#X = np.reshape(poemX, (n_patterns, seq_length, n_poems))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# normalize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpusX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m# one hot encode the output variable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpusY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "#X = np.reshape(poemX, (n_patterns, seq_length, n_poems))\n",
    "# normalize\n",
    "X = np.reshape(X, (len(corpusX), seq_length, 1))\n",
    "# one hot encode the output variable\n",
    "y = to_categorical(corpusY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1723806, 5, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/200\n",
      "1723806/1723806 [==============================] - 182s 106us/step - loss: 3.0667\n",
      "\n",
      "Epoch 00001: loss improved from inf to 3.06671, saving model to letter-weights-01-3.0667.hdf5\n",
      "Epoch 2/200\n",
      "1723806/1723806 [==============================] - 1063s 617us/step - loss: 3.0615\n",
      "\n",
      "Epoch 00002: loss improved from 3.06671 to 3.06151, saving model to letter-weights-02-3.0615.hdf5\n",
      "Epoch 3/200\n",
      "1723806/1723806 [==============================] - 1341s 778us/step - loss: 3.0610\n",
      "\n",
      "Epoch 00003: loss improved from 3.06151 to 3.06097, saving model to letter-weights-03-3.0610.hdf5\n",
      "Epoch 4/200\n",
      "1723806/1723806 [==============================] - 1341s 778us/step - loss: 3.0606\n",
      "\n",
      "Epoch 00004: loss improved from 3.06097 to 3.06062, saving model to letter-weights-04-3.0606.hdf5\n",
      "Epoch 5/200\n",
      "1723806/1723806 [==============================] - 1342s 778us/step - loss: 3.0604\n",
      "\n",
      "Epoch 00005: loss improved from 3.06062 to 3.06036, saving model to letter-weights-05-3.0604.hdf5\n",
      "Epoch 6/200\n",
      "1723806/1723806 [==============================] - 1341s 778us/step - loss: 3.0602\n",
      "\n",
      "Epoch 00006: loss improved from 3.06036 to 3.06022, saving model to letter-weights-06-3.0602.hdf5\n",
      "Epoch 7/200\n",
      "1723806/1723806 [==============================] - 1343s 779us/step - loss: 3.0601\n",
      "\n",
      "Epoch 00007: loss improved from 3.06022 to 3.06009, saving model to letter-weights-07-3.0601.hdf5\n",
      "Epoch 8/200\n",
      "1723806/1723806 [==============================] - 1336s 775us/step - loss: 3.0600\n",
      "\n",
      "Epoch 00008: loss improved from 3.06009 to 3.05999, saving model to letter-weights-08-3.0600.hdf5\n",
      "Epoch 9/200\n",
      "1723806/1723806 [==============================] - 1344s 779us/step - loss: 3.0599\n",
      "\n",
      "Epoch 00009: loss improved from 3.05999 to 3.05992, saving model to letter-weights-09-3.0599.hdf5\n",
      "Epoch 10/200\n",
      "1723806/1723806 [==============================] - 1341s 778us/step - loss: 3.0599\n",
      "\n",
      "Epoch 00010: loss improved from 3.05992 to 3.05988, saving model to letter-weights-10-3.0599.hdf5\n",
      "Epoch 11/200\n",
      "1723806/1723806 [==============================] - 1341s 778us/step - loss: 3.0598\n",
      "\n",
      "Epoch 00011: loss improved from 3.05988 to 3.05977, saving model to letter-weights-11-3.0598.hdf5\n",
      "Epoch 12/200\n",
      "1723806/1723806 [==============================] - 1341s 778us/step - loss: 3.0598\n",
      "\n",
      "Epoch 00012: loss improved from 3.05977 to 3.05976, saving model to letter-weights-12-3.0598.hdf5\n",
      "Epoch 13/200\n",
      "1723806/1723806 [==============================] - 1343s 779us/step - loss: 3.0597\n",
      "\n",
      "Epoch 00013: loss improved from 3.05976 to 3.05970, saving model to letter-weights-13-3.0597.hdf5\n",
      "Epoch 14/200\n",
      "1723806/1723806 [==============================] - 1341s 778us/step - loss: 3.0596\n",
      "\n",
      "Epoch 00014: loss improved from 3.05970 to 3.05964, saving model to letter-weights-14-3.0596.hdf5\n",
      "Epoch 15/200\n",
      "1723806/1723806 [==============================] - 1342s 778us/step - loss: 3.0596\n",
      "\n",
      "Epoch 00015: loss improved from 3.05964 to 3.05959, saving model to letter-weights-15-3.0596.hdf5\n",
      "Epoch 16/200\n",
      "1723806/1723806 [==============================] - 1343s 779us/step - loss: 3.0595\n",
      "\n",
      "Epoch 00016: loss improved from 3.05959 to 3.05952, saving model to letter-weights-16-3.0595.hdf5\n",
      "Epoch 17/200\n",
      "1723806/1723806 [==============================] - 1341s 778us/step - loss: 3.0543\n",
      "\n",
      "Epoch 00017: loss improved from 3.05952 to 3.05433, saving model to letter-weights-17-3.0543.hdf5\n",
      "Epoch 18/200\n",
      "1723806/1723806 [==============================] - 1340s 778us/step - loss: 2.9453\n",
      "\n",
      "Epoch 00018: loss improved from 3.05433 to 2.94531, saving model to letter-weights-18-2.9453.hdf5\n",
      "Epoch 19/200\n",
      "1723806/1723806 [==============================] - 1341s 778us/step - loss: 2.8958\n",
      "\n",
      "Epoch 00019: loss improved from 2.94531 to 2.89581, saving model to letter-weights-19-2.8958.hdf5\n",
      "Epoch 20/200\n",
      "1723806/1723806 [==============================] - 1341s 778us/step - loss: 2.8672\n",
      "\n",
      "Epoch 00020: loss improved from 2.89581 to 2.86715, saving model to letter-weights-20-2.8672.hdf5\n",
      "Epoch 21/200\n",
      "1723806/1723806 [==============================] - 1341s 778us/step - loss: 2.8517\n",
      "\n",
      "Epoch 00021: loss improved from 2.86715 to 2.85169, saving model to letter-weights-21-2.8517.hdf5\n",
      "Epoch 22/200\n",
      "1723806/1723806 [==============================] - 1340s 777us/step - loss: 2.8352\n",
      "\n",
      "Epoch 00022: loss improved from 2.85169 to 2.83522, saving model to letter-weights-22-2.8352.hdf5\n",
      "Epoch 23/200\n",
      "1723806/1723806 [==============================] - 1340s 777us/step - loss: 2.8186\n",
      "\n",
      "Epoch 00023: loss improved from 2.83522 to 2.81863, saving model to letter-weights-23-2.8186.hdf5\n",
      "Epoch 24/200\n",
      "1723806/1723806 [==============================] - 1341s 778us/step - loss: 2.8046\n",
      "\n",
      "Epoch 00024: loss improved from 2.81863 to 2.80456, saving model to letter-weights-24-2.8046.hdf5\n",
      "Epoch 25/200\n",
      "1723806/1723806 [==============================] - 1340s 777us/step - loss: 2.7935\n",
      "\n",
      "Epoch 00025: loss improved from 2.80456 to 2.79348, saving model to letter-weights-25-2.7935.hdf5\n",
      "Epoch 26/200\n",
      "1723806/1723806 [==============================] - 1340s 777us/step - loss: 2.7836\n",
      "\n",
      "Epoch 00026: loss improved from 2.79348 to 2.78359, saving model to letter-weights-26-2.7836.hdf5\n",
      "Epoch 27/200\n",
      "1723806/1723806 [==============================] - 1343s 779us/step - loss: 2.7756\n",
      "\n",
      "Epoch 00027: loss improved from 2.78359 to 2.77558, saving model to letter-weights-27-2.7756.hdf5\n",
      "Epoch 28/200\n",
      "1723806/1723806 [==============================] - 1344s 780us/step - loss: 2.7660\n",
      "\n",
      "Epoch 00028: loss improved from 2.77558 to 2.76605, saving model to letter-weights-28-2.7660.hdf5\n",
      "Epoch 29/200\n",
      "1723806/1723806 [==============================] - 1344s 779us/step - loss: 2.7583\n",
      "\n",
      "Epoch 00029: loss improved from 2.76605 to 2.75827, saving model to letter-weights-29-2.7583.hdf5\n",
      "Epoch 30/200\n",
      "1723806/1723806 [==============================] - 1344s 780us/step - loss: 2.7499\n",
      "\n",
      "Epoch 00030: loss improved from 2.75827 to 2.74990, saving model to letter-weights-30-2.7499.hdf5\n",
      "Epoch 31/200\n",
      "1723806/1723806 [==============================] - 1344s 780us/step - loss: 2.7418\n",
      "\n",
      "Epoch 00031: loss improved from 2.74990 to 2.74175, saving model to letter-weights-31-2.7418.hdf5\n",
      "Epoch 32/200\n",
      "1723806/1723806 [==============================] - 1343s 779us/step - loss: 2.7345\n",
      "\n",
      "Epoch 00032: loss improved from 2.74175 to 2.73450, saving model to letter-weights-32-2.7345.hdf5\n",
      "Epoch 33/200\n",
      "1723806/1723806 [==============================] - 1346s 781us/step - loss: 2.7283\n",
      "\n",
      "Epoch 00033: loss improved from 2.73450 to 2.72826, saving model to letter-weights-33-2.7283.hdf5\n",
      "Epoch 34/200\n",
      "1723806/1723806 [==============================] - 1343s 779us/step - loss: 2.7228\n",
      "\n",
      "Epoch 00034: loss improved from 2.72826 to 2.72282, saving model to letter-weights-34-2.7228.hdf5\n",
      "Epoch 35/200\n",
      "1723806/1723806 [==============================] - 1343s 779us/step - loss: 2.7157\n",
      "\n",
      "Epoch 00035: loss improved from 2.72282 to 2.71568, saving model to letter-weights-35-2.7157.hdf5\n",
      "Epoch 36/200\n",
      "1723806/1723806 [==============================] - 1344s 779us/step - loss: 2.7095\n",
      "\n",
      "Epoch 00036: loss improved from 2.71568 to 2.70951, saving model to letter-weights-36-2.7095.hdf5\n",
      "Epoch 37/200\n",
      "1723806/1723806 [==============================] - 1344s 779us/step - loss: 2.7033\n",
      "\n",
      "Epoch 00037: loss improved from 2.70951 to 2.70329, saving model to letter-weights-37-2.7033.hdf5\n",
      "Epoch 38/200\n",
      "1723806/1723806 [==============================] - 1344s 779us/step - loss: 2.6967\n",
      "\n",
      "Epoch 00038: loss improved from 2.70329 to 2.69666, saving model to letter-weights-38-2.6967.hdf5\n",
      "Epoch 39/200\n",
      "1723806/1723806 [==============================] - 1343s 779us/step - loss: 2.6921\n",
      "\n",
      "Epoch 00039: loss improved from 2.69666 to 2.69214, saving model to letter-weights-39-2.6921.hdf5\n",
      "Epoch 40/200\n",
      "1723806/1723806 [==============================] - 1344s 780us/step - loss: 2.6855\n",
      "\n",
      "Epoch 00040: loss improved from 2.69214 to 2.68553, saving model to letter-weights-40-2.6855.hdf5\n",
      "Epoch 41/200\n",
      "1723806/1723806 [==============================] - 1343s 779us/step - loss: 2.6808\n",
      "\n",
      "Epoch 00041: loss improved from 2.68553 to 2.68077, saving model to letter-weights-41-2.6808.hdf5\n",
      "Epoch 42/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1723806/1723806 [==============================] - 1339s 777us/step - loss: 2.6748\n",
      "\n",
      "Epoch 00042: loss improved from 2.68077 to 2.67475, saving model to letter-weights-42-2.6748.hdf5\n",
      "Epoch 43/200\n",
      "1723806/1723806 [==============================] - 1339s 777us/step - loss: 2.6696\n",
      "\n",
      "Epoch 00043: loss improved from 2.67475 to 2.66957, saving model to letter-weights-43-2.6696.hdf5\n",
      "Epoch 44/200\n",
      "1723806/1723806 [==============================] - 1339s 777us/step - loss: 2.6640\n",
      "\n",
      "Epoch 00044: loss improved from 2.66957 to 2.66401, saving model to letter-weights-44-2.6640.hdf5\n",
      "Epoch 45/200\n",
      "1723806/1723806 [==============================] - 1339s 777us/step - loss: 2.6589\n",
      "\n",
      "Epoch 00045: loss improved from 2.66401 to 2.65886, saving model to letter-weights-45-2.6589.hdf5\n",
      "Epoch 46/200\n",
      "1723806/1723806 [==============================] - 1339s 777us/step - loss: 2.6549\n",
      "\n",
      "Epoch 00046: loss improved from 2.65886 to 2.65488, saving model to letter-weights-46-2.6549.hdf5\n",
      "Epoch 47/200\n",
      "1723806/1723806 [==============================] - 1339s 777us/step - loss: 2.6507\n",
      "\n",
      "Epoch 00047: loss improved from 2.65488 to 2.65070, saving model to letter-weights-47-2.6507.hdf5\n",
      "Epoch 48/200\n",
      "1723806/1723806 [==============================] - 1339s 777us/step - loss: 2.6453\n",
      "\n",
      "Epoch 00048: loss improved from 2.65070 to 2.64529, saving model to letter-weights-48-2.6453.hdf5\n",
      "Epoch 49/200\n",
      "1723806/1723806 [==============================] - 1339s 777us/step - loss: 2.6413\n",
      "\n",
      "Epoch 00049: loss improved from 2.64529 to 2.64126, saving model to letter-weights-49-2.6413.hdf5\n",
      "Epoch 50/200\n",
      "1723806/1723806 [==============================] - 1339s 777us/step - loss: 2.6367\n",
      "\n",
      "Epoch 00050: loss improved from 2.64126 to 2.63666, saving model to letter-weights-50-2.6367.hdf5\n",
      "Epoch 51/200\n",
      "1723806/1723806 [==============================] - 1339s 777us/step - loss: 2.6303\n",
      "\n",
      "Epoch 00051: loss improved from 2.63666 to 2.63031, saving model to letter-weights-51-2.6303.hdf5\n",
      "Epoch 52/200\n",
      "1723806/1723806 [==============================] - 1339s 777us/step - loss: 2.6274\n",
      "\n",
      "Epoch 00052: loss improved from 2.63031 to 2.62736, saving model to letter-weights-52-2.6274.hdf5\n",
      "Epoch 53/200\n",
      "1723806/1723806 [==============================] - 1347s 781us/step - loss: 2.6222\n",
      "\n",
      "Epoch 00053: loss improved from 2.62736 to 2.62218, saving model to letter-weights-53-2.6222.hdf5\n",
      "Epoch 54/200\n",
      "1723806/1723806 [==============================] - 1343s 779us/step - loss: 2.6171\n",
      "\n",
      "Epoch 00054: loss improved from 2.62218 to 2.61714, saving model to letter-weights-54-2.6171.hdf5\n",
      "Epoch 55/200\n",
      "1723806/1723806 [==============================] - 1344s 779us/step - loss: 2.6127\n",
      "\n",
      "Epoch 00055: loss improved from 2.61714 to 2.61269, saving model to letter-weights-55-2.6127.hdf5\n",
      "Epoch 56/200\n",
      "1723806/1723806 [==============================] - 1343s 779us/step - loss: 2.6082\n",
      "\n",
      "Epoch 00056: loss improved from 2.61269 to 2.60819, saving model to letter-weights-56-2.6082.hdf5\n",
      "Epoch 57/200\n",
      "1723806/1723806 [==============================] - 1344s 779us/step - loss: 2.6039\n",
      "\n",
      "Epoch 00057: loss improved from 2.60819 to 2.60394, saving model to letter-weights-57-2.6039.hdf5\n",
      "Epoch 58/200\n",
      "1723806/1723806 [==============================] - 1343s 779us/step - loss: 2.5991\n",
      "\n",
      "Epoch 00058: loss improved from 2.60394 to 2.59913, saving model to letter-weights-58-2.5991.hdf5\n",
      "Epoch 59/200\n",
      "1723806/1723806 [==============================] - 1344s 779us/step - loss: 2.5952\n",
      "\n",
      "Epoch 00059: loss improved from 2.59913 to 2.59516, saving model to letter-weights-59-2.5952.hdf5\n",
      "Epoch 60/200\n",
      "1723806/1723806 [==============================] - 1343s 779us/step - loss: 2.5920\n",
      "\n",
      "Epoch 00060: loss improved from 2.59516 to 2.59204, saving model to letter-weights-60-2.5920.hdf5\n",
      "Epoch 61/200\n",
      "1723806/1723806 [==============================] - 1343s 779us/step - loss: 2.5871\n",
      "\n",
      "Epoch 00061: loss improved from 2.59204 to 2.58706, saving model to letter-weights-61-2.5871.hdf5\n",
      "Epoch 62/200\n",
      "1723806/1723806 [==============================] - 1343s 779us/step - loss: 2.5835\n",
      "\n",
      "Epoch 00062: loss improved from 2.58706 to 2.58345, saving model to letter-weights-62-2.5835.hdf5\n",
      "Epoch 63/200\n",
      "1723806/1723806 [==============================] - 1326s 769us/step - loss: 2.5795\n",
      "\n",
      "Epoch 00063: loss improved from 2.58345 to 2.57952, saving model to letter-weights-63-2.5795.hdf5\n",
      "Epoch 64/200\n",
      "1723806/1723806 [==============================] - 1341s 778us/step - loss: 2.5748\n",
      "\n",
      "Epoch 00064: loss improved from 2.57952 to 2.57479, saving model to letter-weights-64-2.5748.hdf5\n",
      "Epoch 65/200\n",
      "1723806/1723806 [==============================] - 1341s 778us/step - loss: 2.5715\n",
      "\n",
      "Epoch 00065: loss improved from 2.57479 to 2.57153, saving model to letter-weights-65-2.5715.hdf5\n",
      "Epoch 66/200\n",
      "1723806/1723806 [==============================] - 1341s 778us/step - loss: 2.5681\n",
      "\n",
      "Epoch 00066: loss improved from 2.57153 to 2.56811, saving model to letter-weights-66-2.5681.hdf5\n",
      "Epoch 67/200\n",
      "1723806/1723806 [==============================] - 1341s 778us/step - loss: 2.5648\n",
      "\n",
      "Epoch 00067: loss improved from 2.56811 to 2.56481, saving model to letter-weights-67-2.5648.hdf5\n",
      "Epoch 68/200\n",
      "1723806/1723806 [==============================] - 1342s 778us/step - loss: 2.5606\n",
      "\n",
      "Epoch 00068: loss improved from 2.56481 to 2.56060, saving model to letter-weights-68-2.5606.hdf5\n",
      "Epoch 69/200\n",
      "1723806/1723806 [==============================] - 1342s 778us/step - loss: 2.5569\n",
      "\n",
      "Epoch 00069: loss improved from 2.56060 to 2.55687, saving model to letter-weights-69-2.5569.hdf5\n",
      "Epoch 70/200\n",
      "1723806/1723806 [==============================] - 1342s 778us/step - loss: 2.5520\n",
      "\n",
      "Epoch 00070: loss improved from 2.55687 to 2.55199, saving model to letter-weights-70-2.5520.hdf5\n",
      "Epoch 71/200\n",
      "1723806/1723806 [==============================] - 1342s 778us/step - loss: 2.5499\n",
      "\n",
      "Epoch 00071: loss improved from 2.55199 to 2.54985, saving model to letter-weights-71-2.5499.hdf5\n",
      "Epoch 72/200\n",
      "1723806/1723806 [==============================] - 1351s 784us/step - loss: 2.5454\n",
      "\n",
      "Epoch 00072: loss improved from 2.54985 to 2.54535, saving model to letter-weights-72-2.5454.hdf5\n",
      "Epoch 73/200\n",
      "1723806/1723806 [==============================] - 1342s 778us/step - loss: 2.5451\n",
      "\n",
      "Epoch 00073: loss improved from 2.54535 to 2.54514, saving model to letter-weights-73-2.5451.hdf5\n",
      "Epoch 74/200\n",
      "1723806/1723806 [==============================] - 1342s 779us/step - loss: 2.5403\n",
      "\n",
      "Epoch 00074: loss improved from 2.54514 to 2.54030, saving model to letter-weights-74-2.5403.hdf5\n",
      "Epoch 75/200\n",
      "1723806/1723806 [==============================] - 1342s 778us/step - loss: 2.5381\n",
      "\n",
      "Epoch 00075: loss improved from 2.54030 to 2.53810, saving model to letter-weights-75-2.5381.hdf5\n",
      "Epoch 76/200\n",
      "1723806/1723806 [==============================] - 1350s 783us/step - loss: 2.5334\n",
      "\n",
      "Epoch 00076: loss improved from 2.53810 to 2.53336, saving model to letter-weights-76-2.5334.hdf5\n",
      "Epoch 77/200\n",
      "1723806/1723806 [==============================] - 1343s 779us/step - loss: 2.5307\n",
      "\n",
      "Epoch 00077: loss improved from 2.53336 to 2.53074, saving model to letter-weights-77-2.5307.hdf5\n",
      "Epoch 78/200\n",
      "1723806/1723806 [==============================] - 1339s 777us/step - loss: 2.5295\n",
      "\n",
      "Epoch 00078: loss improved from 2.53074 to 2.52952, saving model to letter-weights-78-2.5295.hdf5\n",
      "Epoch 79/200\n",
      "1723806/1723806 [==============================] - 1322s 767us/step - loss: 2.5255\n",
      "\n",
      "Epoch 00079: loss improved from 2.52952 to 2.52548, saving model to letter-weights-79-2.5255.hdf5\n",
      "Epoch 80/200\n",
      "1723806/1723806 [==============================] - 1320s 766us/step - loss: 2.5234\n",
      "\n",
      "Epoch 00080: loss improved from 2.52548 to 2.52342, saving model to letter-weights-80-2.5234.hdf5\n",
      "Epoch 81/200\n",
      "1723806/1723806 [==============================] - 1320s 766us/step - loss: 2.5190\n",
      "\n",
      "Epoch 00081: loss improved from 2.52342 to 2.51898, saving model to letter-weights-81-2.5190.hdf5\n",
      "Epoch 82/200\n",
      "1723806/1723806 [==============================] - 1328s 770us/step - loss: 2.5162\n",
      "\n",
      "Epoch 00082: loss improved from 2.51898 to 2.51616, saving model to letter-weights-82-2.5162.hdf5\n",
      "Epoch 83/200\n",
      "1723806/1723806 [==============================] - 1342s 779us/step - loss: 2.5143\n",
      "\n",
      "Epoch 00083: loss improved from 2.51616 to 2.51429, saving model to letter-weights-83-2.5143.hdf5\n",
      "Epoch 84/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1723806/1723806 [==============================] - 1339s 777us/step - loss: 2.5116\n",
      "\n",
      "Epoch 00084: loss improved from 2.51429 to 2.51163, saving model to letter-weights-84-2.5116.hdf5\n",
      "Epoch 85/200\n",
      "1723806/1723806 [==============================] - 1339s 777us/step - loss: 2.5073\n",
      "\n",
      "Epoch 00085: loss improved from 2.51163 to 2.50735, saving model to letter-weights-85-2.5073.hdf5\n",
      "Epoch 86/200\n",
      "1723806/1723806 [==============================] - 1339s 777us/step - loss: 2.5066\n",
      "\n",
      "Epoch 00086: loss improved from 2.50735 to 2.50655, saving model to letter-weights-86-2.5066.hdf5\n",
      "Epoch 87/200\n",
      "1723806/1723806 [==============================] - 1339s 777us/step - loss: 2.5044\n",
      "\n",
      "Epoch 00087: loss improved from 2.50655 to 2.50437, saving model to letter-weights-87-2.5044.hdf5\n",
      "Epoch 88/200\n",
      "1723806/1723806 [==============================] - 1339s 777us/step - loss: 2.5004\n",
      "\n",
      "Epoch 00088: loss improved from 2.50437 to 2.50037, saving model to letter-weights-88-2.5004.hdf5\n",
      "Epoch 89/200\n",
      "1723806/1723806 [==============================] - 1340s 777us/step - loss: 2.4985\n",
      "\n",
      "Epoch 00089: loss improved from 2.50037 to 2.49852, saving model to letter-weights-89-2.4985.hdf5\n",
      "Epoch 90/200\n",
      "1723806/1723806 [==============================] - 1346s 781us/step - loss: 2.4975\n",
      "\n",
      "Epoch 00090: loss improved from 2.49852 to 2.49747, saving model to letter-weights-90-2.4975.hdf5\n",
      "Epoch 91/200\n",
      "1723806/1723806 [==============================] - 1371s 795us/step - loss: 2.4934\n",
      "\n",
      "Epoch 00091: loss improved from 2.49747 to 2.49341, saving model to letter-weights-91-2.4934.hdf5\n",
      "Epoch 92/200\n",
      "1723806/1723806 [==============================] - 1395s 809us/step - loss: 2.4922\n",
      "\n",
      "Epoch 00092: loss improved from 2.49341 to 2.49218, saving model to letter-weights-92-2.4922.hdf5\n",
      "Epoch 93/200\n",
      "1723806/1723806 [==============================] - 1395s 809us/step - loss: 2.4883\n",
      "\n",
      "Epoch 00093: loss improved from 2.49218 to 2.48832, saving model to letter-weights-93-2.4883.hdf5\n",
      "Epoch 94/200\n",
      "1723806/1723806 [==============================] - 1395s 809us/step - loss: 2.4869\n",
      "\n",
      "Epoch 00094: loss improved from 2.48832 to 2.48692, saving model to letter-weights-94-2.4869.hdf5\n",
      "Epoch 95/200\n",
      "1723806/1723806 [==============================] - 1396s 810us/step - loss: 2.4841\n",
      "\n",
      "Epoch 00095: loss improved from 2.48692 to 2.48406, saving model to letter-weights-95-2.4841.hdf5\n",
      "Epoch 96/200\n",
      "1723806/1723806 [==============================] - 1395s 809us/step - loss: 2.4811\n",
      "\n",
      "Epoch 00096: loss improved from 2.48406 to 2.48109, saving model to letter-weights-96-2.4811.hdf5\n",
      "Epoch 97/200\n",
      "1723806/1723806 [==============================] - 1384s 803us/step - loss: 2.4780\n",
      "\n",
      "Epoch 00097: loss improved from 2.48109 to 2.47799, saving model to letter-weights-97-2.4780.hdf5\n",
      "Epoch 98/200\n",
      "1723806/1723806 [==============================] - 1398s 811us/step - loss: 2.4760\n",
      "\n",
      "Epoch 00098: loss improved from 2.47799 to 2.47604, saving model to letter-weights-98-2.4760.hdf5\n",
      "Epoch 99/200\n",
      "1723806/1723806 [==============================] - 1396s 810us/step - loss: 2.4748\n",
      "\n",
      "Epoch 00099: loss improved from 2.47604 to 2.47485, saving model to letter-weights-99-2.4748.hdf5\n",
      "Epoch 100/200\n",
      "1723806/1723806 [==============================] - 1395s 809us/step - loss: 2.4703\n",
      "\n",
      "Epoch 00100: loss improved from 2.47485 to 2.47026, saving model to letter-weights-100-2.4703.hdf5\n",
      "Epoch 101/200\n",
      "1723806/1723806 [==============================] - 1394s 809us/step - loss: 2.4695\n",
      "\n",
      "Epoch 00101: loss improved from 2.47026 to 2.46945, saving model to letter-weights-101-2.4695.hdf5\n",
      "Epoch 102/200\n",
      "1723806/1723806 [==============================] - 1394s 809us/step - loss: 2.4667\n",
      "\n",
      "Epoch 00102: loss improved from 2.46945 to 2.46666, saving model to letter-weights-102-2.4667.hdf5\n",
      "Epoch 103/200\n",
      "1723806/1723806 [==============================] - 1387s 805us/step - loss: 2.4658\n",
      "\n",
      "Epoch 00103: loss improved from 2.46666 to 2.46580, saving model to letter-weights-103-2.4658.hdf5\n",
      "Epoch 104/200\n",
      "1723806/1723806 [==============================] - 1342s 779us/step - loss: 2.4628\n",
      "\n",
      "Epoch 00104: loss improved from 2.46580 to 2.46276, saving model to letter-weights-104-2.4628.hdf5\n",
      "Epoch 105/200\n",
      "1723806/1723806 [==============================] - 1342s 779us/step - loss: 2.4596\n",
      "\n",
      "Epoch 00105: loss improved from 2.46276 to 2.45963, saving model to letter-weights-105-2.4596.hdf5\n",
      "Epoch 106/200\n",
      "1723806/1723806 [==============================] - 1342s 779us/step - loss: 2.4572\n",
      "\n",
      "Epoch 00106: loss improved from 2.45963 to 2.45718, saving model to letter-weights-106-2.4572.hdf5\n",
      "Epoch 107/200\n",
      "1723806/1723806 [==============================] - 1342s 779us/step - loss: 2.4533\n",
      "\n",
      "Epoch 00107: loss improved from 2.45718 to 2.45334, saving model to letter-weights-107-2.4533.hdf5\n",
      "Epoch 108/200\n",
      "1723806/1723806 [==============================] - 1343s 779us/step - loss: 2.4532\n",
      "\n",
      "Epoch 00108: loss improved from 2.45334 to 2.45321, saving model to letter-weights-108-2.4532.hdf5\n",
      "Epoch 109/200\n",
      "1723806/1723806 [==============================] - 1342s 779us/step - loss: 2.4528\n",
      "\n",
      "Epoch 00109: loss improved from 2.45321 to 2.45278, saving model to letter-weights-109-2.4528.hdf5\n",
      "Epoch 110/200\n",
      "1723806/1723806 [==============================] - 1342s 779us/step - loss: 2.4488\n",
      "\n",
      "Epoch 00110: loss improved from 2.45278 to 2.44880, saving model to letter-weights-110-2.4488.hdf5\n",
      "Epoch 111/200\n",
      "1723806/1723806 [==============================] - 1342s 779us/step - loss: 2.4445\n",
      "\n",
      "Epoch 00111: loss improved from 2.44880 to 2.44449, saving model to letter-weights-111-2.4445.hdf5\n",
      "Epoch 112/200\n",
      "1723806/1723806 [==============================] - 1342s 779us/step - loss: 2.4440\n",
      "\n",
      "Epoch 00112: loss improved from 2.44449 to 2.44395, saving model to letter-weights-112-2.4440.hdf5\n",
      "Epoch 113/200\n",
      "1723806/1723806 [==============================] - 1342s 778us/step - loss: 2.4417\n",
      "\n",
      "Epoch 00113: loss improved from 2.44395 to 2.44174, saving model to letter-weights-113-2.4417.hdf5\n",
      "Epoch 114/200\n",
      "1723806/1723806 [==============================] - 1342s 778us/step - loss: 2.4394\n",
      "\n",
      "Epoch 00114: loss improved from 2.44174 to 2.43945, saving model to letter-weights-114-2.4394.hdf5\n",
      "Epoch 115/200\n",
      "1723806/1723806 [==============================] - 1342s 778us/step - loss: 2.4400\n",
      "\n",
      "Epoch 00115: loss did not improve from 2.43945\n",
      "Epoch 116/200\n",
      "1723806/1723806 [==============================] - 1342s 779us/step - loss: 2.4358\n",
      "\n",
      "Epoch 00116: loss improved from 2.43945 to 2.43584, saving model to letter-weights-116-2.4358.hdf5\n",
      "Epoch 117/200\n",
      "1723806/1723806 [==============================] - 1336s 775us/step - loss: 2.4350\n",
      "\n",
      "Epoch 00117: loss improved from 2.43584 to 2.43503, saving model to letter-weights-117-2.4350.hdf5\n",
      "Epoch 118/200\n",
      "1723806/1723806 [==============================] - 1333s 773us/step - loss: 2.4325\n",
      "\n",
      "Epoch 00118: loss improved from 2.43503 to 2.43252, saving model to letter-weights-118-2.4325.hdf5\n",
      "Epoch 119/200\n",
      "1723806/1723806 [==============================] - 1333s 773us/step - loss: 2.4298\n",
      "\n",
      "Epoch 00119: loss improved from 2.43252 to 2.42977, saving model to letter-weights-119-2.4298.hdf5\n",
      "Epoch 120/200\n",
      "1723806/1723806 [==============================] - 1333s 773us/step - loss: 2.4301\n",
      "\n",
      "Epoch 00120: loss did not improve from 2.42977\n",
      "Epoch 121/200\n",
      "1723806/1723806 [==============================] - 1333s 773us/step - loss: 2.4289\n",
      "\n",
      "Epoch 00121: loss improved from 2.42977 to 2.42891, saving model to letter-weights-121-2.4289.hdf5\n",
      "Epoch 122/200\n",
      "1723806/1723806 [==============================] - 1342s 778us/step - loss: 2.4254\n",
      "\n",
      "Epoch 00122: loss improved from 2.42891 to 2.42543, saving model to letter-weights-122-2.4254.hdf5\n",
      "Epoch 123/200\n",
      "1723806/1723806 [==============================] - 1358s 788us/step - loss: 2.4238\n",
      "\n",
      "Epoch 00123: loss improved from 2.42543 to 2.42376, saving model to letter-weights-123-2.4238.hdf5\n",
      "Epoch 124/200\n",
      "1723806/1723806 [==============================] - 1330s 771us/step - loss: 2.4219\n",
      "\n",
      "Epoch 00124: loss improved from 2.42376 to 2.42188, saving model to letter-weights-124-2.4219.hdf5\n",
      "Epoch 125/200\n",
      "1723806/1723806 [==============================] - 1323s 768us/step - loss: 2.4207\n",
      "\n",
      "Epoch 00125: loss improved from 2.42188 to 2.42070, saving model to letter-weights-125-2.4207.hdf5\n",
      "Epoch 126/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1723806/1723806 [==============================] - 1323s 768us/step - loss: 2.4187\n",
      "\n",
      "Epoch 00126: loss improved from 2.42070 to 2.41866, saving model to letter-weights-126-2.4187.hdf5\n",
      "Epoch 127/200\n",
      "1723806/1723806 [==============================] - 1321s 767us/step - loss: 2.4168\n",
      "\n",
      "Epoch 00127: loss improved from 2.41866 to 2.41677, saving model to letter-weights-127-2.4168.hdf5\n",
      "Epoch 128/200\n",
      "1723806/1723806 [==============================] - 1321s 766us/step - loss: 2.4142\n",
      "\n",
      "Epoch 00128: loss improved from 2.41677 to 2.41424, saving model to letter-weights-128-2.4142.hdf5\n",
      "Epoch 129/200\n",
      "1723806/1723806 [==============================] - 1324s 768us/step - loss: 2.4137\n",
      "\n",
      "Epoch 00129: loss improved from 2.41424 to 2.41365, saving model to letter-weights-129-2.4137.hdf5\n",
      "Epoch 130/200\n",
      "1723806/1723806 [==============================] - 1321s 766us/step - loss: 2.4117\n",
      "\n",
      "Epoch 00130: loss improved from 2.41365 to 2.41167, saving model to letter-weights-130-2.4117.hdf5\n",
      "Epoch 131/200\n",
      "1723806/1723806 [==============================] - 1322s 767us/step - loss: 2.4115\n",
      "\n",
      "Epoch 00131: loss improved from 2.41167 to 2.41151, saving model to letter-weights-131-2.4115.hdf5\n",
      "Epoch 132/200\n",
      "1723806/1723806 [==============================] - 1321s 766us/step - loss: 2.4074\n",
      "\n",
      "Epoch 00132: loss improved from 2.41151 to 2.40742, saving model to letter-weights-132-2.4074.hdf5\n",
      "Epoch 133/200\n",
      " 173952/1723806 [==>...........................] - ETA: 20:01 - loss: 2.4046"
     ]
    }
   ],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"letter-weights-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.fit(X, y, epochs=200, batch_size=128, callbacks=callbacks_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
