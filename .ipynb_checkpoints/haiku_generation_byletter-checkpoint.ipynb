{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "RANDOM = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "haikus_train_df = pd.read_pickle('./data/haikus_train_df.pickle')\n",
    "haikus_test_df = pd.read_pickle('./data/haikus_test_df.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Input, LSTM, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoetryGenerator():\n",
    "    \n",
    "    def __init__(self, engine, tokenize=):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.engine = engine\n",
    "        self.tokenize = tokenize\n",
    "    \n",
    "    def load_corpus(self, test, train, special_tokens):\n",
    "        \"\"\"\n",
    "        Assumes that special tokens have already been put into the dataset\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def transform(self, corpus, seq_len)\n",
    "    \n",
    "        poem_count = len(corpus)\n",
    "        self.pattern_count = 0\n",
    "        \n",
    "        # prepare the dataset of input to output pairs encoded as integers\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.poemX = []\n",
    "        self.poemY = []\n",
    "        self.pattern_count = 0\n",
    "\n",
    "        self.corpusX = []\n",
    "        self.corpusY = []\n",
    "        for poem_index in range(0, poem_count):\n",
    "\n",
    "            textX = []\n",
    "            textY = []\n",
    "            \n",
    "            poem = corpus[poem_index]\n",
    "            # add padding to poem\n",
    "            poem = list(np.full(seq_length - 1, '')) + list(poem)\n",
    "            \n",
    "            for i in range(0,  len(poem) - seq_length, 1):\n",
    "                seq_in = poem[i:i + seq_len]\n",
    "                seq_out = poem[i + seq_len]\n",
    "                textX.append([self.token_to_int[token] for token in seq_in])\n",
    "                textY.append(self.token_to_int[seq_out])\n",
    "\n",
    "            self.pattern_count = max(self.pattern_count, len(textX))\n",
    "\n",
    "            self.poemX.append(textX)\n",
    "            self.poemY.append(textY)\n",
    "\n",
    "            self.corpusX += textX\n",
    "            self.corpusY += textY\n",
    "    \n",
    "    def create_dict(self):\n",
    "        \n",
    "        # create corpus_raw\n",
    "        \n",
    "        self.tokens = sorted(set(list(corpus_raw)))\n",
    "        self.token_to_int = dict((t, i) for i, t in enumerate(self.tokens))\n",
    "        self.int_to_token = dict((i, t) for i, t in enumerate(self.tokens))\n",
    "        \n",
    "        self.token_count = len(corpus_raw)\n",
    "        self.vocab_count = len(tokens)\n",
    "\n",
    "    \n",
    "    def fit(self, seq_len=10):\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.fitted = True\n",
    "    \n",
    "    def generate(self, temperature=1.0):\n",
    "        \n",
    "        if not self.fitted:\n",
    "            raise ValueError('Model not fitted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoetryGenByWord(PoetryGenerator):\n",
    "    \n",
    "    def __init__(self, engine)\n",
    "    \n",
    "        super().__init__(engine, tokenize='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  1849446\n",
      "Total Vocab:  107\n"
     ]
    }
   ],
   "source": [
    "corpus_raw = ''.join(haikus_train_df['textchar_withtokens'])\n",
    "\n",
    "chars = sorted(set(list(corpus_raw)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "n_chars = len(corpus_raw)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_int[''] = n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25128"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_poems = len(haikus_train_df)\n",
    "\n",
    "n_poems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "i\n",
    "love\n",
    "you\n",
    "\n",
    "\n",
    "hello\n",
    "world\n",
    "\n",
    "---\n",
    "\n",
    "seq = 5\n",
    "\n",
    "[0 0 0 0 i] -> \\n\n",
    "[0 0 0 i \\n] -> love\n",
    "[0 0 i \\n love] -> \\n\n",
    "...\n",
    "[ ... \\n you] -> \\end\n",
    "[0 0 0 0 hello] -> \\n\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max patterns per poem:  801\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 40\n",
    "\n",
    "poemX = []\n",
    "poemY = []\n",
    "n_patterns = 0\n",
    "\n",
    "corpusX = []\n",
    "corpusY = []\n",
    "for poem_index in range(0, n_poems):\n",
    "\n",
    "    textX = []\n",
    "    textY = []\n",
    "    poem = haikus_train_df['textchar_withtokens'].iloc[poem_index]\n",
    "    # add padding to poem\n",
    "    poem = list(np.full(seq_length - 1, '')) + list(poem)\n",
    "    for i in range(0,  len(poem) - seq_length, 1):\n",
    "        seq_in = poem[i:i + seq_length]\n",
    "        seq_out = poem[i + seq_length]\n",
    "        textX.append([char_to_int[char] for char in seq_in])\n",
    "        textY.append(char_to_int[seq_out])\n",
    "\n",
    "    n_patterns = max(n_patterns, len(textX))\n",
    "    \n",
    "    poemX.append(textX)\n",
    "    poemY.append(textY)\n",
    "    \n",
    "    corpusX += textX\n",
    "    corpusY += textY\n",
    "\n",
    "print(\"Max patterns per poem: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoem_charindex = char_to_int['◘']\n",
    "newline_charindex = char_to_int['↕']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>...</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>...</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>59</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>...</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>59</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>...</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>59</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>...</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>59</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1824313</th>\n",
       "      <td>73</td>\n",
       "      <td>79</td>\n",
       "      <td>65</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>78</td>\n",
       "      <td>73</td>\n",
       "      <td>80</td>\n",
       "      <td>63</td>\n",
       "      <td>...</td>\n",
       "      <td>81</td>\n",
       "      <td>73</td>\n",
       "      <td>76</td>\n",
       "      <td>70</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>66</td>\n",
       "      <td>67</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1824314</th>\n",
       "      <td>79</td>\n",
       "      <td>65</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>78</td>\n",
       "      <td>73</td>\n",
       "      <td>80</td>\n",
       "      <td>63</td>\n",
       "      <td>74</td>\n",
       "      <td>...</td>\n",
       "      <td>73</td>\n",
       "      <td>76</td>\n",
       "      <td>70</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>66</td>\n",
       "      <td>67</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1824315</th>\n",
       "      <td>65</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>78</td>\n",
       "      <td>73</td>\n",
       "      <td>80</td>\n",
       "      <td>63</td>\n",
       "      <td>74</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>76</td>\n",
       "      <td>70</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>66</td>\n",
       "      <td>67</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1824316</th>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>78</td>\n",
       "      <td>73</td>\n",
       "      <td>80</td>\n",
       "      <td>63</td>\n",
       "      <td>74</td>\n",
       "      <td>67</td>\n",
       "      <td>74</td>\n",
       "      <td>...</td>\n",
       "      <td>70</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>66</td>\n",
       "      <td>67</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>63</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1824317</th>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>78</td>\n",
       "      <td>73</td>\n",
       "      <td>80</td>\n",
       "      <td>63</td>\n",
       "      <td>74</td>\n",
       "      <td>67</td>\n",
       "      <td>74</td>\n",
       "      <td>63</td>\n",
       "      <td>...</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>66</td>\n",
       "      <td>67</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>63</td>\n",
       "      <td>76</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1824318 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    1    2    3    4    5    6    7    8    9   ...   30   31   32  \\\n",
       "0        107  107  107  107  107  107  107  107  107  107  ...  107  107  107   \n",
       "1        107  107  107  107  107  107  107  107  107  107  ...  107  107  107   \n",
       "2        107  107  107  107  107  107  107  107  107  107  ...  107  107  107   \n",
       "3        107  107  107  107  107  107  107  107  107  107  ...  107  107  107   \n",
       "4        107  107  107  107  107  107  107  107  107  107  ...  107  107  107   \n",
       "...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "1824313   73   79   65   66    0   77   78   73   80   63  ...   81   73   76   \n",
       "1824314   79   65   66    0   77   78   73   80   63   74  ...   73   76   70   \n",
       "1824315   65   66    0   77   78   73   80   63   74   67  ...   76   70   62   \n",
       "1824316   66    0   77   78   73   80   63   74   67   74  ...   70   62    0   \n",
       "1824317    0   77   78   73   80   63   74   67   74   63  ...   62    0   77   \n",
       "\n",
       "          33   34   35   36   37   38  39  \n",
       "0        107  107  107  107  107  107  59  \n",
       "1        107  107  107  107  107   59  72  \n",
       "2        107  107  107  107   59   72   0  \n",
       "3        107  107  107   59   72    0  73  \n",
       "4        107  107   59   72    0   73  59  \n",
       "...      ...  ...  ...  ...  ...  ...  ..  \n",
       "1824313   70   62    0   77   66   67  71  \n",
       "1824314   62    0   77   66   67   71  71  \n",
       "1824315    0   77   66   67   71   71  63  \n",
       "1824316   77   66   67   71   71   63  76  \n",
       "1824317   66   67   71   71   63   76  77  \n",
       "\n",
       "[1824318 rows x 40 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(corpusX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72,\n",
       " 0,\n",
       " 73,\n",
       " 59,\n",
       " 77,\n",
       " 67,\n",
       " 77,\n",
       " 105,\n",
       " 67,\n",
       " 72,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 30,\n",
       " 67,\n",
       " 60,\n",
       " 70,\n",
       " 63,\n",
       " 0,\n",
       " 30,\n",
       " 63,\n",
       " 70,\n",
       " 78,\n",
       " 0,\n",
       " 10,\n",
       " 10,\n",
       " 105,\n",
       " 59,\n",
       " 62,\n",
       " 79,\n",
       " 70,\n",
       " 78,\n",
       " 0,\n",
       " 60,\n",
       " 73,\n",
       " 73,\n",
       " 69,\n",
       " 0,\n",
       " 77,\n",
       " 78,\n",
       " 73,\n",
       " 76,\n",
       " 63,\n",
       " 106,\n",
       " 71,\n",
       " 73,\n",
       " 76,\n",
       " 63,\n",
       " 78,\n",
       " 78,\n",
       " 67,\n",
       " 0,\n",
       " 77,\n",
       " 73,\n",
       " 72,\n",
       " 72,\n",
       " 63,\n",
       " 78,\n",
       " 0,\n",
       " 82,\n",
       " 82,\n",
       " 80,\n",
       " 67,\n",
       " 105,\n",
       " 63,\n",
       " 105,\n",
       " 77,\n",
       " 74,\n",
       " 63,\n",
       " 72,\n",
       " 77,\n",
       " 63,\n",
       " 76,\n",
       " 106,\n",
       " 66,\n",
       " 63,\n",
       " 76,\n",
       " 63,\n",
       " 0,\n",
       " 81,\n",
       " 66,\n",
       " 63,\n",
       " 72,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 83,\n",
       " 0,\n",
       " 61,\n",
       " 59,\n",
       " 71,\n",
       " 63,\n",
       " 0,\n",
       " 71,\n",
       " 67,\n",
       " 72,\n",
       " 62,\n",
       " 0,\n",
       " 77,\n",
       " 79,\n",
       " 64,\n",
       " 64,\n",
       " 63,\n",
       " 76,\n",
       " 63,\n",
       " 62,\n",
       " 0,\n",
       " 77,\n",
       " 66,\n",
       " 59,\n",
       " 71,\n",
       " 63,\n",
       " 105,\n",
       " 58,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 77,\n",
       " 63,\n",
       " 0,\n",
       " 60,\n",
       " 63,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 77,\n",
       " 59,\n",
       " 71,\n",
       " 63,\n",
       " 0,\n",
       " 59,\n",
       " 72,\n",
       " 62,\n",
       " 0,\n",
       " 72,\n",
       " 73,\n",
       " 78,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 77,\n",
       " 59,\n",
       " 71,\n",
       " 63,\n",
       " 105,\n",
       " 59,\n",
       " 10,\n",
       " 81,\n",
       " 73,\n",
       " 72,\n",
       " 62,\n",
       " 63,\n",
       " 76,\n",
       " 67,\n",
       " 72,\n",
       " 65,\n",
       " 0,\n",
       " 81,\n",
       " 66,\n",
       " 67,\n",
       " 77,\n",
       " 74,\n",
       " 63,\n",
       " 76,\n",
       " 63,\n",
       " 62,\n",
       " 0,\n",
       " 71,\n",
       " 67,\n",
       " 72,\n",
       " 62,\n",
       " 106,\n",
       " 62,\n",
       " 70,\n",
       " 63,\n",
       " 0,\n",
       " 61,\n",
       " 73,\n",
       " 72,\n",
       " 80,\n",
       " 63,\n",
       " 76,\n",
       " 77,\n",
       " 59,\n",
       " 78,\n",
       " 67,\n",
       " 73,\n",
       " 72,\n",
       " 105,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 62,\n",
       " 59,\n",
       " 64,\n",
       " 64,\n",
       " 73,\n",
       " 62,\n",
       " 67,\n",
       " 70,\n",
       " 77,\n",
       " 0,\n",
       " 72,\n",
       " 73,\n",
       " 62,\n",
       " 62,\n",
       " 67,\n",
       " 72,\n",
       " 65,\n",
       " 105,\n",
       " 37,\n",
       " 72,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 60,\n",
       " 76,\n",
       " 63,\n",
       " 63,\n",
       " 84,\n",
       " 63,\n",
       " 106,\n",
       " 67,\n",
       " 80,\n",
       " 63,\n",
       " 76,\n",
       " 0,\n",
       " 64,\n",
       " 73,\n",
       " 65,\n",
       " 105,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 77,\n",
       " 70,\n",
       " 73,\n",
       " 81,\n",
       " 0,\n",
       " 61,\n",
       " 76,\n",
       " 63,\n",
       " 59,\n",
       " 69,\n",
       " 67,\n",
       " 72,\n",
       " 65,\n",
       " 105,\n",
       " 73,\n",
       " 64,\n",
       " 0,\n",
       " 59,\n",
       " 72,\n",
       " 0,\n",
       " 59,\n",
       " 72,\n",
       " 61,\n",
       " 66,\n",
       " 73,\n",
       " 76,\n",
       " 0,\n",
       " 70,\n",
       " 67,\n",
       " 72,\n",
       " 63,\n",
       " 106,\n",
       " 70,\n",
       " 78,\n",
       " 66,\n",
       " 73,\n",
       " 79,\n",
       " 65,\n",
       " 66,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 76,\n",
       " 63,\n",
       " 0,\n",
       " 81,\n",
       " 59,\n",
       " 77,\n",
       " 0,\n",
       " 72,\n",
       " 73,\n",
       " 0,\n",
       " 77,\n",
       " 73,\n",
       " 79,\n",
       " 72,\n",
       " 62,\n",
       " 0,\n",
       " 67,\n",
       " 72,\n",
       " 0,\n",
       " 59,\n",
       " 70,\n",
       " 70,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 66,\n",
       " 73,\n",
       " 79,\n",
       " 77,\n",
       " 63,\n",
       " 105,\n",
       " 67,\n",
       " 0,\n",
       " 61,\n",
       " 73,\n",
       " 79,\n",
       " 70,\n",
       " 62,\n",
       " 0,\n",
       " 72,\n",
       " 73,\n",
       " 78,\n",
       " 0,\n",
       " 64,\n",
       " 73,\n",
       " 76,\n",
       " 60,\n",
       " 63,\n",
       " 59,\n",
       " 76,\n",
       " 0,\n",
       " 70,\n",
       " 67,\n",
       " 77,\n",
       " 78,\n",
       " 63,\n",
       " 72,\n",
       " 67,\n",
       " 72,\n",
       " 65,\n",
       " 0,\n",
       " 64,\n",
       " 73,\n",
       " 76,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 61,\n",
       " 76,\n",
       " 83,\n",
       " 0,\n",
       " 73,\n",
       " 64,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 73,\n",
       " 77,\n",
       " 63,\n",
       " 0,\n",
       " 70,\n",
       " 73,\n",
       " 72,\n",
       " 65,\n",
       " 0,\n",
       " 81,\n",
       " 66,\n",
       " 67,\n",
       " 78,\n",
       " 63,\n",
       " 0,\n",
       " 76,\n",
       " 67,\n",
       " 74,\n",
       " 74,\n",
       " 70,\n",
       " 67,\n",
       " 72,\n",
       " 65,\n",
       " 0,\n",
       " 81,\n",
       " 59,\n",
       " 80,\n",
       " 63,\n",
       " 77,\n",
       " 105,\n",
       " 62,\n",
       " 76,\n",
       " 59,\n",
       " 65,\n",
       " 65,\n",
       " 67,\n",
       " 72,\n",
       " 65,\n",
       " 0,\n",
       " 79,\n",
       " 74,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 67,\n",
       " 76,\n",
       " 0,\n",
       " 77,\n",
       " 78,\n",
       " 76,\n",
       " 63,\n",
       " 72,\n",
       " 65,\n",
       " 78,\n",
       " 66,\n",
       " 0,\n",
       " 78,\n",
       " 73,\n",
       " 0,\n",
       " 60,\n",
       " 76,\n",
       " 63,\n",
       " 59,\n",
       " 69,\n",
       " 0,\n",
       " 73,\n",
       " 72,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 77,\n",
       " 79,\n",
       " 70,\n",
       " 70,\n",
       " 63,\n",
       " 72,\n",
       " 0,\n",
       " 60,\n",
       " 63,\n",
       " 59,\n",
       " 61,\n",
       " 66,\n",
       " 0,\n",
       " 73,\n",
       " 64,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 77,\n",
       " 69,\n",
       " 83,\n",
       " 106,\n",
       " 73,\n",
       " 81,\n",
       " 105,\n",
       " 61,\n",
       " 59,\n",
       " 72,\n",
       " 105,\n",
       " 81,\n",
       " 63,\n",
       " 0,\n",
       " 64,\n",
       " 67,\n",
       " 72,\n",
       " 62,\n",
       " 28,\n",
       " 106,\n",
       " 79,\n",
       " 67,\n",
       " 63,\n",
       " 78,\n",
       " 0,\n",
       " 77,\n",
       " 78,\n",
       " 67,\n",
       " 70,\n",
       " 70,\n",
       " 23,\n",
       " 0,\n",
       " 72,\n",
       " 73,\n",
       " 71,\n",
       " 105,\n",
       " 77,\n",
       " 65,\n",
       " 105,\n",
       " 65,\n",
       " 67,\n",
       " 64,\n",
       " 0,\n",
       " 66,\n",
       " 67,\n",
       " 71,\n",
       " 106,\n",
       " 67,\n",
       " 70,\n",
       " 70,\n",
       " 0,\n",
       " 63,\n",
       " 80,\n",
       " 63,\n",
       " 76,\n",
       " 83,\n",
       " 73,\n",
       " 72,\n",
       " 63,\n",
       " 0,\n",
       " 65,\n",
       " 73,\n",
       " 78,\n",
       " 0,\n",
       " 67,\n",
       " 78,\n",
       " 0,\n",
       " 73,\n",
       " 64,\n",
       " 64,\n",
       " 0,\n",
       " 60,\n",
       " 83,\n",
       " 0,\n",
       " 66,\n",
       " 63,\n",
       " 59,\n",
       " 76,\n",
       " 78,\n",
       " 105,\n",
       " 63,\n",
       " 80,\n",
       " 63,\n",
       " 76,\n",
       " 83,\n",
       " 0,\n",
       " 77,\n",
       " 78,\n",
       " 59,\n",
       " 76,\n",
       " 78,\n",
       " 0,\n",
       " 81,\n",
       " 59,\n",
       " 77,\n",
       " 0,\n",
       " 59,\n",
       " 0,\n",
       " 60,\n",
       " 59,\n",
       " 62,\n",
       " 0,\n",
       " 77,\n",
       " 78,\n",
       " 59,\n",
       " 76,\n",
       " 78,\n",
       " 105,\n",
       " 77,\n",
       " 67,\n",
       " 72,\n",
       " 61,\n",
       " 63,\n",
       " 0,\n",
       " 59,\n",
       " 70,\n",
       " 70,\n",
       " 0,\n",
       " 61,\n",
       " 73,\n",
       " 72,\n",
       " 61,\n",
       " 70,\n",
       " 79,\n",
       " 77,\n",
       " 67,\n",
       " 73,\n",
       " 72,\n",
       " 77,\n",
       " 0,\n",
       " 81,\n",
       " 63,\n",
       " 76,\n",
       " 63,\n",
       " 0,\n",
       " 64,\n",
       " 73,\n",
       " 76,\n",
       " 63,\n",
       " 65,\n",
       " 73,\n",
       " 72,\n",
       " 63,\n",
       " 106,\n",
       " 64,\n",
       " 78,\n",
       " 63,\n",
       " 76,\n",
       " 72,\n",
       " 73,\n",
       " 73,\n",
       " 72,\n",
       " 0,\n",
       " 63,\n",
       " 77,\n",
       " 61,\n",
       " 59,\n",
       " 74,\n",
       " 63,\n",
       " 105,\n",
       " 81,\n",
       " 67,\n",
       " 78,\n",
       " 66,\n",
       " 67,\n",
       " 72,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 74,\n",
       " 76,\n",
       " 67,\n",
       " 77,\n",
       " 73,\n",
       " 72,\n",
       " 0,\n",
       " 81,\n",
       " 59,\n",
       " 70,\n",
       " 70,\n",
       " 77,\n",
       " 105,\n",
       " 59,\n",
       " 0,\n",
       " 70,\n",
       " 67,\n",
       " 60,\n",
       " 76,\n",
       " 59,\n",
       " 76,\n",
       " 83,\n",
       " 106,\n",
       " 79,\n",
       " 78,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 59,\n",
       " 78,\n",
       " 105,\n",
       " 67,\n",
       " 77,\n",
       " 0,\n",
       " 72,\n",
       " 73,\n",
       " 78,\n",
       " 0,\n",
       " 81,\n",
       " 66,\n",
       " 59,\n",
       " 78,\n",
       " 0,\n",
       " 73,\n",
       " 61,\n",
       " 61,\n",
       " 79,\n",
       " 74,\n",
       " 67,\n",
       " 63,\n",
       " 77,\n",
       " 0,\n",
       " 71,\n",
       " 83,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 73,\n",
       " 79,\n",
       " 65,\n",
       " 66,\n",
       " 78,\n",
       " 77,\n",
       " 0,\n",
       " 59,\n",
       " 78,\n",
       " 0,\n",
       " 74,\n",
       " 76,\n",
       " 63,\n",
       " 77,\n",
       " 63,\n",
       " 72,\n",
       " 78,\n",
       " 105,\n",
       " 72,\n",
       " 73,\n",
       " 76,\n",
       " 0,\n",
       " 81,\n",
       " 66,\n",
       " 83,\n",
       " 0,\n",
       " 67,\n",
       " 0,\n",
       " 77,\n",
       " 63,\n",
       " 72,\n",
       " 78,\n",
       " 0,\n",
       " 64,\n",
       " 73,\n",
       " 76,\n",
       " 0,\n",
       " 83,\n",
       " 73,\n",
       " 79,\n",
       " 0,\n",
       " 71,\n",
       " 63,\n",
       " 77,\n",
       " 77,\n",
       " 63,\n",
       " 76,\n",
       " 0,\n",
       " 71,\n",
       " 67,\n",
       " 61,\n",
       " 66,\n",
       " 63,\n",
       " 70,\n",
       " 63,\n",
       " 106,\n",
       " 63,\n",
       " 59,\n",
       " 76,\n",
       " 67,\n",
       " 72,\n",
       " 65,\n",
       " 0,\n",
       " 59,\n",
       " 0,\n",
       " 81,\n",
       " 66,\n",
       " 67,\n",
       " 78,\n",
       " 63,\n",
       " 0,\n",
       " 71,\n",
       " 59,\n",
       " 77,\n",
       " 69,\n",
       " 105,\n",
       " 60,\n",
       " 63,\n",
       " 61,\n",
       " 59,\n",
       " 79,\n",
       " 77,\n",
       " 63,\n",
       " 0,\n",
       " 73,\n",
       " 64,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 74,\n",
       " 73,\n",
       " 70,\n",
       " 70,\n",
       " 79,\n",
       " 78,\n",
       " 67,\n",
       " 73,\n",
       " 72,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 105,\n",
       " 59,\n",
       " 0,\n",
       " 62,\n",
       " 59,\n",
       " 83,\n",
       " 0,\n",
       " 71,\n",
       " 73,\n",
       " 73,\n",
       " 72,\n",
       " 106,\n",
       " 61,\n",
       " 63,\n",
       " 72,\n",
       " 78,\n",
       " 0,\n",
       " 73,\n",
       " 64,\n",
       " 0,\n",
       " 59,\n",
       " 74,\n",
       " 74,\n",
       " 70,\n",
       " 63,\n",
       " 77,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 105,\n",
       " 64,\n",
       " 63,\n",
       " 63,\n",
       " 70,\n",
       " 0,\n",
       " 73,\n",
       " 64,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 74,\n",
       " 59,\n",
       " 76,\n",
       " 67,\n",
       " 72,\n",
       " 65,\n",
       " 0,\n",
       " 60,\n",
       " 70,\n",
       " 59,\n",
       " 62,\n",
       " 63,\n",
       " 105,\n",
       " 60,\n",
       " 63,\n",
       " 78,\n",
       " 81,\n",
       " 63,\n",
       " 63,\n",
       " 72,\n",
       " 0,\n",
       " 71,\n",
       " 83,\n",
       " 0,\n",
       " 64,\n",
       " 67,\n",
       " 72,\n",
       " 65,\n",
       " 63,\n",
       " 76,\n",
       " 77,\n",
       " 106,\n",
       " 76,\n",
       " 61,\n",
       " 66,\n",
       " 105,\n",
       " 5,\n",
       " 78,\n",
       " 0,\n",
       " 67,\n",
       " 77,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 59,\n",
       " 70,\n",
       " 66,\n",
       " 59,\n",
       " 71,\n",
       " 60,\n",
       " 76,\n",
       " 59,\n",
       " 105,\n",
       " 79,\n",
       " 72,\n",
       " 62,\n",
       " 63,\n",
       " 76,\n",
       " 0,\n",
       " 81,\n",
       " 66,\n",
       " 73,\n",
       " 77,\n",
       " 63,\n",
       " 0,\n",
       " 78,\n",
       " 73,\n",
       " 81,\n",
       " 63,\n",
       " 76,\n",
       " 77,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 65,\n",
       " 83,\n",
       " 74,\n",
       " 77,\n",
       " 83,\n",
       " 0,\n",
       " 61,\n",
       " 59,\n",
       " 71,\n",
       " 74,\n",
       " 0,\n",
       " 81,\n",
       " 59,\n",
       " 77,\n",
       " 0,\n",
       " 74,\n",
       " 67,\n",
       " 78,\n",
       " 61,\n",
       " 66,\n",
       " 63,\n",
       " 62,\n",
       " 106,\n",
       " 72,\n",
       " 0,\n",
       " 83,\n",
       " 73,\n",
       " 79,\n",
       " 0,\n",
       " 76,\n",
       " 79,\n",
       " 60,\n",
       " 83,\n",
       " 105,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 74,\n",
       " 59,\n",
       " 67,\n",
       " 72,\n",
       " 105,\n",
       " 83,\n",
       " 73,\n",
       " 79,\n",
       " 0,\n",
       " 81,\n",
       " 59,\n",
       " 69,\n",
       " 63,\n",
       " 0,\n",
       " 78,\n",
       " 73,\n",
       " 0,\n",
       " 67,\n",
       " 77,\n",
       " 0,\n",
       " 72,\n",
       " 73,\n",
       " 78,\n",
       " 0,\n",
       " 83,\n",
       " 73,\n",
       " 79,\n",
       " 76,\n",
       " 77,\n",
       " 106,\n",
       " 66,\n",
       " 76,\n",
       " 67,\n",
       " 77,\n",
       " 78,\n",
       " 71,\n",
       " 59,\n",
       " 77,\n",
       " 0,\n",
       " 63,\n",
       " 80,\n",
       " 63,\n",
       " 28,\n",
       " 105,\n",
       " 60,\n",
       " 76,\n",
       " 59,\n",
       " 72,\n",
       " 62,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "#X = np.reshape(poemX, (n_patterns, seq_length, n_poems))\n",
    "# normalize\n",
    "X = np.array([np.array([char / float(n_chars) for char in seq]) for poem in poemX for seq in poem])\n",
    "X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "# one hot encode the output variable\n",
    "y = to_categorical([nextchar for poem in poemY for nextchar in poem])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-f6c53b0fe054>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#X = np.reshape(poemX, (n_patterns, seq_length, n_poems))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# normalize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpusX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m# one hot encode the output variable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpusY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "#X = np.reshape(poemX, (n_patterns, seq_length, n_poems))\n",
    "# normalize\n",
    "X = np.reshape(X, (len(corpusX), seq_length, 1))\n",
    "# one hot encode the output variable\n",
    "y = to_categorical(corpusY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1824318, 40, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256), activation='relu')\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights/letter/letter-weights-new-cont-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('weights/letter/letter-weights-new-25-3.0578.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/175\n",
      "1824318/1824318 [==============================] - 972s 533us/step - loss: 3.0578\n",
      "\n",
      "Epoch 00001: loss improved from inf to 3.05783, saving model to weights/letter/letter-weights-new-cont-01-3.0578.hdf5\n",
      "Epoch 2/175\n",
      "1824318/1824318 [==============================] - 977s 536us/step - loss: 3.0578\n",
      "\n",
      "Epoch 00002: loss improved from 3.05783 to 3.05782, saving model to weights/letter/letter-weights-new-cont-02-3.0578.hdf5\n",
      "Epoch 3/175\n",
      "1824318/1824318 [==============================] - 990s 542us/step - loss: 3.0578\n",
      "\n",
      "Epoch 00003: loss did not improve from 3.05782\n",
      "Epoch 4/175\n",
      "1824318/1824318 [==============================] - 1008s 552us/step - loss: 3.0578\n",
      "\n",
      "Epoch 00004: loss improved from 3.05782 to 3.05779, saving model to weights/letter/letter-weights-new-cont-04-3.0578.hdf5\n",
      "Epoch 5/175\n",
      "1824318/1824318 [==============================] - 1178s 646us/step - loss: 3.0578\n",
      "\n",
      "Epoch 00005: loss improved from 3.05779 to 3.05779, saving model to weights/letter/letter-weights-new-cont-05-3.0578.hdf5\n",
      "Epoch 6/175\n",
      "1824318/1824318 [==============================] - 987s 541us/step - loss: 3.0578\n",
      "\n",
      "Epoch 00006: loss improved from 3.05779 to 3.05778, saving model to weights/letter/letter-weights-new-cont-06-3.0578.hdf5\n",
      "Epoch 7/175\n",
      "1824318/1824318 [==============================] - 982s 538us/step - loss: 3.0578\n",
      "\n",
      "Epoch 00007: loss improved from 3.05778 to 3.05776, saving model to weights/letter/letter-weights-new-cont-07-3.0578.hdf5\n",
      "Epoch 8/175\n",
      "1824318/1824318 [==============================] - 961s 527us/step - loss: 3.0578\n",
      "\n",
      "Epoch 00008: loss improved from 3.05776 to 3.05776, saving model to weights/letter/letter-weights-new-cont-08-3.0578.hdf5\n",
      "Epoch 9/175\n",
      "1824318/1824318 [==============================] - 956s 524us/step - loss: 3.0577\n",
      "\n",
      "Epoch 00009: loss improved from 3.05776 to 3.05774, saving model to weights/letter/letter-weights-new-cont-09-3.0577.hdf5\n",
      "Epoch 10/175\n",
      "1824318/1824318 [==============================] - 972s 533us/step - loss: 3.0577\n",
      "\n",
      "Epoch 00010: loss did not improve from 3.05774\n",
      "Epoch 11/175\n",
      "1824318/1824318 [==============================] - 981s 538us/step - loss: 3.0577\n",
      "\n",
      "Epoch 00011: loss improved from 3.05774 to 3.05774, saving model to weights/letter/letter-weights-new-cont-11-3.0577.hdf5\n",
      "Epoch 12/175\n",
      "1824318/1824318 [==============================] - 971s 532us/step - loss: 3.0577\n",
      "\n",
      "Epoch 00012: loss did not improve from 3.05774\n",
      "Epoch 13/175\n",
      "1824318/1824318 [==============================] - 976s 535us/step - loss: 3.0578\n",
      "\n",
      "Epoch 00013: loss did not improve from 3.05774\n",
      "Epoch 14/175\n",
      "1824318/1824318 [==============================] - 980s 537us/step - loss: 3.0578\n",
      "\n",
      "Epoch 00014: loss did not improve from 3.05774\n",
      "Epoch 15/175\n",
      "1824318/1824318 [==============================] - 980s 537us/step - loss: 3.0578\n",
      "\n",
      "Epoch 00015: loss did not improve from 3.05774\n",
      "Epoch 16/175\n",
      "1824318/1824318 [==============================] - 974s 534us/step - loss: 3.0578\n",
      "\n",
      "Epoch 00016: loss did not improve from 3.05774\n",
      "Epoch 17/175\n",
      "1824318/1824318 [==============================] - 956s 524us/step - loss: 3.0578\n",
      "\n",
      "Epoch 00017: loss did not improve from 3.05774\n",
      "Epoch 18/175\n",
      "1824318/1824318 [==============================] - 960s 526us/step - loss: 3.0578\n",
      "\n",
      "Epoch 00018: loss did not improve from 3.05774\n",
      "Epoch 19/175\n",
      "  66432/1824318 [>.............................] - ETA: 15:28 - loss: 3.0520"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-263f1fc4fecc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# fit the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m175\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(X, y, epochs=175, batch_size=128, callbacks=callbacks_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
