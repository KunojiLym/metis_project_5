{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "RANDOM = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "haikus_train_df = pd.read_pickle('./data/haikus_train_df.pickle')\n",
    "haikus_test_df = pd.read_pickle('./data/haikus_test_df.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Input, LSTM, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoetryGenerator():\n",
    "    \n",
    "    def __init__(self, engine, tokenize=):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.engine = engine\n",
    "        self.tokenize = tokenize\n",
    "    \n",
    "    def load_corpus(self, train, valid, special_tokens):\n",
    "        \"\"\"\n",
    "        Assumes that special tokens have already been put into the dataset\n",
    "        \n",
    "        Parameters\n",
    "        ---\n",
    "        train:          training corpus\n",
    "        valid:          validation corpus\n",
    "        special_tokens: dictionary of the form token_type: token\n",
    "                        must include 'newline' and 'endpoem' tokens\n",
    "                        other possible tokens include 'newstanza'\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def transform(self, corpus, seq_len)\n",
    "    \n",
    "        poem_count = len(corpus)\n",
    "        self.pattern_count = 0\n",
    "        \n",
    "        # prepare the dataset of input to output pairs encoded as integers\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.poemX = []\n",
    "        self.poemY = []\n",
    "        self.pattern_count = 0\n",
    "\n",
    "        self.corpusX = []\n",
    "        self.corpusY = []\n",
    "        for poem_index in range(0, poem_count):\n",
    "\n",
    "            textX = []\n",
    "            textY = []\n",
    "            \n",
    "            poem = corpus[poem_index]\n",
    "            # add padding to poem\n",
    "            poem = list(np.full(seq_length - 1, '')) + list(poem)\n",
    "            \n",
    "            for i in range(0,  len(poem) - seq_len, 1):\n",
    "                seq_in = poem[i:i + seq_len]\n",
    "                seq_out = poem[i + seq_len]\n",
    "                textX.append([self.token_to_int[token] for token in seq_in])\n",
    "                textY.append(self.token_to_int[seq_out])\n",
    "\n",
    "            self.pattern_count = max(self.pattern_count, len(textX))\n",
    "\n",
    "            self.poemX.append(textX)\n",
    "            self.poemY.append(textY)\n",
    "\n",
    "            self.corpusX += textX\n",
    "            self.corpusY += textY\n",
    "    \n",
    "    def create_dict(self):\n",
    "        \n",
    "        # create corpus_raw\n",
    "        \n",
    "        self.tokens = sorted(set(list(corpus_raw)))\n",
    "        self.token_to_int = dict((t, i) for i, t in enumerate(self.tokens))\n",
    "        self.int_to_token = dict((i, t) for i, t in enumerate(self.tokens))\n",
    "        \n",
    "        self.token_count = len(corpus_raw)\n",
    "        self.vocab_count = len(tokens)\n",
    "\n",
    "    \n",
    "    def fit(self):\n",
    "                \n",
    "        self.fitted = True\n",
    "    \n",
    "    def generate(self, temperature=1.0):\n",
    "        \n",
    "        if not self.fitted:\n",
    "            raise ValueError('Model not fitted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoetryGenByWord(PoetryGenerator):\n",
    "    \n",
    "    def __init__(self, engine)\n",
    "    \n",
    "        super().__init__(engine, tokenize='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  1849446\n",
      "Total Vocab:  107\n"
     ]
    }
   ],
   "source": [
    "corpus_raw = ''.join(haikus_train_df['textchar_withtokens'])\n",
    "\n",
    "chars = sorted(set(list(corpus_raw)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "n_chars = len(corpus_raw)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_int[''] = n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25128"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_poems = len(haikus_train_df)\n",
    "\n",
    "n_poems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "i\n",
    "love\n",
    "you\n",
    "\n",
    "\n",
    "hello\n",
    "world\n",
    "\n",
    "---\n",
    "\n",
    "seq = 5\n",
    "\n",
    "[0 0 0 0 i] -> \\n\n",
    "[0 0 0 i \\n] -> love\n",
    "[0 0 i \\n love] -> \\n\n",
    "...\n",
    "[ ... \\n you] -> \\end\n",
    "[0 0 0 0 hello] -> \\n\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max patterns per poem:  801\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 150\n",
    "\n",
    "poemX = []\n",
    "poemY = []\n",
    "n_patterns = 0\n",
    "\n",
    "for poem_index in range(0, n_poems):\n",
    "\n",
    "    textX = []\n",
    "    textY = []\n",
    "    poem = haikus_train_df['textchar_withtokens'].iloc[poem_index]\n",
    "    # add padding to poem\n",
    "    poem = list(np.full(seq_length - 1, '')) + list(poem)\n",
    "    for i in range(0,  len(poem) - seq_length, 1):\n",
    "        seq_in = poem[i:i + seq_length]\n",
    "        seq_out = poem[i + seq_length]\n",
    "        textX.append([char_to_int[char] for char in seq_in])\n",
    "        textY.append(char_to_int[seq_out])\n",
    "\n",
    "    n_patterns = max(n_patterns, len(textX))\n",
    "    \n",
    "    poemX.append(textX)\n",
    "    poemY.append(textY)\n",
    "\n",
    "print(\"Max patterns per poem: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoem_charindex = char_to_int['◘']\n",
    "newline_charindex = char_to_int['↕']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "#X = np.reshape(poemX, (n_patterns, seq_length, n_poems))\n",
    "# normalize\n",
    "X = np.array([np.array([char / float(n_chars) for char in seq]) for poem in poemX for seq in poem])\n",
    "X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "# one hot encode the output variable\n",
    "y = to_categorical([nextchar for poem in poemY for nextchar in poem])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1824318, 150, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights/letter/letter-weights-corrected-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('weights/letter/letter-weights-new-25-3.0578.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1824318/1824318 [==============================] - 3567s 2ms/step - loss: 6792965275621.7305\n",
      "\n",
      "Epoch 00001: loss improved from inf to 6792965275621.73047, saving model to weights/letter/letter-weights-corrected-01-6792965275621.7305.hdf5\n",
      "Epoch 2/200\n",
      "1824318/1824318 [==============================] - 3541s 2ms/step - loss: 3.0595\n",
      "\n",
      "Epoch 00002: loss improved from 6792965275621.73047 to 3.05950, saving model to weights/letter/letter-weights-corrected-02-3.0595.hdf5\n",
      "Epoch 3/200\n",
      "1824318/1824318 [==============================] - 3564s 2ms/step - loss: 3.0590\n",
      "\n",
      "Epoch 00003: loss improved from 3.05950 to 3.05900, saving model to weights/letter/letter-weights-corrected-03-3.0590.hdf5\n",
      "Epoch 4/200\n",
      "1824318/1824318 [==============================] - 3546s 2ms/step - loss: 3.0588\n",
      "\n",
      "Epoch 00004: loss improved from 3.05900 to 3.05878, saving model to weights/letter/letter-weights-corrected-04-3.0588.hdf5\n",
      "Epoch 5/200\n",
      "1824318/1824318 [==============================] - 3558s 2ms/step - loss: 3.0587\n",
      "\n",
      "Epoch 00005: loss improved from 3.05878 to 3.05867, saving model to weights/letter/letter-weights-corrected-05-3.0587.hdf5\n",
      "Epoch 6/200\n",
      "1824318/1824318 [==============================] - 3535s 2ms/step - loss: 3.0585\n",
      "\n",
      "Epoch 00006: loss improved from 3.05867 to 3.05853, saving model to weights/letter/letter-weights-corrected-06-3.0585.hdf5\n",
      "Epoch 7/200\n",
      "1824318/1824318 [==============================] - 3554s 2ms/step - loss: 3.0584\n",
      "\n",
      "Epoch 00007: loss improved from 3.05853 to 3.05838, saving model to weights/letter/letter-weights-corrected-07-3.0584.hdf5\n",
      "Epoch 8/200\n",
      "1824318/1824318 [==============================] - 3570s 2ms/step - loss: 3.0583\n",
      "\n",
      "Epoch 00008: loss improved from 3.05838 to 3.05828, saving model to weights/letter/letter-weights-corrected-08-3.0583.hdf5\n",
      "Epoch 9/200\n",
      "1824318/1824318 [==============================] - 3639s 2ms/step - loss: 3.0582\n",
      "\n",
      "Epoch 00009: loss improved from 3.05828 to 3.05818, saving model to weights/letter/letter-weights-corrected-09-3.0582.hdf5\n",
      "Epoch 10/200\n",
      "1824318/1824318 [==============================] - 3646s 2ms/step - loss: 3.0582\n",
      "\n",
      "Epoch 00010: loss improved from 3.05818 to 3.05815, saving model to weights/letter/letter-weights-corrected-10-3.0582.hdf5\n",
      "Epoch 11/200\n",
      "1824318/1824318 [==============================] - 3652s 2ms/step - loss: 3.0581\n",
      "\n",
      "Epoch 00011: loss improved from 3.05815 to 3.05813, saving model to weights/letter/letter-weights-corrected-11-3.0581.hdf5\n",
      "Epoch 12/200\n",
      "1824318/1824318 [==============================] - 3644s 2ms/step - loss: 3.0581\n",
      "\n",
      "Epoch 00012: loss improved from 3.05813 to 3.05808, saving model to weights/letter/letter-weights-corrected-12-3.0581.hdf5\n",
      "Epoch 13/200\n",
      "1824318/1824318 [==============================] - 3648s 2ms/step - loss: 3.0580\n",
      "\n",
      "Epoch 00013: loss improved from 3.05808 to 3.05800, saving model to weights/letter/letter-weights-corrected-13-3.0580.hdf5\n",
      "Epoch 14/200\n",
      "1824318/1824318 [==============================] - 3650s 2ms/step - loss: 3.0580\n",
      "\n",
      "Epoch 00014: loss did not improve from 3.05800\n",
      "Epoch 15/200\n",
      "1824318/1824318 [==============================] - 3651s 2ms/step - loss: 3.0580\n",
      "\n",
      "Epoch 00015: loss improved from 3.05800 to 3.05795, saving model to weights/letter/letter-weights-corrected-15-3.0580.hdf5\n",
      "Epoch 16/200\n",
      "1824318/1824318 [==============================] - 3645s 2ms/step - loss: 3.0579\n",
      "\n",
      "Epoch 00016: loss improved from 3.05795 to 3.05795, saving model to weights/letter/letter-weights-corrected-16-3.0579.hdf5\n",
      "Epoch 17/200\n",
      "1824318/1824318 [==============================] - 3620s 2ms/step - loss: 3.0579\n",
      "\n",
      "Epoch 00017: loss improved from 3.05795 to 3.05790, saving model to weights/letter/letter-weights-corrected-17-3.0579.hdf5\n",
      "Epoch 18/200\n",
      "1824318/1824318 [==============================] - 3622s 2ms/step - loss: 3.0579\n",
      "\n",
      "Epoch 00018: loss did not improve from 3.05790\n",
      "Epoch 19/200\n",
      "1824318/1824318 [==============================] - 3615s 2ms/step - loss: 3.0579\n",
      "\n",
      "Epoch 00019: loss did not improve from 3.05790\n",
      "Epoch 20/200\n",
      "1824318/1824318 [==============================] - 3561s 2ms/step - loss: 3.0579\n",
      "\n",
      "Epoch 00020: loss improved from 3.05790 to 3.05788, saving model to weights/letter/letter-weights-corrected-20-3.0579.hdf5\n",
      "Epoch 21/200\n",
      "1824318/1824318 [==============================] - 3607s 2ms/step - loss: 3.0579\n",
      "\n",
      "Epoch 00021: loss improved from 3.05788 to 3.05785, saving model to weights/letter/letter-weights-corrected-21-3.0579.hdf5\n",
      "Epoch 22/200\n",
      "1824318/1824318 [==============================] - 3603s 2ms/step - loss: 3.0578\n",
      "\n",
      "Epoch 00022: loss improved from 3.05785 to 3.05784, saving model to weights/letter/letter-weights-corrected-22-3.0578.hdf5\n",
      "Epoch 23/200\n",
      "1824318/1824318 [==============================] - 3603s 2ms/step - loss: 3.0578\n",
      "\n",
      "Epoch 00023: loss improved from 3.05784 to 3.05783, saving model to weights/letter/letter-weights-corrected-23-3.0578.hdf5\n",
      "Epoch 24/200\n",
      "1824318/1824318 [==============================] - 3607s 2ms/step - loss: 3.0578\n",
      "\n",
      "Epoch 00024: loss improved from 3.05783 to 3.05780, saving model to weights/letter/letter-weights-corrected-24-3.0578.hdf5\n",
      "Epoch 25/200\n",
      "1824318/1824318 [==============================] - 3595s 2ms/step - loss: 3.0578\n",
      "\n",
      "Epoch 00025: loss improved from 3.05780 to 3.05780, saving model to weights/letter/letter-weights-corrected-25-3.0578.hdf5\n",
      "Epoch 26/200\n",
      "1824318/1824318 [==============================] - 3617s 2ms/step - loss: 3.0578\n",
      "\n",
      "Epoch 00026: loss improved from 3.05780 to 3.05778, saving model to weights/letter/letter-weights-corrected-26-3.0578.hdf5\n",
      "Epoch 27/200\n",
      "1824318/1824318 [==============================] - 3629s 2ms/step - loss: 3.0578\n",
      "\n",
      "Epoch 00027: loss improved from 3.05778 to 3.05777, saving model to weights/letter/letter-weights-corrected-27-3.0578.hdf5\n",
      "Epoch 28/200\n",
      "1824318/1824318 [==============================] - 3631s 2ms/step - loss: 3.0578\n",
      "\n",
      "Epoch 00028: loss did not improve from 3.05777\n",
      "Epoch 29/200\n",
      "1824318/1824318 [==============================] - 3624s 2ms/step - loss: 3.0578\n",
      "\n",
      "Epoch 00029: loss improved from 3.05777 to 3.05776, saving model to weights/letter/letter-weights-corrected-29-3.0578.hdf5\n",
      "Epoch 30/200\n",
      "1824318/1824318 [==============================] - 3645s 2ms/step - loss: 3.0578\n",
      "\n",
      "Epoch 00030: loss improved from 3.05776 to 3.05775, saving model to weights/letter/letter-weights-corrected-30-3.0578.hdf5\n",
      "Epoch 31/200\n",
      "1824318/1824318 [==============================] - 3620s 2ms/step - loss: 3.0578\n",
      "\n",
      "Epoch 00031: loss did not improve from 3.05775\n",
      "Epoch 32/200\n",
      "1824318/1824318 [==============================] - 3630s 2ms/step - loss: 3.0577\n",
      "\n",
      "Epoch 00032: loss improved from 3.05775 to 3.05775, saving model to weights/letter/letter-weights-corrected-32-3.0577.hdf5\n",
      "Epoch 33/200\n",
      "1824318/1824318 [==============================] - 3638s 2ms/step - loss: 3.0577\n",
      "\n",
      "Epoch 00033: loss improved from 3.05775 to 3.05774, saving model to weights/letter/letter-weights-corrected-33-3.0577.hdf5\n",
      "Epoch 34/200\n",
      "1824318/1824318 [==============================] - 3628s 2ms/step - loss: 3.0577\n",
      "\n",
      "Epoch 00034: loss improved from 3.05774 to 3.05774, saving model to weights/letter/letter-weights-corrected-34-3.0577.hdf5\n",
      "Epoch 35/200\n",
      "1824318/1824318 [==============================] - 3640s 2ms/step - loss: 3.0577\n",
      "\n",
      "Epoch 00035: loss improved from 3.05774 to 3.05773, saving model to weights/letter/letter-weights-corrected-35-3.0577.hdf5\n",
      "Epoch 36/200\n",
      "1824318/1824318 [==============================] - 3632s 2ms/step - loss: 3.0577\n",
      "\n",
      "Epoch 00036: loss did not improve from 3.05773\n",
      "Epoch 37/200\n",
      "1824318/1824318 [==============================] - 3635s 2ms/step - loss: 3.0577\n",
      "\n",
      "Epoch 00037: loss did not improve from 3.05773\n",
      "Epoch 38/200\n",
      "1824318/1824318 [==============================] - 3619s 2ms/step - loss: 3.0578\n",
      "\n",
      "Epoch 00038: loss did not improve from 3.05773\n",
      "Epoch 39/200\n",
      "1824318/1824318 [==============================] - 3595s 2ms/step - loss: 3.0577\n",
      "\n",
      "Epoch 00039: loss did not improve from 3.05773\n",
      "Epoch 40/200\n",
      "1824318/1824318 [==============================] - 3623s 2ms/step - loss: 3.0578\n",
      "\n",
      "Epoch 00040: loss did not improve from 3.05773\n",
      "Epoch 41/200\n",
      "1824318/1824318 [==============================] - 3659s 2ms/step - loss: 3.0578\n",
      "\n",
      "Epoch 00041: loss did not improve from 3.05773\n",
      "Epoch 42/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1824318/1824318 [==============================] - 3621s 2ms/step - loss: 3.0578\n",
      "\n",
      "Epoch 00042: loss did not improve from 3.05773\n",
      "Epoch 43/200\n",
      "1824318/1824318 [==============================] - 3622s 2ms/step - loss: 3.0578\n",
      "\n",
      "Epoch 00043: loss did not improve from 3.05773\n",
      "Epoch 44/200\n",
      "1824318/1824318 [==============================] - 3614s 2ms/step - loss: 3.0578\n",
      "\n",
      "Epoch 00044: loss did not improve from 3.05773\n",
      "Epoch 45/200\n",
      "1824318/1824318 [==============================] - 3524s 2ms/step - loss: 3.0578\n",
      "\n",
      "Epoch 00045: loss did not improve from 3.05773\n",
      "Epoch 46/200\n",
      "1824318/1824318 [==============================] - 3538s 2ms/step - loss: 3.0578\n",
      "\n",
      "Epoch 00046: loss did not improve from 3.05773\n",
      "Epoch 47/200\n",
      " 690176/1824318 [==========>...................] - ETA: 36:58 - loss: 3.0567"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-c27a99e6d079>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# fit the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "history = model.fit(X, y, epochs=200, batch_size=128, callbacks=callbacks_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
