{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "RANDOM = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "haikus_train_df = pd.read_pickle('./data/haikus_train_df.pickle')\n",
    "haikus_test_df = pd.read_pickle('./data/haikus_test_df.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Input, LSTM, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  1849446\n",
      "Total Vocab:  107\n"
     ]
    }
   ],
   "source": [
    "corpus_raw = ''.join(haikus_train_df['textchar_withtokens'])\n",
    "\n",
    "chars = sorted(set(list(corpus_raw)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "n_chars = len(corpus_raw)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " '!',\n",
       " '\"',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '[',\n",
       " ']',\n",
       " '_',\n",
       " '`',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '{',\n",
       " '~',\n",
       " '\\x85',\n",
       " '\\x92',\n",
       " '\\x96',\n",
       " '\\x97',\n",
       " '\\xa0',\n",
       " 'à',\n",
       " 'ä',\n",
       " 'é',\n",
       " 'ü',\n",
       " 'ē',\n",
       " 'ū',\n",
       " 'ŭ',\n",
       " '\\u200b',\n",
       " '‘',\n",
       " '’',\n",
       " '“',\n",
       " '”',\n",
       " '…',\n",
       " '↕',\n",
       " '◘']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25128"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_poems = len(haikus_train_df)\n",
    "\n",
    "n_poems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max patterns per poem:  797\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 5\n",
    "\n",
    "poemX = []\n",
    "poemY = []\n",
    "n_patterns = 0\n",
    "\n",
    "corpusX = []\n",
    "corpusY = []\n",
    "for poem_index in range(0, n_poems):\n",
    "\n",
    "    textX = []\n",
    "    textY = []\n",
    "    poem = haikus_train_df['textchar_withtokens'].iloc[poem_index]\n",
    "    for i in range(0,  len(poem) - seq_length, 1):\n",
    "        seq_in = poem[i:i + seq_length]\n",
    "        seq_out = poem[i + seq_length]\n",
    "        textX.append([char_to_int[char] for char in seq_in])\n",
    "        textY.append(char_to_int[seq_out])\n",
    "    n_patterns = max(n_patterns, len(textX))\n",
    "    \n",
    "    poemX.append(textX)\n",
    "    poemY.append(textY)\n",
    "    \n",
    "    corpusX += textX\n",
    "    corpusY += textY\n",
    "\n",
    "print(\"Max patterns per poem: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoem_charindex = char_to_int['◘']\n",
    "newline_charindex = char_to_int['↕']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>59</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>59</td>\n",
       "      <td>77</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73</td>\n",
       "      <td>59</td>\n",
       "      <td>77</td>\n",
       "      <td>67</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59</td>\n",
       "      <td>77</td>\n",
       "      <td>67</td>\n",
       "      <td>77</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723801</th>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>66</td>\n",
       "      <td>67</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723802</th>\n",
       "      <td>77</td>\n",
       "      <td>66</td>\n",
       "      <td>67</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723803</th>\n",
       "      <td>66</td>\n",
       "      <td>67</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723804</th>\n",
       "      <td>67</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>63</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723805</th>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>63</td>\n",
       "      <td>76</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1723806 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0   1   2   3    4\n",
       "0        59  72   0  73   59\n",
       "1        72   0  73  59   77\n",
       "2         0  73  59  77   67\n",
       "3        73  59  77  67   77\n",
       "4        59  77  67  77  105\n",
       "...      ..  ..  ..  ..  ...\n",
       "1723801   0  77  66  67   71\n",
       "1723802  77  66  67  71   71\n",
       "1723803  66  67  71  71   63\n",
       "1723804  67  71  71  63   76\n",
       "1723805  71  71  63  76   77\n",
       "\n",
       "[1723806 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(corpusX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[77,\n",
       " 67,\n",
       " 77,\n",
       " 105,\n",
       " 67,\n",
       " 72,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 30,\n",
       " 67,\n",
       " 60,\n",
       " 70,\n",
       " 63,\n",
       " 0,\n",
       " 30,\n",
       " 63,\n",
       " 70,\n",
       " 78,\n",
       " 0,\n",
       " 10,\n",
       " 10,\n",
       " 105,\n",
       " 59,\n",
       " 62,\n",
       " 79,\n",
       " 70,\n",
       " 78,\n",
       " 0,\n",
       " 60,\n",
       " 73,\n",
       " 73,\n",
       " 69,\n",
       " 0,\n",
       " 77,\n",
       " 78,\n",
       " 73,\n",
       " 76,\n",
       " 63,\n",
       " 106,\n",
       " 78,\n",
       " 78,\n",
       " 67,\n",
       " 0,\n",
       " 77,\n",
       " 73,\n",
       " 72,\n",
       " 72,\n",
       " 63,\n",
       " 78,\n",
       " 0,\n",
       " 82,\n",
       " 82,\n",
       " 80,\n",
       " 67,\n",
       " 105,\n",
       " 63,\n",
       " 105,\n",
       " 77,\n",
       " 74,\n",
       " 63,\n",
       " 72,\n",
       " 77,\n",
       " 63,\n",
       " 76,\n",
       " 106,\n",
       " 0,\n",
       " 81,\n",
       " 66,\n",
       " 63,\n",
       " 72,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 83,\n",
       " 0,\n",
       " 61,\n",
       " 59,\n",
       " 71,\n",
       " 63,\n",
       " 0,\n",
       " 71,\n",
       " 67,\n",
       " 72,\n",
       " 62,\n",
       " 0,\n",
       " 77,\n",
       " 79,\n",
       " 64,\n",
       " 64,\n",
       " 63,\n",
       " 76,\n",
       " 63,\n",
       " 62,\n",
       " 0,\n",
       " 77,\n",
       " 66,\n",
       " 59,\n",
       " 71,\n",
       " 63,\n",
       " 105,\n",
       " 58,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 77,\n",
       " 63,\n",
       " 0,\n",
       " 60,\n",
       " 63,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 77,\n",
       " 59,\n",
       " 71,\n",
       " 63,\n",
       " 0,\n",
       " 59,\n",
       " 72,\n",
       " 62,\n",
       " 0,\n",
       " 72,\n",
       " 73,\n",
       " 78,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 77,\n",
       " 59,\n",
       " 71,\n",
       " 63,\n",
       " 105,\n",
       " 59,\n",
       " 10,\n",
       " 81,\n",
       " 73,\n",
       " 72,\n",
       " 62,\n",
       " 63,\n",
       " 76,\n",
       " 67,\n",
       " 72,\n",
       " 65,\n",
       " 0,\n",
       " 81,\n",
       " 66,\n",
       " 67,\n",
       " 77,\n",
       " 74,\n",
       " 63,\n",
       " 76,\n",
       " 63,\n",
       " 62,\n",
       " 0,\n",
       " 71,\n",
       " 67,\n",
       " 72,\n",
       " 62,\n",
       " 106,\n",
       " 61,\n",
       " 73,\n",
       " 72,\n",
       " 80,\n",
       " 63,\n",
       " 76,\n",
       " 77,\n",
       " 59,\n",
       " 78,\n",
       " 67,\n",
       " 73,\n",
       " 72,\n",
       " 105,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 62,\n",
       " 59,\n",
       " 64,\n",
       " 64,\n",
       " 73,\n",
       " 62,\n",
       " 67,\n",
       " 70,\n",
       " 77,\n",
       " 0,\n",
       " 72,\n",
       " 73,\n",
       " 62,\n",
       " 62,\n",
       " 67,\n",
       " 72,\n",
       " 65,\n",
       " 105,\n",
       " 37,\n",
       " 72,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 60,\n",
       " 76,\n",
       " 63,\n",
       " 63,\n",
       " 84,\n",
       " 63,\n",
       " 106,\n",
       " 0,\n",
       " 64,\n",
       " 73,\n",
       " 65,\n",
       " 105,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 77,\n",
       " 70,\n",
       " 73,\n",
       " 81,\n",
       " 0,\n",
       " 61,\n",
       " 76,\n",
       " 63,\n",
       " 59,\n",
       " 69,\n",
       " 67,\n",
       " 72,\n",
       " 65,\n",
       " 105,\n",
       " 73,\n",
       " 64,\n",
       " 0,\n",
       " 59,\n",
       " 72,\n",
       " 0,\n",
       " 59,\n",
       " 72,\n",
       " 61,\n",
       " 66,\n",
       " 73,\n",
       " 76,\n",
       " 0,\n",
       " 70,\n",
       " 67,\n",
       " 72,\n",
       " 63,\n",
       " 106,\n",
       " 79,\n",
       " 65,\n",
       " 66,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 76,\n",
       " 63,\n",
       " 0,\n",
       " 81,\n",
       " 59,\n",
       " 77,\n",
       " 0,\n",
       " 72,\n",
       " 73,\n",
       " 0,\n",
       " 77,\n",
       " 73,\n",
       " 79,\n",
       " 72,\n",
       " 62,\n",
       " 0,\n",
       " 67,\n",
       " 72,\n",
       " 0,\n",
       " 59,\n",
       " 70,\n",
       " 70,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 66,\n",
       " 73,\n",
       " 79,\n",
       " 77,\n",
       " 63,\n",
       " 105,\n",
       " 67,\n",
       " 0,\n",
       " 61,\n",
       " 73,\n",
       " 79,\n",
       " 70,\n",
       " 62,\n",
       " 0,\n",
       " 72,\n",
       " 73,\n",
       " 78,\n",
       " 0,\n",
       " 64,\n",
       " 73,\n",
       " 76,\n",
       " 60,\n",
       " 63,\n",
       " 59,\n",
       " 76,\n",
       " 0,\n",
       " 70,\n",
       " 67,\n",
       " 77,\n",
       " 78,\n",
       " 63,\n",
       " 72,\n",
       " 67,\n",
       " 72,\n",
       " 65,\n",
       " 0,\n",
       " 64,\n",
       " 73,\n",
       " 76,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 61,\n",
       " 76,\n",
       " 83,\n",
       " 0,\n",
       " 73,\n",
       " 64,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 73,\n",
       " 77,\n",
       " 63,\n",
       " 0,\n",
       " 70,\n",
       " 73,\n",
       " 72,\n",
       " 65,\n",
       " 0,\n",
       " 81,\n",
       " 66,\n",
       " 67,\n",
       " 78,\n",
       " 63,\n",
       " 0,\n",
       " 76,\n",
       " 67,\n",
       " 74,\n",
       " 74,\n",
       " 70,\n",
       " 67,\n",
       " 72,\n",
       " 65,\n",
       " 0,\n",
       " 81,\n",
       " 59,\n",
       " 80,\n",
       " 63,\n",
       " 77,\n",
       " 105,\n",
       " 62,\n",
       " 76,\n",
       " 59,\n",
       " 65,\n",
       " 65,\n",
       " 67,\n",
       " 72,\n",
       " 65,\n",
       " 0,\n",
       " 79,\n",
       " 74,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 67,\n",
       " 76,\n",
       " 0,\n",
       " 77,\n",
       " 78,\n",
       " 76,\n",
       " 63,\n",
       " 72,\n",
       " 65,\n",
       " 78,\n",
       " 66,\n",
       " 0,\n",
       " 78,\n",
       " 73,\n",
       " 0,\n",
       " 60,\n",
       " 76,\n",
       " 63,\n",
       " 59,\n",
       " 69,\n",
       " 0,\n",
       " 73,\n",
       " 72,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 77,\n",
       " 79,\n",
       " 70,\n",
       " 70,\n",
       " 63,\n",
       " 72,\n",
       " 0,\n",
       " 60,\n",
       " 63,\n",
       " 59,\n",
       " 61,\n",
       " 66,\n",
       " 0,\n",
       " 73,\n",
       " 64,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 77,\n",
       " 69,\n",
       " 83,\n",
       " 106,\n",
       " 59,\n",
       " 72,\n",
       " 105,\n",
       " 81,\n",
       " 63,\n",
       " 0,\n",
       " 64,\n",
       " 67,\n",
       " 72,\n",
       " 62,\n",
       " 28,\n",
       " 106,\n",
       " 0,\n",
       " 77,\n",
       " 78,\n",
       " 67,\n",
       " 70,\n",
       " 70,\n",
       " 23,\n",
       " 0,\n",
       " 72,\n",
       " 73,\n",
       " 71,\n",
       " 105,\n",
       " 77,\n",
       " 65,\n",
       " 105,\n",
       " 65,\n",
       " 67,\n",
       " 64,\n",
       " 0,\n",
       " 66,\n",
       " 67,\n",
       " 71,\n",
       " 106,\n",
       " 63,\n",
       " 80,\n",
       " 63,\n",
       " 76,\n",
       " 83,\n",
       " 73,\n",
       " 72,\n",
       " 63,\n",
       " 0,\n",
       " 65,\n",
       " 73,\n",
       " 78,\n",
       " 0,\n",
       " 67,\n",
       " 78,\n",
       " 0,\n",
       " 73,\n",
       " 64,\n",
       " 64,\n",
       " 0,\n",
       " 60,\n",
       " 83,\n",
       " 0,\n",
       " 66,\n",
       " 63,\n",
       " 59,\n",
       " 76,\n",
       " 78,\n",
       " 105,\n",
       " 63,\n",
       " 80,\n",
       " 63,\n",
       " 76,\n",
       " 83,\n",
       " 0,\n",
       " 77,\n",
       " 78,\n",
       " 59,\n",
       " 76,\n",
       " 78,\n",
       " 0,\n",
       " 81,\n",
       " 59,\n",
       " 77,\n",
       " 0,\n",
       " 59,\n",
       " 0,\n",
       " 60,\n",
       " 59,\n",
       " 62,\n",
       " 0,\n",
       " 77,\n",
       " 78,\n",
       " 59,\n",
       " 76,\n",
       " 78,\n",
       " 105,\n",
       " 77,\n",
       " 67,\n",
       " 72,\n",
       " 61,\n",
       " 63,\n",
       " 0,\n",
       " 59,\n",
       " 70,\n",
       " 70,\n",
       " 0,\n",
       " 61,\n",
       " 73,\n",
       " 72,\n",
       " 61,\n",
       " 70,\n",
       " 79,\n",
       " 77,\n",
       " 67,\n",
       " 73,\n",
       " 72,\n",
       " 77,\n",
       " 0,\n",
       " 81,\n",
       " 63,\n",
       " 76,\n",
       " 63,\n",
       " 0,\n",
       " 64,\n",
       " 73,\n",
       " 76,\n",
       " 63,\n",
       " 65,\n",
       " 73,\n",
       " 72,\n",
       " 63,\n",
       " 106,\n",
       " 72,\n",
       " 73,\n",
       " 73,\n",
       " 72,\n",
       " 0,\n",
       " 63,\n",
       " 77,\n",
       " 61,\n",
       " 59,\n",
       " 74,\n",
       " 63,\n",
       " 105,\n",
       " 81,\n",
       " 67,\n",
       " 78,\n",
       " 66,\n",
       " 67,\n",
       " 72,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 74,\n",
       " 76,\n",
       " 67,\n",
       " 77,\n",
       " 73,\n",
       " 72,\n",
       " 0,\n",
       " 81,\n",
       " 59,\n",
       " 70,\n",
       " 70,\n",
       " 77,\n",
       " 105,\n",
       " 59,\n",
       " 0,\n",
       " 70,\n",
       " 67,\n",
       " 60,\n",
       " 76,\n",
       " 59,\n",
       " 76,\n",
       " 83,\n",
       " 106,\n",
       " 66,\n",
       " 59,\n",
       " 78,\n",
       " 105,\n",
       " 67,\n",
       " 77,\n",
       " 0,\n",
       " 72,\n",
       " 73,\n",
       " 78,\n",
       " 0,\n",
       " 81,\n",
       " 66,\n",
       " 59,\n",
       " 78,\n",
       " 0,\n",
       " 73,\n",
       " 61,\n",
       " 61,\n",
       " 79,\n",
       " 74,\n",
       " 67,\n",
       " 63,\n",
       " 77,\n",
       " 0,\n",
       " 71,\n",
       " 83,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 73,\n",
       " 79,\n",
       " 65,\n",
       " 66,\n",
       " 78,\n",
       " 77,\n",
       " 0,\n",
       " 59,\n",
       " 78,\n",
       " 0,\n",
       " 74,\n",
       " 76,\n",
       " 63,\n",
       " 77,\n",
       " 63,\n",
       " 72,\n",
       " 78,\n",
       " 105,\n",
       " 72,\n",
       " 73,\n",
       " 76,\n",
       " 0,\n",
       " 81,\n",
       " 66,\n",
       " 83,\n",
       " 0,\n",
       " 67,\n",
       " 0,\n",
       " 77,\n",
       " 63,\n",
       " 72,\n",
       " 78,\n",
       " 0,\n",
       " 64,\n",
       " 73,\n",
       " 76,\n",
       " 0,\n",
       " 83,\n",
       " 73,\n",
       " 79,\n",
       " 0,\n",
       " 71,\n",
       " 63,\n",
       " 77,\n",
       " 77,\n",
       " 63,\n",
       " 76,\n",
       " 0,\n",
       " 71,\n",
       " 67,\n",
       " 61,\n",
       " 66,\n",
       " 63,\n",
       " 70,\n",
       " 63,\n",
       " 106,\n",
       " 72,\n",
       " 65,\n",
       " 0,\n",
       " 59,\n",
       " 0,\n",
       " 81,\n",
       " 66,\n",
       " 67,\n",
       " 78,\n",
       " 63,\n",
       " 0,\n",
       " 71,\n",
       " 59,\n",
       " 77,\n",
       " 69,\n",
       " 105,\n",
       " 60,\n",
       " 63,\n",
       " 61,\n",
       " 59,\n",
       " 79,\n",
       " 77,\n",
       " 63,\n",
       " 0,\n",
       " 73,\n",
       " 64,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 74,\n",
       " 73,\n",
       " 70,\n",
       " 70,\n",
       " 79,\n",
       " 78,\n",
       " 67,\n",
       " 73,\n",
       " 72,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 105,\n",
       " 59,\n",
       " 0,\n",
       " 62,\n",
       " 59,\n",
       " 83,\n",
       " 0,\n",
       " 71,\n",
       " 73,\n",
       " 73,\n",
       " 72,\n",
       " 106,\n",
       " 0,\n",
       " 73,\n",
       " 64,\n",
       " 0,\n",
       " 59,\n",
       " 74,\n",
       " 74,\n",
       " 70,\n",
       " 63,\n",
       " 77,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 105,\n",
       " 64,\n",
       " 63,\n",
       " 63,\n",
       " 70,\n",
       " 0,\n",
       " 73,\n",
       " 64,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 74,\n",
       " 59,\n",
       " 76,\n",
       " 67,\n",
       " 72,\n",
       " 65,\n",
       " 0,\n",
       " 60,\n",
       " 70,\n",
       " 59,\n",
       " 62,\n",
       " 63,\n",
       " 105,\n",
       " 60,\n",
       " 63,\n",
       " 78,\n",
       " 81,\n",
       " 63,\n",
       " 63,\n",
       " 72,\n",
       " 0,\n",
       " 71,\n",
       " 83,\n",
       " 0,\n",
       " 64,\n",
       " 67,\n",
       " 72,\n",
       " 65,\n",
       " 63,\n",
       " 76,\n",
       " 77,\n",
       " 106,\n",
       " 5,\n",
       " 78,\n",
       " 0,\n",
       " 67,\n",
       " 77,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 59,\n",
       " 70,\n",
       " 66,\n",
       " 59,\n",
       " 71,\n",
       " 60,\n",
       " 76,\n",
       " 59,\n",
       " 105,\n",
       " 79,\n",
       " 72,\n",
       " 62,\n",
       " 63,\n",
       " 76,\n",
       " 0,\n",
       " 81,\n",
       " 66,\n",
       " 73,\n",
       " 77,\n",
       " 63,\n",
       " 0,\n",
       " 78,\n",
       " 73,\n",
       " 81,\n",
       " 63,\n",
       " 76,\n",
       " 77,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 65,\n",
       " 83,\n",
       " 74,\n",
       " 77,\n",
       " 83,\n",
       " 0,\n",
       " 61,\n",
       " 59,\n",
       " 71,\n",
       " 74,\n",
       " 0,\n",
       " 81,\n",
       " 59,\n",
       " 77,\n",
       " 0,\n",
       " 74,\n",
       " 67,\n",
       " 78,\n",
       " 61,\n",
       " 66,\n",
       " 63,\n",
       " 62,\n",
       " 106,\n",
       " 79,\n",
       " 0,\n",
       " 76,\n",
       " 79,\n",
       " 60,\n",
       " 83,\n",
       " 105,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 74,\n",
       " 59,\n",
       " 67,\n",
       " 72,\n",
       " 105,\n",
       " 83,\n",
       " 73,\n",
       " 79,\n",
       " 0,\n",
       " 81,\n",
       " 59,\n",
       " 69,\n",
       " 63,\n",
       " 0,\n",
       " 78,\n",
       " 73,\n",
       " 0,\n",
       " 67,\n",
       " 77,\n",
       " 0,\n",
       " 72,\n",
       " 73,\n",
       " 78,\n",
       " 0,\n",
       " 83,\n",
       " 73,\n",
       " 79,\n",
       " 76,\n",
       " 77,\n",
       " 106,\n",
       " 78,\n",
       " 71,\n",
       " 59,\n",
       " 77,\n",
       " 0,\n",
       " 63,\n",
       " 80,\n",
       " 63,\n",
       " 28,\n",
       " 105,\n",
       " 60,\n",
       " 76,\n",
       " 59,\n",
       " 72,\n",
       " 62,\n",
       " 0,\n",
       " 72,\n",
       " 63,\n",
       " 81,\n",
       " 0,\n",
       " 77,\n",
       " 63,\n",
       " 63,\n",
       " 62,\n",
       " 105,\n",
       " 67,\n",
       " 72,\n",
       " 0,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 64,\n",
       " 63,\n",
       " 63,\n",
       " 62,\n",
       " 63,\n",
       " 76,\n",
       " 106,\n",
       " 63,\n",
       " 60,\n",
       " 76,\n",
       " 79,\n",
       " 59,\n",
       " 76,\n",
       " 83,\n",
       " 105,\n",
       " 78,\n",
       " 66,\n",
       " 63,\n",
       " 0,\n",
       " 64,\n",
       " 67,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 0,\n",
       " 65,\n",
       " 59,\n",
       " 76,\n",
       " 62,\n",
       " 63,\n",
       " 72,\n",
       " 0,\n",
       " 77,\n",
       " 63,\n",
       " 63,\n",
       " 62,\n",
       " 77,\n",
       " 0,\n",
       " 62,\n",
       " 67,\n",
       " 77,\n",
       " 74,\n",
       " 70,\n",
       " 59,\n",
       " 83,\n",
       " 105,\n",
       " 59,\n",
       " 78,\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "#X = np.reshape(poemX, (n_patterns, seq_length, n_poems))\n",
    "# normalize\n",
    "X = np.array([np.array([char / float(n_chars) for char in seq]) for poem in poemX for seq in poem])\n",
    "X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "# one hot encode the output variable\n",
    "y = to_categorical([nextchar for poem in poemY for nextchar in poem])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "#X = np.reshape(poemX, (n_patterns, seq_length, n_poems))\n",
    "# normalize\n",
    "X = np.reshape(X, (len(corpusX), seq_length, 1))\n",
    "# one hot encode the output variable\n",
    "y = to_categorical(corpusY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1723806, 5, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"letter-weights-cont-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\vi_ci\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = load_model('letter-weights-132-2.4074.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/68\n",
      "1723806/1723806 [==============================] - 184s 107us/step - loss: 2.4063\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.40626, saving model to letter-weights-cont-01-2.4063.hdf5\n",
      "Epoch 2/68\n",
      "1723806/1723806 [==============================] - 186s 108us/step - loss: 2.4052\n",
      "\n",
      "Epoch 00002: loss improved from 2.40626 to 2.40516, saving model to letter-weights-cont-02-2.4052.hdf5\n",
      "Epoch 3/68\n",
      "1723806/1723806 [==============================] - 182s 106us/step - loss: 2.4025\n",
      "\n",
      "Epoch 00003: loss improved from 2.40516 to 2.40253, saving model to letter-weights-cont-03-2.4025.hdf5\n",
      "Epoch 4/68\n",
      "1723806/1723806 [==============================] - 182s 106us/step - loss: 2.4011\n",
      "\n",
      "Epoch 00004: loss improved from 2.40253 to 2.40108, saving model to letter-weights-cont-04-2.4011.hdf5\n",
      "Epoch 5/68\n",
      "1723806/1723806 [==============================] - 182s 106us/step - loss: 2.3988\n",
      "\n",
      "Epoch 00005: loss improved from 2.40108 to 2.39877, saving model to letter-weights-cont-05-2.3988.hdf5\n",
      "Epoch 6/68\n",
      "1723806/1723806 [==============================] - 182s 106us/step - loss: 2.3974\n",
      "\n",
      "Epoch 00006: loss improved from 2.39877 to 2.39739, saving model to letter-weights-cont-06-2.3974.hdf5\n",
      "Epoch 7/68\n",
      "1723806/1723806 [==============================] - 182s 106us/step - loss: 2.3952\n",
      "\n",
      "Epoch 00007: loss improved from 2.39739 to 2.39518, saving model to letter-weights-cont-07-2.3952.hdf5\n",
      "Epoch 8/68\n",
      "1723806/1723806 [==============================] - 182s 106us/step - loss: 2.3944\n",
      "\n",
      "Epoch 00008: loss improved from 2.39518 to 2.39436, saving model to letter-weights-cont-08-2.3944.hdf5\n",
      "Epoch 9/68\n",
      "1723806/1723806 [==============================] - 182s 106us/step - loss: 2.3945\n",
      "\n",
      "Epoch 00009: loss did not improve from 2.39436\n",
      "Epoch 10/68\n",
      "1723806/1723806 [==============================] - 182s 106us/step - loss: 2.3936\n",
      "\n",
      "Epoch 00010: loss improved from 2.39436 to 2.39363, saving model to letter-weights-cont-10-2.3936.hdf5\n",
      "Epoch 11/68\n",
      "1723806/1723806 [==============================] - 182s 106us/step - loss: 2.3901\n",
      "\n",
      "Epoch 00011: loss improved from 2.39363 to 2.39005, saving model to letter-weights-cont-11-2.3901.hdf5\n",
      "Epoch 12/68\n",
      "1723806/1723806 [==============================] - 182s 106us/step - loss: 2.3880\n",
      "\n",
      "Epoch 00012: loss improved from 2.39005 to 2.38798, saving model to letter-weights-cont-12-2.3880.hdf5\n",
      "Epoch 13/68\n",
      "1723806/1723806 [==============================] - 183s 106us/step - loss: 2.3875\n",
      "\n",
      "Epoch 00013: loss improved from 2.38798 to 2.38748, saving model to letter-weights-cont-13-2.3875.hdf5\n",
      "Epoch 14/68\n",
      "1723806/1723806 [==============================] - 183s 106us/step - loss: 2.3870\n",
      "\n",
      "Epoch 00014: loss improved from 2.38748 to 2.38698, saving model to letter-weights-cont-14-2.3870.hdf5\n",
      "Epoch 15/68\n",
      "1723806/1723806 [==============================] - 182s 106us/step - loss: 2.3849\n",
      "\n",
      "Epoch 00015: loss improved from 2.38698 to 2.38494, saving model to letter-weights-cont-15-2.3849.hdf5\n",
      "Epoch 16/68\n",
      "1723806/1723806 [==============================] - 183s 106us/step - loss: 2.3830\n",
      "\n",
      "Epoch 00016: loss improved from 2.38494 to 2.38298, saving model to letter-weights-cont-16-2.3830.hdf5\n",
      "Epoch 17/68\n",
      "1723806/1723806 [==============================] - 183s 106us/step - loss: 2.3798\n",
      "\n",
      "Epoch 00017: loss improved from 2.38298 to 2.37983, saving model to letter-weights-cont-17-2.3798.hdf5\n",
      "Epoch 18/68\n",
      "1723806/1723806 [==============================] - 188s 109us/step - loss: 2.3792\n",
      "\n",
      "Epoch 00018: loss improved from 2.37983 to 2.37920, saving model to letter-weights-cont-18-2.3792.hdf5\n",
      "Epoch 19/68\n",
      "1723806/1723806 [==============================] - 184s 107us/step - loss: 2.3783\n",
      "\n",
      "Epoch 00019: loss improved from 2.37920 to 2.37832, saving model to letter-weights-cont-19-2.3783.hdf5\n",
      "Epoch 20/68\n",
      "1723806/1723806 [==============================] - 185s 107us/step - loss: 2.3773\n",
      "\n",
      "Epoch 00020: loss improved from 2.37832 to 2.37734, saving model to letter-weights-cont-20-2.3773.hdf5\n",
      "Epoch 21/68\n",
      "1723806/1723806 [==============================] - 184s 107us/step - loss: 2.3753\n",
      "\n",
      "Epoch 00021: loss improved from 2.37734 to 2.37529, saving model to letter-weights-cont-21-2.3753.hdf5\n",
      "Epoch 22/68\n",
      "1723806/1723806 [==============================] - 184s 107us/step - loss: 2.3729\n",
      "\n",
      "Epoch 00022: loss improved from 2.37529 to 2.37286, saving model to letter-weights-cont-22-2.3729.hdf5\n",
      "Epoch 23/68\n",
      "1723806/1723806 [==============================] - 184s 107us/step - loss: 2.3726\n",
      "\n",
      "Epoch 00023: loss improved from 2.37286 to 2.37256, saving model to letter-weights-cont-23-2.3726.hdf5\n",
      "Epoch 24/68\n",
      "1723806/1723806 [==============================] - 184s 107us/step - loss: 2.3734\n",
      "\n",
      "Epoch 00024: loss did not improve from 2.37256\n",
      "Epoch 25/68\n",
      "1723806/1723806 [==============================] - 185s 107us/step - loss: 2.3707\n",
      "\n",
      "Epoch 00025: loss improved from 2.37256 to 2.37073, saving model to letter-weights-cont-25-2.3707.hdf5\n",
      "Epoch 26/68\n",
      "1723806/1723806 [==============================] - 184s 107us/step - loss: 2.3689\n",
      "\n",
      "Epoch 00026: loss improved from 2.37073 to 2.36886, saving model to letter-weights-cont-26-2.3689.hdf5\n",
      "Epoch 27/68\n",
      "1723806/1723806 [==============================] - 185s 107us/step - loss: 2.3667\n",
      "\n",
      "Epoch 00027: loss improved from 2.36886 to 2.36668, saving model to letter-weights-cont-27-2.3667.hdf5\n",
      "Epoch 28/68\n",
      "1723806/1723806 [==============================] - 185s 107us/step - loss: 2.3660\n",
      "\n",
      "Epoch 00028: loss improved from 2.36668 to 2.36602, saving model to letter-weights-cont-28-2.3660.hdf5\n",
      "Epoch 29/68\n",
      "1723806/1723806 [==============================] - 185s 107us/step - loss: 2.3651\n",
      "\n",
      "Epoch 00029: loss improved from 2.36602 to 2.36509, saving model to letter-weights-cont-29-2.3651.hdf5\n",
      "Epoch 30/68\n",
      " 472448/1723806 [=======>......................] - ETA: 2:14 - loss: 2.3653"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(X, y, epochs=68, batch_size=128, callbacks=callbacks_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
